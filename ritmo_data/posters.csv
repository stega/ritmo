;;;;;;;;;
;;;;;Posters are uplpaded here https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/;Videos are named with the EasyChair number followed by the first author's last name;;;
Easychair#;Poster Session;Poster number (within session);authors;title;keywords;abstract;YouTube link;Vortex link;
112;1;1;Dana Swarbrick, Cagri Erdem, Finn Upham, Kayla Burnim and Alexander Refsum Jensenius;The MusicLab App – Exploring the usage of mobile phones to measure audience movement and respiration;"concerts
movement
motion
embodiment
rhythm
entrainment
respiration";"The MusicLab App was developed to provide researchers with a tool for measuring participants’ motion. The mobile application leverages participants’ own smartphones and records data from the accelerometer, gyroscope, and GPS to measure motion, orientation, and location. So far, the open-source mobile application has been used during two virtual concerts and in May 2021, the application will be used at an in-person concert. The virtual concerts featured the genres of Algorave dance music and classical-Arabic fusion. The in-person concert will feature a classical quartet and since classical music audiences typically sit still during performances, we aim to measure respiration. 

The aim of this research is to examine different motion features that can be measured with the MusicLab App. In particular, we aim to measure quantity and periodicity of motion and respiration. We also aim to develop open-source tools that researchers can use to analyze the data provided by the application. 

We collected pilot data from 12 participants during two virtual concerts. To enable synchronization between the musical sound and the motion data during the virtual concerts, participants were instructed to shake their phones when they first heard the music. Participants then placed the phones on their upper bodies. During the in-person concert, participants will be provided with a phone holder that will position their phones on their chests.

To assess the accuracy of the mobile phones’ sensors, we will conduct a comparison between  the MusicLab App’s data and a high-resolution motion capture system to assess the accuracy and precision across different phone models and across a variety of contexts aiming to measure naturalistic music-related motion and respiration.  

Data collection and analysis are ongoing. Quantity of motion and periodicity will be extracted from the acceleration data and compared with musical features. Entrainment to the music and to other audience members and performers will also be examined using a variety of analysis methods including cross-correlations and recurrence analyses. The pilot data and comparison with the motion capture system will allow us to optimize the application prior to the in-person concert.

The MusicLab App aims to equip researchers with the tools needed to measure and analyze motion in a variety of contexts—especially musical experiences. The software and the analysis techniques developed through this project contribute to the mandate of open science.";https://youtu.be/cLAE5Gc_4hs;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%93112%20swarbrick.mp4?vrtx=view-as-webpage;
29;1;2;Andrew Chang, Xiangbin Teng, M. Florencia Assaneo and David Poeppel;Does the amplitude modulation of a sound determine its categorization as speech or music?;"Amplitude modulation (AM)
Speech perception
Music perception
Categorization";Despite our increasingly rich understanding of how humans process speech and music, surprisingly little is known about how they are treated as different auditory signals in the first place. From the perspective of acoustics, the properties of the signals can be differentiated. It has been shown that speech and music tend to have different amplitude modulation (AM) rates. Specifically, the AM rate of speech peaks between 4 and 5 Hz, while the AM rate of music tends to be slower, emphasizing modulation rate around 2 Hz (Ding et al., 2017). In addition, it is often argued that the AM of music tends to be more temporally regular or isochronous than speech (Kotz et al., 2018). Based on these insights, we hypothesized that the AM temporal features of an acoustic signal, especially its peak rate and regularity, are critical factors that determine whether a signal will be categorized as speech or music. Here we parametrically manipulated (i) the AM peak frequency (0.6 – 6.0 Hz) and (ii) the AM regularity to generate a variety of signals with varying AM envelopes. The AM envelopes were synthesized with an identical broadband low-noise noise carrier sound. Each stimulus is an amplitude modulated noise excerpt with manipulated AM features. More than 300 participants have taken part in two online behavioral experiments. On each trial, they listened to one of the generated stimuli and were prompted to make a binary judgment on whether it sounds more like a “speech” or a “music” recording. The preliminary results support the hypothesis that, across participants, the sound excerpts with slower peak AM rate and more temporally regular AM were more likely to be judged as music. These factors appear to have around 25-30% of explanatory power, suggesting that the amplitude envelope alone is essential to differentiate speech and music. To the best of our knowledge, this is the first study showing that the AM temporal features are critical low-level factors of determining a sound to be interpreted as speech or music.;;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9329%20chang.mp4?vrtx=view-as-webpage;
35;1;3;Anne Danielsen, Martin Langerød, Kristian Nymoen and Justin London;Genre expertise modulates timing perception and micro-level synchronization to auditory stimuli;"P-center
Synchronization
Timing
Musical genre
Musical expertise";"Background and Aims
When musicians play in synchrony it may be with greater or lesser precision. Musical expertise is generally regarded as improving of precision of both perception and performance (e.g., Repp & Doggett 2007; Cameron & Grahn 2014; Manning, Harris, & Schutz,2017). Is this rhythmic expertise generic, or is it tied to the particular style(s) and genre(s) of one’s musical enculturation? We investigated effects of genre-specific musical expertise on micro-level timing and synchronization, asking: Do expert musicians from different musical genres perceive and/or synchronize differently to familiar vs. unfamiliar musical sounds?  

Methods
60 expert musicians/producers were recruited from three genres: Jazz (N=21), Folk (N=19) and EDM/HipHop (N=20). Stimuli were in three categories: 4 quasi-musical sounds (“Neutral”), 4 from jazz and folk (“Organic”), and four from EDM/hip-hop (“Electronic”). Each category balanced the acoustic factors of attack (slow/fast) and duration (short/long) in a 2 x 2 design. Participants had two tasks: click alignment (via the method of adjustment) and tapping with claves. Dependent variables were the mean location and variability of the clicks and taps.

Results
As in our previous research (Danielsen et al. 2019, London et al. 2019), slow attack and long duration lead to later click/tap placement and higher variability (all p<0.001) in both tasks. A mixed RM ANCOVA showed significant effects of Stimulus Category, Participant Group, and a significant interaction for both dependent measures (all p≤0.001). Post-hoc tests show that folk and jazz musicians synchronize differently to the Organic sounds than the producers in both tasks (mean click/tap is 40 ms later for folk and 20 ms later for jazz musicians), and higher click/tap variability (11 and 9 ms, respectively). 

Conclusions
Genre expertise effects our generic, low-level perceptions of sounds as well as their affordance(s) for action/synchronization. The differences we found between participant groups was mainly driven by responses to the slow-long fiddle sound, a sound with strong associations to the Scandinavian fiddle music, suggesting top-down influence on bottom-up sensorimotor processing. When the sounds invoke a genre affiliation and/or the relevant musical task requires or affords rhythmic flexibility, as in a Norwegian Telespringar (Bengtsson 1974; Johansson 2010), then genre-related timing expectations, stylistic rules, and task-specific habits become relevant and influence both the perception and production of what the “correct” timing should be. 


References
Bengtsson, I. (1974). On notation of time, signature and rhythm in Swedish polskas. In G. Hilleström (Ed.), Studia instrumentorum musicae popularis III (pp. 22-31). Stockholm: Musikhistoriska museet/Nordiska Musikförlaget.

Cameron, D. J., & Grahn, J. A. (2014). Enhanced timing abilities in percussionists generalize to rhythms without a musical beat. Frontiers in Human Neuroscience, 8, 100.

Danielsen, A., Nymoen, K., Anderson, E., Câmara, G. S., Langerød, M. T., Thompson, M. R., & London, J. (2019). Where is the beat in that note? Effects of attack, duration, and frequency on the perceived timing of musical and quasi-musical sounds. Journal of Experimental Psychology: Human Perception and Performance, 45(3), 402-418.

Johansson, M. (2010b). The Concept of Rhythmic Tolerance – Examining Flexible Grooves in Scandinavian Folk-fiddling. In A. Danielsen (Ed.), Musical Rhythm in the Age of Digital Reproduction (pp. 69-84). 

London, J., Nymoen, K., Langerød, M. T., Thompson, M. R., Code, D. L., & Danielsen, A. (2019). A comparison of methods for investigating the perceptual center of musical sounds. Attention, Perception, & Psychophysics, 81(6), 2088-2101.

Manning, F. C., Harris, J. & Schutz, M. (2017). Temporal prediction abilities are mediated by motor effector and rhythmic expertise. Experimental Brain Research, 235(3), 861–871.
 
Repp, B. H., & Doggett, R. (2007). Tapping to a very slow beat: A comparison of musicians and non-musicians. Music Perception, 24(4), 367–376.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9335%20danielsen-london-genre-expertise.mp4?vrtx=view-as-webpage;
86;1;4;Cecilie Møller, Jan Stupacher, Alexandre Celma-Miralles and Peter Vuust;Beat perception in polyrhythms: Units of time come in pairs;"tempo perception
beat perception
subdivisions
metrical structure
polyrhythms";"The capacity for beat perception in music requires that the listener extracts at least two levels of a rhythm’s metrical hierarchy, i.e. the regular beat and its subdivisions (London, 2002). In the special case of polyrhythms, two or more mutually exclusive metrical hierarchies co-exist, yet the listener hears only one salient beat. The number of subdivisions within a polyrhythm cycle is defined by the least common denominator of the constituent isochronous pulse trains, and the beat level depends on the grouping of those subdivisions. For instance, the simplest 2:3 polyrhythm contains six grid points at the subdivision level, and the metrical structures underlying the slow (2) and fast (3) pulse trains contain ternary and binary subdivisions, respectively.

Here, we used polyrhythms to investigate the extent to which subdivision grouping is related to beat perception. In two online finger-tapping experiments, participants tapped in time with the perceived beat of (i) 2:3 and 3:4 polyrhythms in a wide range of tempi (n = 100), and (ii) polyrhythms with increasing levels of complexity, i.e., 2:3, 2:5, 3:4, 3:5, 4:5, 5:6 (n = 120). Using the Rayleigh test for circular statistics, participants’ consistent tapping responses were assigned to one of several possible beat categories, including slow and fast pulse trains and their half- and double-time equivalents. We hypothesized that participants would prefer to tap in time with a beat containing simpler subdivision grouping, i.e., they would prefer binary grouping over ternary grouping, and ternary grouping over irregular grouping of subdivisions.

Consistent with this main hypothesis, results of both experiments reflected tapping preferences for pulse trains containing simpler subdivision grouping. In the first experiment, response distributions were centered between 90 and 142 BPM, corresponding to the human preferred motor tempo (Parncutt, 1994). Notably, these response distributions were only observed within pulse trains containing binary subdivisions, i.e., the fast and slow pulse trains in the 2:3 and the 3:4 polyrhythms, respectively. The vast majority of participants consistently avoided ternary subdivisions. Their strategy was to metrically restructure the polyrhythm when tapping rates were too fast or too slow. Here, binary subdivision groupings were maintained by grouping cycles together and/or engaging in half- or double time tapping. Similar metrical restructuring was evident in the second experiment using more complex polyrhythms. Here, the preference for simpler subdivision groupings faded, however, as the polyrhythms required grouping of more than four subdivisions, suggesting that subdivision grouping predicts beat perception only below this limit. Closer inspection of the temporal aspects of the stimuli revealed that tapping preference transition points were tightly linked to the tempo of the subdivisions, rather than to the tempo of the pulse trains. 

In conclusion, beat perception in polyrhythms is constrained not by the tempo of the beat itself, but by the tempo of the subdivisions underlying it. We have a propensity towards beats containing binary subdivisions. These findings add important and yet neglected nuance to the understanding of temporal aspects of beat perception. 



References:
London, J. (2002). Cognitive Constraints on Metric Systems: Some Observations and Hypotheses. Music Perception, 19(4), 529–550. https://doi.org/10.1525/mp.2002.19.4.529

Parncutt, R. (1994). A Perceptual Model of Pulse Salience and Metrical Accent in Musical Rhythms. Music Perception, 11(4), 409–464. https://doi.org/10.2307/40285633";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9386%20moeller.mp4?vrtx=view-as-webpage;
89;1;5;Ji Chul Kim and Edward W. Large;A canonical modeling framework for rhythmic movement coordination and learning;"Bimanual coordination
Multifrequency coordination
Dynamical systems
Hebbian learning
The HKB model";"Haken, Kelso and Bunz’s (1985) model, commonly known as the HKB model, is a paradigmatic model for the bistability and transition in rhythmic hand movements, which has been extended to account for multifrequency coordination (Haken et al., 1996) and learning (Schöner, Zanone, & Kelso, 1992). The HKB model describes coordinated movements at the level of the collective variable, relative phase, which is derived from the equations of component oscillators representing the movements of individual hands. The equation of motion for relative phase is simple and easily expandable, with each term expressing a different mode of coordination (i.e., a different phase and frequency relationship). However, the equations of component oscillators (e.g., the van der Pol equation) are less transparent, typically with a single nonlinear coupling function accounting for multiple modes of coordination, and thus sophisticated mathematical procedures are required to obtain the equation of relative phase from the component-level equations.

Here we present an alternative modeling framework that captures the component-level dynamics of coordinated movements with a simple and transparent model. From the equations of component oscillators (e.g., the van der Pol equation), we derive a simple component-level model in which autonomous oscillations are represented as a circular limit cycle on the complex plane. This component-level model is called “canonical” in the sense that various equations of limit-cycle oscillation used to model rhythmic movements can be transformed to it via a continuous change of variables (Hoppensteadt & Izhikevich, 1997). The canonical model captures the dynamics of individual coordination modes with distinct coupling terms, as does the HKB equation of relative phase, but at the level of component oscillators. The isolation of individual modes (or resonant relations) makes the canonical model highly tractable, allowing close analysis of the component-level dynamics, and renders it easily expandable to complex architectures. Also, Hebbian plasticity can be incorporated to the dynamics of the canonical model (Kim & Large, 2021). The evolution of each coupling coefficient according to a Hebb’s rule models the learning (and forgetting) of an individual mode of coordinated movements.

To validate, we first show that the component-level canonical model exhibits the characteristic dynamic behaviors captured by the original HKB model and its extensions, including bistability in bimanual coordination, transition and critical slowing down at the critical frequency, and the relative stability of different modes of multifrequency coordination. We then demonstrate that the canonical model can be flexibly applied to complex experimental situations, with a replication of empirical findings on the acquisition and long-term retention of multifrequency (3:1) bimanual coordination (Park, Dijkstra, & Sternad, 2013).

References

Haken, H., Kelso, J. A. S., & Bunz, H. (1985). A theoretical model of phase transitions in human hand movements. Biological Cybernetics, 51(5), 347–356.
Haken, H., Peper, C. E., Beek, P. J., & Daffertshofer, A. (1996). A model for phase transitions in human hand movements during multifrequency tapping. Physica D: Nonlinear Phenomena, 90(1–2), 179–196.
Hoppensteadt, F. C., & Izhikevich, E. M. (1997). Weakly connected neural networks. New York: Springer-Verlag.
Kim, J. C., & Large, E. W. (2021). Multifrequency Hebbian plasticity in coupled neural oscillators. Biological Cybernetics. doi: 10.1007/s00422-020-00854-6
Park, S.-W., Dijkstra, T. M. H., & Sternad, D. (2013). Learning to never forget—Time scales and specificity of long-term memory of a motor skill. Frontiers in Computational Neuroscience, 7, 111.
Schöner, G., Zanone, P. G., & Kelso, J. A. S. (1992). Learning as change of coordination dynamics: Theory and experiment. Journal of Motor Behavior, 24(1), 29–48.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9389%20kim-large%20canonical%20modeling.mp4?vrtx=view-as-webpage;
10;1;6;Jonathan Cannon;Bayes-optimal rhythm perception;"Model
Dynamic Attending Theory
Entrainment
Predictive processing";"Is there an “optimal” way to parse a rhythm? Following the “predictive processing” / Bayesian brain ansatz, I argue that the process of rhythm perception can be viewed as dynamic inference of a hidden state based on sensory observations and a model of how the hidden state produces sensation. The hidden state is the rate and progress (or, in the case of periodic rhythms, phase and tempo) of an underlying temporal stream. The sensory observations are the timing of discrete sensory events. And the model combines enculturated metrical expectations with learned expectations for external or internal timing noise. Viewed in this way, the “objective” of rhythm perception is to continuously maintain the best possible estimate of phase and tempo, including the level of uncertainty about both, which can then be used to anticipate future events with appropriate levels of precision.

When this inference problem (Phase And Tempo Inference from Point Process Event Timing, or PATIPPET) is stated formally, solved approximately using variational Bayes, and simulated, it mimics human rhythm perception, both at a gross level and in various experimentally identified nuances. These include reinterpreting certain overly syncopated rhythms, perceiving illusory accelerations when expected events are omitted, and making larger error corrections during entrainment to slower rhythms, all phenomena that are not adequately addressed by existing models.

As a model of cognition and behavior, PATIPPET provides a normative framework for the insights of Dynamic Attending Theory [1] while introducing a novel role for dynamic temporal uncertainty. It points towards new ways of quantifying culture-specific rhythmic prototypes and new ways of breaking perceptual and motor entrainment measures into cognitively meaningful components. And it offers a mathematically rigorous “active inference” lens for understanding the rhythm-tracking advantage granted by tapping and other movement.

[1] Large, E. W., & Jones, M. R. (1999). The dynamics of attending: How people track time-varying events. Psychological Review, 106(1), 119–159.";https://youtu.be/We75kiiovPA;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9310%20cannon%20bayes-optimal.mov?vrtx=view-as-webpage;
118;1;7;Julien Laroche, Alice Tomassini, Luciano Fadiga, Antonio Camurri, Gualtiero Volpe and Alessandro D'Ausilio;Driving the rhythm « head on »: violinists' intra-body coordination depends on their inter-body coupling within the orchestra;"intrapersonal coordination
head movement
embodied rhythm
multiscale
musical ensemble";"More and more attention is being devoted to the study of behavioral coupling across bodies (i.e. how people coordinate the timing of their actions). This interest often reﬂects the assumption according to which individual actions depend on the behavior of others as well as on the dynamics of the group itself. Yet, few studies have addressed how group dynamics do inﬂuence intra-body coordination. The musical ensemble is a privileged and ecological context to dig into such an issue: performers are coupled in a multimodal fashion (mainly audio-visually, with a particular importance of vision when a conductor drives an orchestra) and their task requires the coordination of several parts of their bodies. Plus, musicians often exhibit supplementary gestures (e.g. head movements) that are deemed « ancillary », in the sense that they do not participate to the instrumental production of the sound. Rather, they are generally attributed with expressive and communicative purposes. We propose that this distinction (ancillary versus instrumental) is more blurry than currently (implicitly) assumed. More precisely, we hypothesize that, instead of being mere expression of action (destined to be communicated to others), non-instrumental body movements can play a constitutive role in the coordination of musical action. In other words, we propose that the body provides cues that can reinforce and stabilize information received from other modalities and/or other performers and even compensate for missing information when the group dynamics are perturbed.

To gauge the inﬂuence of group dynamics on intra-body coupling and the role of non-instrumental gestures in the coordination of musical performance, we captured the motion of the bow and the head of (ﬁrst-section) violinists playing a piece along other members of a conducted orchestra. We more speciﬁcally contrasted two experimental conditions : 1) Normal (habitual spatial disposition of the musicians) 2) Perturbed (violonists of the ﬁrst section turned their back on the conductor and faced the second section of violins). In virtue of the role of vestibular stimulations in the perception of rhythm, we expected violinists to compensate for the lack of visual rhythmic cues from the conductor in the perturbed condition with an increased coupling between head and bow movements.

We ﬁrst conﬁrmed our hypothesis using windowed cross-correlations. Then, spectral analysis helped uncover a strategical change in head movements. In normal situation, head movements were articulating the metrical level of a whole note, while in perturbed condition they got more complex by articulating multiple musically-relevant timescales (from whole to eight note), thus resembling the rhythmicity of the bow more. Phase analysis conﬁrmed the increase in coupling at salient frequencies of movement. Head movements, by embodying the temporality of musical actions, can thus play a functional and supporting role in the performance itself, and they do so through their coupling with gestures that are more directly instrumental in nature. Overall, this shows how the coupling dynamics at the group level impacts intra-body coupling.";https://youtu.be/SlL15k5SgsU;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%93118%20laroche.mp4?vrtx=view-as-webpage;
128;1;8;Kathryn Franich;How we speak when we speak to a beat;"Speech
Stress
Environmental coupling
Metronome synchronization
Cross-linguistic variation
African languages";"Introduction: Environmental coupling leads to increased movement amplitude in studies of
hand and limb movement [1,2], but little work has investigated the extent to which coupling
changes qualitative properties of speech. We investigate how speaking in-phase vs. out-of-phase
with a metronome influences speech production measures of syllable/vowel duration,
fundamental frequency, and amplitude in two languages with distinct rhythmic profiles, US
English, and Medʉmba, a Grassfields Bantu language. English is a stress-timed language where
lexical stress is cued through increased syllable duration and amplitude and more extreme
fundamental frequency. Medʉmba is a syllable-timed lexical tone language which shows no
evidence of lexical stress, though phonological restrictions suggest that word-initial syllables
bear greater prosodic prominence than non-initial syllables.
Method: A synchronization-continuation paradigm was used in which speakers of the two
languages repeated words and short (2-4 syllable) phrases with different stress/prosodic patterns
in time with a metronome. Speakers heard four beats of the metronome before initiating speech
on the fifth beat, repeating four times with the metronome, and continuing for four more ‘beats’
once the metronome ceased. On half the trials, participants were instructed to align the first
syllable of the word/phrase with the metronome beat; on the other half, they were asked to align
the first syllable just after the beat, on the ‘offbeat’ of the metronome. Trials were blocked by
phasing strategy (onbeat/in-phase vs. offbeat/out of phase); blocks were randomized by subject.
Results: English speakers generally preferred to align stressed syllables with the metronome,
regardless of their position. Linear mixed effects models used to analyze English data included
fixed effects of PHASING (onbeat/offbeat), POSITION (initial/medial/final), and STRESS
(stressed/unstressed); for Medʉmba data TONE (high/low) was used instead of stress. By-subject
random effects were also incorporated for both language models. Preliminary data from 10
speakers of each language revealed a significant effect of PHASING on vowel duration in English
(p<.001), with longer overall duration in the onbeat condition. An interaction between PHASING
and STRESS revealed that this effect was only significant for stressed syllables, however (p<.01),
suggesting that phasing did not affect overall word/phrase durations in the task (Fig 1). In
Medʉmba, vowels produced onbeat were also found to be longer (p<.01); an interaction between
PHASING and word/phrase POSITION revealed that vowels in initial position were more likely to be
lengthened in the in-phase condition (p<.05) (Fig 2). In both languages, f0 was found to be
overall very slightly lower in the onbeat condition (ps<.05). No effect of phasing was found on
intensity in either language.
Discussion: Metronome-coupled speech shows striking similarities across languages, most
notably evidenced by increased duration on in-phase coupled syllables across both languages.
Rhythmic-prosodic structure of the language played a role, however: English speakers strongly
preferred to align stressed syllables, regardless of position, to the metronome, while Medʉmba
speakers consistently aligned word/phrase initial syllables with the metronome, even in a subset
of English loanwords with non-initial stress. Our results may shed light on the nature of stress
typology more broadly, particularly with respect to the commonness of duration as a cue to stress
cross-linguistically.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%93128%20franich.mp4?vrtx=view-as-webpage;
14;1;9;Kjell Andreas Oddekalv;Verses in verses – convergent and divergent metrical structure in rap flows;"Metre
Musical structure
Music analysis
Rap
Lineation";"A common way of transcribing rap lyrics for musical and/or literary analysis is by aligning the lyrics to the musical metrical structure – the bars. Whether this stems from a goal of representing one music rhythmic parameter in a format otherwise devoid of musical information, or is an attempt to formalise the most common metrical structure in transcription is hard to tell, but the practice brings more questions than answers. This paper tackles some of these questions.
In rap music, the most common relationship between musical and poetic metrical spans – musical bars and poetic lines (typically referred to as ‘verses’ in literary theory) – is a convergent one. One poetic line spans one musical bar, with a rhyme placed on (or near) the fourth beat . A structure this paper will refer to as convergent metrical structure, from the convergence of the two metrical spans. However, it is common (increasingly so throughout hip-hop’s history) that this convergence is deviated from, both to lesser and greater degrees. When the metrical spans do not coincide, there is some sort of divergent metrical structure.
The prevalence of convergent metrical structure might lead one to believe that there is some sort of structural causal relationship for this overlap, and that musical metre has a type of gravity to it – a gravitic force forcing the poetic lines to adhere to it . This paper will argue that this is not the case. Without other evidence for lineation , like syntax and rhyme placement, supporting it, musical metre has little to no bearing on a listeners experience of poetic lines. Convergent metrical structure is a convention, not a result of musical metre’s gravitic force.
The paper will show examples of various types of divergent metrical structure, from the mild to the severe, from slight divergences to completely ‘polymetric’ structures. Additionally it will present an approach to visual representation of rap flows that highlights both musical and poetic metrical spans, as well as their interaction.
This paper can be considered for either oral or poster presentation.

Condit-Schultz, N. (2016). MCFlow: A Digital Corpus of Rap Transcriptions. Empirical Musicology Review, 11(2), 124-147. doi:10.18061/emr.v11i2.4961
Fabb, N. (2002). Language and literary structure : the linguistic analysis of form in verse and narrative. Cambridge: Cambridge University Press.
Katz, J. (2008). Towards a generative theory of hip-hop. Paper presented at the Music, Language, and the Mind, Medford MA. 
Mattessich, J. J. (2019). This Flow Ain’t Free: Generative Elements in Kendrick Lamar’s To Pimp a Butterfly. Music Theory Online, 25(1). doi:10.30535/mto.25.1.11
Ohriner, M. (2016). Metric Ambiguity and Flow in Rap Music: A Corpus-Assisted Study of Outkast's ""Mainstream"" (1996). Empirical Musicology Review, 11(2), 153-179. doi:10.18061/emr.v11i2.4896";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9314%20oddekalv.webm?vrtx=view-as-webpage;
77;1;10;Nicolas Pironio, Diego Fernandez Slezak and Martin Miguel;Evaluation of pulse clarity models on multiple datasets;"pulse clarity
deep learning
cross-evaluation";"The concept of pulse clarity describes the strength with which the pulse of a music stimulus is felt by a listener and is often used in music psychology experiments [1, 2]. It can also be conceived as the confidence with which a listener performs a beat tracking task. Models of pulse clarity estimate this property from an auditory stimulus. This work focuses on evaluating existing pulse clarity models on existing datasets as well and a new one. The work also introduces a novel pulse clarity model based on a deep learning beat tracking model.

Traditionally, pulse clarity has been estimated from auditory stimuli with MIRToolbox [3]. Recently, Miguel et al. [4] presented another model aimed to calculate pulse clarity on rhythmic symbolic stimulus. This work presents a novel pulse clarity model based on the beat tracking architecture introduced by Krebs et al. [5]. This architecture uses a recurrent neural network to estimate, for each time frame, the probability of a beat occurring at that frame. The distribution is later filtered by a dynamic bayesian network that estimates the probabilities of different beat interpretations. We propose inspecting the spread of the distribution, by means of its entropy, as a proxy of pulse clarity.
 
We evaluated all three models on three datasets with distinct types of stimuli. One dataset contains 100 movie soundtrack excerpts, 5 seconds in length, presented in [3], with target pulse clarity obtained as the mean of listeners' annotations. The second one is the training dataset for the MIREX Beat Tracking competition, comprised of 30 song excerpts (30 seconds) from various genres (from folk to R&B) [6]. In this dataset, pulse clarity was estimated from listeners' beat tapping annotations. We introduce a third dataset of 28 symbolic rhythmic stimuli (~30 seconds) where listeners tapped to a self-selected beat and also annotated perceived beat tapping difficulty. Here, pulse clarity is obtained both from the difficulty score and from the tapping annotations. In the last two datasets, pulse clarity was estimated from
tapping data as the entropy of the inter-tap interval distribution. Entropy measures how concentrated a distribution is, considering tapping multi-modality allowed in the experiment. Both entropy and pulse clarity scores where z-standardized per subject. The entropy calculation correlated significantly with subjective pulse clarity reports (spearman-r = 0.83, p < 0.001).

The performance of the pulse clarity metrics was calculated as the absolute spearman rank correlation against the empiric data of each dataset. Mean correlation r was 0.71 for the novel pulse clarity model, 0.64 for Miguel et al. [4] and 0.56 for Lartillot et al. [3]. The average standard deviation of the correlations across datasets was 0.18. In general, the novel model had better correlations, except for the MIREX dataset where Lartillot et al. [3] scored better. The exploration shows that all models can be used on domains outside those on which they were originally tested. Yet, on some datasets their performance changes significantly, opening the question of further evaluation on distinct domains.

[1] Burger, B., Thompson, M. R., Luck, G., Saarikallio, S., & Toiviainen, P. (2012, July). Music moves us: Beat-related musical features influence regularity of music-induced movement. In Proceedings of the 12th International Conference in Music Perception and Cognition and the 8th Triennial Conference of the European Society for the Cognitive Sciences for Music (pp. 183-187).

[2] Gonzalez-Sanchez, V. E., Zelechowska, A., & Jensenius, A. R. (2018). Correspondences between music and involuntary human micromotion during standstill. Frontiers in psychology, 9, 1382.

[3] Lartillot, O., Eerola, T., Toiviainen, P., & Fornari, J. (2008, September). Multi-Feature Modeling of Pulse Clarity: Design, Validation and Optimization. In ISMIR (pp. 521-526).

[4] Miguel MA, Sigman M, Fernandez Slezak D (2020) From beat tracking to beat expectation: Cognitive-based beat tracking for capturing pulse clarity through time. PLOS ONE 15(11): e0242207. https://doi.org/10.1371/journal.pone.0242207 

[5] Krebs, F., Böck, S., & Widmer, G. (2015). An Efficient State-Space Model for Joint Tempo and Meter Tracking. In ISMIR (pp. 72-78). 

[6] Mirex Beat Tracking training dataset.; 2006. Available from: https://www.music-ir.org/mirex/wiki/2019:Audio_Beat_Tracking";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9377%20pironio-slezak-miguel.mp4?vrtx=view-as-webpage;
100;1;11;Rebecca Schaefer and Benjamin Schultz;Tapping skills in the elderly: Associations with musical behaviors but not hearing abilities;"Sensorimotor synchronization
Aging
Hearing deficits
Musical behaviors";"As humans age, both motor skills and hearing abilities deteriorate. To better understand the effects of aging on sensorimotor synchronization, we performed an online experiment that included three aspects of hearing (thresholds for pitch, tempo, and dynamics), an inventory of musical sophistication (Gold-MSI questionnaire (Müllensiefen et al., 2014), which has separate subscales measuring musical training, perceptual abilities, singing abilities, active engagement, experiencing musical emotions, and a general sophistication factor), and a tapping task using several pop songs, where participants were asked to tap along freely with songs such as ‘Don’t stop me now’ by Queen and ‘Eye of the tiger’ by Survivor. Ratings of familiarity and liking were also collected.
The online study consisted of three parts, starting with the questionnaire, continuing to the tapping task, and finishing with the hearing tests. Four-hundred eighty predominantly older adults finished the first part, of whom 284 finished the tapping task (197 female, 169 male, and 2 opting not to say), with a mean age of 60.85 years (SD = 11.7 years, range = 19-86), with the hearing tests completed by 251 participants.
Tapping data were analyzed using circular statistics and, after averaging the resultant vector and mean tapping phase over four individual songs with different tempi, the resultant vector was found to be weakly to moderately positively correlated with all Gold-MSI subscales except ‘Experiencing emotions’ (from r = 0.09 for ‘Active engagement’ to r = 0.26 for ‘Musical training’). Mean tapping phase did not significantly correlate with any Gold-MSI subscales. Interestingly, no significant correlations were found between any of the hearing tests and either of these tapping indices, while the tempo and dynamics hearing tests correlated weakly to moderately with all Gold-MSI subscales except ‘Experiencing emotions’ (r = -0.16 to -0.23), and the pitch hearing test showed no such associations. Mean tapping phase was only associated with self-reported hearing loss. Age did not correlate significantly to any hearing or tapping indices, but was positively associated with self-reported hearing loss and negatively correlated with the Gold-MSI subscale ‘Experiencing emotions’. Multi-level analyses investigating the influences of familiarity and liking on tapping indices are underway.
These results suggest that, while the consistency (or precision) of tapping to music is related to all aspects of musical functioning measured in the current study, the tapping phase (or accuracy) is not. Surprisingly, hearing abilities, while related to these aspects of musical functioning, did not significantly correlate with these tapping indices. Moreover, age was not shown to be associated with either tapping measure, suggesting that musical engagement is a much larger predictor of tapping behavior than any perceptual or motor decline that might be expected with age. The contrast between tapping precision and accuracy suggests that tapping precision may be more related to musical behaviors than the precise phase of tapping in elderly participants. Possible clinical implications may be that it is more important to account for musical functioning than hearing abilities when designing music-based clinical interventions. This study demonstrates that these abilities can be detected using online test batteries. 

Müllensiefen, et al. (2014). The Musicality of Non-Musicians: An Index for Assessing Musical Sophistication in the General Population. PLoS ONE 9(2): e89642. doi:10.1371/journal.pone.0089642";https://youtu.be/pQucqPP0VAI;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%93100%20schaefer-schultz.mp4?vrtx=view-as-webpage;
82;1;12;Rebecca Scheurich, Ella Sahlas and Caroline Palmer;Mechanisms underlying musician’s enhanced auditory-motor synchronization flexibility;"synchronization
flexibility
musical training
inhibitory control
auditory-motor integration";"Successful auditory-motor synchronization requires that individuals flexibly adapt their actions to changes in the auditory environment, such as changes in rate. While adaptation to faster rates may depend largely on biomechanical constraints, adaptation to slower rates may depend on a more complex combination of factors (Repp, 2006). Previous research has suggested that musical training affords greater flexibility in adapting to an auditory stimulus that slows down in rate (Scheurich, Pfordresher, & Palmer, 2019; Scheurich, Zamm, & Palmer, 2018; Repp & Doggett, 2007). However, the mechanisms that allow musicians to better adapt to these slowing rate changes are still not well understood. One possibility is that musical training enhances inhibitory control, allowing for greater flexibility. Research has linked greater inhibitory control in musicians to greater synchronization consistency (Slater, Ashley, Tierney, & Kraus, 2017). Alternatively, musical training may enhance coupling between auditory and motor networks, thereby allowing for greater flexibility. Indeed, previous research has shown greater coactivation of auditory and motor networks following a short period of musical training (Bangert & Altenmüller, 2003). 

The current study investigates the contributions of inhibitory control and auditory-motor integration to enhanced synchronization flexibility in musicians. Due to the restrictions imposed by COVID-19, the current sample includes three musicians with more than six years of private instrumental music instruction and three nonmusicians with no private instruction in the past six years (less than two years of private instruction overall). Participants first performed an auditory Stroop task (Morgan & Brandt, 1989), a measure of inhibitory control. Sixty-four channel EEG was then recorded as participants tapped a familiar melody at a comfortable movement rate (the spontaneous rate), and subsequently synchronized their tapping of the same melody with a metronome set to that rate, and 15% and 30% slower than that rate. The between-subjects independent variable was musical training (musician and nonmusician). For the auditory Stroop task, the independent variable was trial type (congruent, incongruent, and neutral) and the dependent variables were accuracy and reaction time for accurate trials. For the synchronization task, the independent variable was rate (15% and 30% slower) and the dependent variable was the mean signed asynchrony adjusted for the mean signed asynchrony at the spontaneous rate. Asynchronies were adjusted in this way to examine the change in accuracy relative to the spontaneous rate. 

Preliminary findings suggest greatest accuracy and fastest reaction time for congruent trials in the auditory Stroop task, with musicians showing overall greater accuracy and faster reaction time than nonmusicians across all trial types. Findings also suggest greater synchronization accuracy in musicians than nonmusicians at the slowest rate relative to the spontaneous rate, replicating previous findings. Recurrence quantification analysis of EEG activity at the tapping rate for all subjects is planned; initial examination of a subset of subjects suggests greater predictability and stability of auditory-motor oscillatory neural activity with increased musical training. Together, these findings suggest that musical training enhances both inhibitory control and auditory-motor integration, which may explain musicians’ enhanced synchronization flexibility at slower rates.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9382%20scheurich.mp4?vrtx=view-as-webpage;
72;1;13;Sinead Rocha, Adam Attaheri, Aine Ni Choisdealbha, Perrine Brusini, Sheila Flanagan, Natasha Mead, Panagiotis Boutris, Samuel Gibbon, Helen Olawole-Scott, Christina Grey, Isabel Williams, Henna Ahmed, Emma Macrae and Usha Goswami;Infant sensorimotor synchronisation: A longitudinal analysis of the first year of life;"Sensorimotor Synchronisation
Infant
Longitudinal
Rhythm
Gross Motor
Language";"The BabyRhythm project is a longitudinal study of 122 infants from two- to 30-months-of-age, investigating neural entrainment and sensorimotor synchronisation (SMS) to acoustic rhythm, in relation to typical language development. Motion Capture was used to assess gross motor rhythmic movement at six timepoints between five- and 11-months-of-age. Infants were recorded drumming to stimuli of increasing linguistic and temporal complexity: Silence (‘Spontaneous Motor Tempo’; SMT), a 2 Hz (500 ms IOI) fixed rate drum beat (‘Drum’), a 2 Hz repetition of the syllable ‘ta’ (‘Syllable’), and infant directed songs of varying tempi from 1 – 2.33 Hz (‘Nursery Rhymes’). 

The current analyses aim to illustrate the typical development of infant SMS to speech and non-speech rhythmic sounds, over the first year of life. We test four hypotheses:
H1) Infant drumming will become faster with age 
H2) Infant drumming will more accurately match the tempo of rhythmic stimuli with age
H3) Infant drumming will more accurately match the tempo of the simpler rhythmic stimuli  
H4) Infant drumming will become more regular with age

Infant periodicity and regularity were derived from enhanced autocorrelations run with the MoCap Toolbox (Burger & Toiviainen, 2013). Linear Mixed Effects Models allowed interrogation of random as well as fixed effects. Our results confirm that infant drumming starts slow (5-month M = 648 ms IOI), and becomes faster with age (11-month M = 549 ms IOI). We further find that infants are more accurate drummers with age, though they are less accurate in the slower and more complex Nursery Rhyme condition. However, it is highly probable that the above results are conflated; that the rate of infant drumming appears closer to our 2 Hz Drum and Syllable stimuli with age when their own SMT is settling closer to 2 Hz. To index whether infants are indeed showing more mature performance when older, we therefore calculated how far infants were adapting away from their own SMT during the three sets of auditory stimuli. We find that older infants are showing a clear shift away from their SMT, particularly in the Nursery Rhyme condition: At older ages infants slow down to better match the Nursery Rhymes presented to them. Finally, we confirm that infants are more regular drummers with age. Interestingly, we also find that infants show much higher regularity when drumming in silence than in the presence of auditory rhythms – and that the general increase in rhythmicity with age is not evident when presented with the linguistic stimuli of the repeated Syllable or Nursery Rhymes. This evokes interesting questions on the role of variability in infant performance when attempting SMS with complex linguistic stimuli, with subtle variations in temporal signals. 

The Temporal Sampling framework (Goswami, 2011) suggests atypical neural entrainment and rhythmic synchronisation to slow rhythm patterns in child language disorders. In children, temporal accuracy/variability of tapping predicts language and phonology (e.g. Corriveau & Goswami, 2009; Carr et al., 2014). Our next work will use the individual differences uncovered here in infancy to predict phonological, lexical, and syntactic development to 2.5-years-of-age. 

References
Carr, K. W., White-Schwoch, T., Tierney, A. T., Strait, D. L., & Kraus, N. (2014). Beat synchronization predicts neural speech encoding and reading readiness in preschoolers. Proceedings of the National Academy of Sciences, 111(40), 14559-14564.
Corriveau, K. H., & Goswami, U. (2009). Rhythmic motor entrainment in children with speech and language impairments: tapping to the beat. cortex, 45(1), 119-130.
Goswami, U. (2011). A temporal sampling framework for developmental dyslexia. Trends in cognitive sciences, 15(1), 3-10.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9372%20rocha.mp4?vrtx=view-as-webpage;
113;1;14;Torbjörn Gulz and Andre Holzapfel;Synchronization in a jazz trio during Accelerando and Ritardando.;"Jazz
piano trio
double bass
drums
timing
tempo
accelerando
ritardando
click-track
note onset";"Background
A prevalent feature of Jazz performance is the maintenance
of an intended constant tempo [1]. Whereas an increase
in tempo may be valued as a means to increase the intensity,
a decrease in tempo is a widely despised practice.
Sometimes, however, a jazz group deliberately wants to
change the tempo, such as during the piano solo in the song
’No blues’ with the Miles Davies Quintet [2]. Changing
the tempo deliberately together as a band has been documented
to be a challenging task [3]. Schulze et al. [4] presented
two different working models used to understand
how musicians relate to change of tempo where the timekeeper
continuously is working with error corrections to
adjust to some predetermined value of tempo.

Aim
This study investigates how well three different professional
double-bass players were able to synchronize to a
drummer whose performance involves continuous tempo
changes. According to Hofmann et al. the tempo is mostly
controlled by the timing of the drummers [5]. Our main
aim is to gain first insights into the differences in synchronization
for accelerando, stable tempo, and ritardando.

Method
The setting was a trio with piano, bass, and drums. The
drummer listened to a pre-recorded click track (quarter notes),
and the other musicians followed his playing. The recorded
music was five choruses of minor blues, where the prerecorded
click specified the tempo. All performances were
played in 4/4, started at tempo 100 bpm, then accelerated
linearly during two choruses to 200 bpm, remained at this
tempo one chorus, and ultimately slowed down (ritardando),
during two choruses to 100 bpm. Three different takes
were made in a row, and all instruments were recorded on
separate tracks. The note onsets of the double bass and
drums were manually annotated using Sonic Visualiser,
and the differences between the click and onsets were calculated.
As a reference, the note onsets in a blues with
a constant tempo were also recorded and analyzed. Although
the bass player only related to the drummer’s interpretation
of the tempo, the analysis of onset differences
was conducted for the relation metronome-drums and metronome-bass.
The pianist was mainly included in the study to result
in an ecologically valid performance setting.

Results
The results show a clear positive time difference (note onsets
are after the metronome) during the accelerando and
a negative time difference during the ritardando for both
drummer and bassist due to the musician’s aim for error
correction. During the chorus with a constant tempo, the
discrepancies decreased and quickly resembled the reference
steady-recording discrepancies. It is also clear that
the musicians play closer to the click during an increase in
tempo than during a decrease in tempo.

Discussion
Although the purpose was to make an ecologically valid
recording, the musicians reported it to be intricate playing
to pre-recorded clicks. When annotating published commercial
recordings, it is obvious that accelerando and ritardando
are often performed step by step rather than in a
continuous, linear fashion.";https://youtu.be/eUjZAGbtYqM;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%93113%20gulz.mp4?vrtx=view-as-webpage;
92;2;1;Alexis Deighton MacIntyre and Sophie Scott;Listeners are sensitive to violations of natural speech breathing timing;"speech
breathing
speech entrainment
embodiment";"Rhythmic entrainment is increasingly viewed as a possible mechanism underpinning speech perception, but how the timing of speech interacts with the timing of non-speech sounds, such as breathing noise, is currently unclear. In this paper we report the results of two studies which investigated the effect of ordinal-temporal position relative to inhalation events on participants' ability to detect a silent gap during naturalistic speech. Experiment 1 (n = 24; in-person), compared awareness of gaps placed directly after either the utterance-initial or utterance-interjected breath at three different durations (200, 400, 800 milliseconds), by asking participants which breath was associated with the gap. Experiment Two (n = 182; online), used the same basic paradigm, but instead asking participants whether they perceived a gap at all. Additionally, we enhanced experimental control by introducing trials without gaps; a finer resolution in gap durations (200, 325, 450, 575, 700 milliseconds); and a third gap position, occurring directly \textit{before} the utterance-interjected breath to control for prior speech exposure. We hypothesised that accuracy would increase with gap duration, and that the utterance-initial breath position would be associated with a lower likelihood of gap detection. Across both studies we reliably found an effect of gap duration that was mediated by gap position; importantly, whether a gap was added before or after an utterance-interjected breath significantly predicted whether participants detected that gap, suggesting that listeners are sensitive to breathing sounds and form temporal expectations with regard to this non-verbal, embodied aspect of speech production. 
We found conflicting results concerning the correspondence between performance in a musical rhythm perception task and the speech gap detection task, despite previous evidence for a robust association between sensitivities to musical and linguistic timing. We conclude by contextualising our findings within the literature, arguing that the verbal acoustic signal is not ""speech itself"" per se, but rather one part of an broader, integrated percept worthy of wider exploration in speech entrainment studies.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9392%20macintyre-listeners-are-sensitive.mp4?vrtx=view-as-webpage;
8;2;2;Ana Clemente, Frances Board, Marcus T. Pearce and Guido Orgs;Hedonic judgment of audiovisual displays: Liking for complexity in dynamic sound and image sequences;"audiovisual
auditory
complexity
multimodal integration
visual";Most experiences in life are multisensory and temporal, like watching a dance or a movie. However, most research in empirical aesthetics has focused on either static visual or auditory modalities. Liking for visual and auditory stimuli has been characterized in terms of complexity, but its role in the appreciation of audiovisual displays has yet to be established. Here, we investigate the appreciation of audiovisual displays as a function of their auditory and visual complexities. We developed a set of non-representational audiovisual sequences varying systematically in auditory and visual complexity. Fifty-three participants rated their liking for combined audiovisual displays, and audios and visuals alone. Overall, liking increased with auditory complexity and followed an inverted–U relationship with visual complexity. Participants’ preference for audiovisual stimuli was dominated by preference for visual complexity. Additionally, whereas informational measures of complexity were good predictors of liking for visual sequences, they were less accurate than feature-based models in predicting liking for auditory and audiovisual sequences, pointing to modality-specific effects of feature-based complexity. There was a general preference for audiovisual congruence that increased with musical sophistication for older individuals. Finally, the results revealed two consistently distinct groups of participants with different hedonic sensitivity profiles, corresponding to liking or disliking complexity regardless of its nature (feature-based or informational), sensory modality (visual or auditory), or stimulus kind (unimodal or bimodal).;;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%938%20clemente.m4v?vrtx=view-as-webpage;
26;2;3;Christopher Corcoran and Klaus Frieler;The swing groove continuum: Analysing jazz soloists’ swing eighths using the Weimar Jazz Database;"jazz
swing
microtiming
groove
corpus analysis";"Type of proposal: Oral presentation

Background
The jazz phrasing structure known as a ‘swing groove’ is traditionally associated with long-short beat subdivisions, so-called ‘swing eighths’, traditionally conceptualised as forming a 2:1 beat-upbeat ratio (BUR). A range of case studies suggests that jazz performers play with much more variable ratios averaging well below 2:1 (e.g., Benadon, 2006; Busse, 2002; Butterfield, 2011; Ellis, 1991; Friberg and Sundström, 2002; Iyer, 2002; Wesolowski, 2012). Based on limited data (phrases, recordings, performers, or participants), these studies offer a useful starting point but cannot provide more conclusive evidence. However, a wider corpus analysis for more comprehensive data is possible with the Weimar Jazz Database (WJD), a corpus containing 456 monophonic jazz solos recorded by 78 canonical performers (Pfleiderer et al., 2017).

Aims
We investigated the distribution of BURs in the WJD’s jazz solos, exploring three questions posed by the literature: whether soloists play their eighth notes with 2:1 BURs, whether BURs decrease with tempo, and how BURs vary by jazz style and performers. 

Methods
Taking into account the WJD’s FlexQ annotation algorithm settings (Frieler, 2017), we applied careful selection criteria to capture a variety of event sequences that can be considered ‘swung eights’. We used binary logarithms to linearize and symmetrise the resulting data of about 15,000 BURs, allowing us to apply Gaussian mixture models to accommodate for multi-modal distributions. Then we explored BUR distributions globally as well as by tempo, style, and performer. 

Results
Results confirm the indications offered by case studies in the literature that most jazz soloists perform with much more diverse rhythms than a steady 2:1 ratio: Soloists tend to play with much lower, only slightly uneven ratios on average. Ratios approaching 2:1 and higher are only used occasionally and systematically occur more frequently at moderate tempi and in relatively modern jazz styles. We found large individual differences between performers and solos. 

Conclusion
The data suggests that—for solos—the constant 2:1 swing ratio is a conceptual myth or at least an oversimplification. We suggest that its popularity may be based on diverse perceptual factors (e.g., earlier drum onsets, tempo preferences) that make soloists’ ratios seem more exaggerated than they are. Soloists possibly use high swing ratios for disruptively gaining listener attention at crucial moments, thereby controlling associated levels of groove and listener enjoyment (Corcoran, 2021). Given the multi-modality of BURs, often even within a single solo, it may make more sense to differentiate between `hard’ and ‘soft’ swing as stylistic devices based on microtiming manipulation.

Implications
Our data shows that swing behaviour differs across jazz styles, tempi, and canonical performers. The results suggest that high and low ratios are employed for different perceptual reasons. Consequently, our results contribute insights relevant to jazz and groove microrhythm studies, practical and historical jazz research, and topics in music perception.


Bibliography
Benadon, F. (2006). Slicing the beat: Jazz eighth-notes as expressive microrhythm. Ethnomusicology, 50(1), 73–98. ISSN: 00141836

Busse, W. G. (2002). Toward objective measurement and evaluation of jazz piano performance via MIDI-based groove quantize templates. Music Perception, 19(3), 443–461. DOI: 10.1525/mp.2002.19.3.443

Butterfield, M. (2011). Why do jazz musicians swing their Eighth Notes?. Music Theory Spectrum, 33(1), 3–26. DOI: 10.1525/mts.2011.33.1.3

Corcoran, C. (2021). Swinging the Score: Compositional and empirical investigations into the performance of swing and groove rhythms by score-reading musicians. (Doctoral dissertation, University of Cambridge). DOI: https://doi.org/10.17863/CAM.63648

Ellis, M. C. (1991). An analysis of “swing” subdivision and asynchronization in three jazz saxophonists. Perceptual and Motor Skills, 73, 707–713. https://doi.org/10.2466/pms.1991.73.3.707 

Friberg, A., & Sundström, A. (2002). Swing ratios and ensemble timing in jazz performance: Evidence for a common rhythmic pattern. Music Perception, 19(3), 333–349. DOI: 10.1525/mp.2002.19.3.333

Frieler, K. (2017). The FlexQ algorithm. In M. Pfleiderer, K. Frieler, J. Abeßer, W.-G. Zaddach, & B. Burkhard (Eds.), Inside the jazzomat. New perspectives for jazz research (pp. 319–322). Mainz: Schott Campus.

Iyer, V. (2002). Embodied mind, situated cognition, and expressive microtiming in African-American Music. Music Perception, 19(3), 387–414. DOI: 10.1525/mp.2002.19.3.387

Pfleiderer, M. (2017). The Weimar Jazz Database. In M. Pfleiderer, K. Frieler, J. Abeßer, W.-G. Zaddach, & B. Burkhart (Eds.), Inside the Jazzomat: New Perspectives for Jazz Research (pp. 19–40). Mainz: Schott Music GmbH & Co. KG.

Wesolowski, B. C. (2012). Testing a model of jazz rhythm: Validating a microstructural swing paradigm (Doctoral dissertation, University of Miami). Retrieved from https://scholarlyrepository.miami.edu/cgi/viewcontent.cgi?article=1747&context=oa_dissertations";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9326%20corcoran.mp4?vrtx=view-as-webpage;
25;2;4;Dongho Kwak;Music for cells: The human body as a rhythm machine;"Rhythmicity
Physiology
Intercellular entrainability";In recent years, mechanobiology has developed a new interest in using sound pressure waves as a mechanical stimulus on cells. Several studies report the biological effects of simple tones (e.g., a sine tone) and music played through speakers on cells in vitro. The outcomes are intriguing, but there are many challenges ahead. One challenge is in the physiological relevance of the acoustic stimulus used in the experiments. This paper explores different rhythmic features of the human body to find a more pertinent model that can be used in acoustic-mechanobiology research. We first aim to give an overview of the rhythmicity that is found in the human body. For example, rhythmicity is typically found where there is a vital function for the body, including the heart, cardiovascular system, respiration, stomach (basal electrical rhythm), and locomotion. Secondly, we explore rhythmicity at the cell level and intercellular entrainability. In general, pacemaker cells are directly related to generating periodic patterns that are detected as rhythms. There have been studies of muscle cell rhythms. However, rhythmicity and contractility in non-muscle cells are less explored. Some recent studies examine the long-range contractility of non-muscle cells. After the overview and exploration of different types of cellular rhythmicity, we investigate how the rhythms are generated at the cellular level and explore whether cells (both muscle and non-muscle cells) show entrainability. The aim is to provide a model of physiological rhythmicity that can aid in developing and integrating this aspect of human physiology into life science research, specifically cell biology and mechanobiology.;;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9325%20kwak.mp4?vrtx=view-as-webpage;
21;2;5;Ece Kaya and Molly Henry;Individual differences in rhythmic entrainment and performance in temporal contexts requiring rapid adaptation;"rhythmic entrainment
motor synchronization
tapping
preferred rate
oscillator flexibility";"Rhythmic structure in speech, music, and other auditory signals helps us track, anticipate, and understand the sounds in our environment. One candidate mechanism that might support our ability to interact with auditory rhythms by synchronizing to them, is an internal – neural or perceptual – oscillator [1]. In this study, we focused on two properties of internal oscillators as well as their individual differences: preferred rate refers to the default rate of an oscillator in the absence of any input, and flexibility is the oscillator's ability to adapt to changes in the external rhythmic context.

In order to characterize preferred rate and flexibility on an individual-by-individual basis, we conducted a synchronization–continuation tapping experiment where participants tapped to isochronous sequences of 5 stimuli and continued tapping at the same rate for another 7 intervals. Each participant tapped at 401 unique rates spanning the tempo range of 1–5 Hz (i.e., 200–1000 ms). Trial-to-trial differences in rate were maximized to challenge participants' ability to adapt. This study thus possessed three unique features relative to previous synchronize–continue tapping experiments: 1) large rate range, 2) drastic trial-to-trial rate changes, and 3) short trials. Each participant completed a second, identical session after minimum of 5 days in order to assess test–retest reliability of the paradigm. 

We expected that preferred rate would manifest as best tapping performance when the stimulus rate matched an individual's preferred rate [2]. Moreover, we predicted that individuals with more flexible oscillators would be better at adapting to drastic rate changes. Indeed, we observed local minima in tempo-matching error during continuation, indicating better performance, at individually specific rate ranges. Moreover, for most participants, the rate range yielding best performance was consistent across sessions, indicating a reliable measure of preferred rate. 

In order to explore the effects of drastic trial-to-trial rate changes on performance, we assessed how single-trial synchronization errors depended on the direction and magnitude of rate-change from the preceding to the current trial. Linear fits quantified the extent of this dependence. 

Slopes from these linear fits showed that synchronization error significantly increased with rate-change magnitude, but only when the rate change was in a negative direction, that is, when the current trial was faster than the previous trial. For single participants, larger slopes across negative rate-change trials indicate a larger effect of rate-change on tapping performance, and thus failures in adaptation to changing temporal context. We thus consider this linear slope as a candidate measure for oscillator “inflexibility”. Critically, linear slopes differed between individuals consistently across sessions, supporting the characterization of slope as a reliable measure of oscillator flexibility. 

Although preferred rate has received some attention in the literature [2,3], oscillator flexibility has been an overlooked individual difference with the potential to index adaptability in difficult real-world listening situations, such as tracking speech in a noisy environment. As such, characterization of preferred rate and oscillator flexibility might provide a window onto some of the listening difficulties that come along with age.


REFERENCES

[1]	Henry, M. J., Herrmann, B., & Obleser, J. (2014). Entrained neural oscillations in multiple frequency bands comodulate behavior. Proc Natl Acad Sci U S A, 111(41), 14935-14940. doi:10.1073/pnas.1408741111

[2]	McAuley, J. D., Jones, M. R., Holub, S., Johnston, H. M., & Miller, N. S. (2006). The time of our lives: life span development of timing and event tracking. J Exp Psychol Gen, 135(3), 348-367. doi:10.1037/0096-3445.135.3.348

[3]	Drake, C., Jones, M. R., & Baruch, C. (2000). The development of rhythmic attending in auditory sequences: attunement, referent period, focal attending. Cognition, 77(3), 251-288. doi:10.1016/s0010-0277(00)00106-2";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9321%20kaya.mp4?vrtx=view-as-webpage;
124;2;6;Joshua Hoddinott and Jessica Grahn;Neural representations of rhythm and beat perception;"Beat
Rhythm
Basal ganglia
Pre-motor cortex
Supplementary motor area
fMRI
MVPA";"Humans spontaneously perceive an underlying pulse, or “beat,” arising from the rhythmic structure in music. Functional magnetic resonance imaging (fMRI) studies show that motor regions of the brain, such as the basal ganglia, supplementary motor area, and premotor cortices have increased activity when people listen to rhythms with a strong beat (Grahn & Brett, 2007). However, previous fMRI studies have generally used univariate analyses, which investigate activity averaged over voxels in a region, whereas multivariate techniques can identify patterns of covariance across voxels that allow greater sensitivity and better identification of stimulus features that predict brain–behavior relationships. Thus, here we use multivariate pattern analysis to compare neural activity patterns elicited by rhythms with strong, weak, or no beat. The results enable us to determine which neural regions are sensitive to beat strength and will allow us to characterize how these regions respond to varying degrees of beat strength. One possibility is that motor areas ‘tune’ activity patterns to each strong-beat rhythm, exhibited by high dissimilarity between activity patterns elicited by individual strong-beat rhythms, and low dissimilarity between patterns elicited by individual weak- and non-beat rhythms; alternatively, motor areas may activate in highly-correlated patterns for strong-beat rhythms, but may exhibit highly-dissimilar activity patterns for rhythms with weak or no beat. Preliminary results reveal that multivariate patterns do indeed appear sensitive to individual rhythms, but may also distinguish between rhythm types, building on previous univariate work.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%93124%20hoddinott.mp4?vrtx=view-as-webpage;
107;2;7;Lana Delasanta, Dobri Dotov, Edward Large and Laurel Trainor;Adapted Kuramoto model successfully captures group drumming variability;"Kuramoto
Collective performance
Modeling
Dynamics
Group dynamics
Nonlinear dynamics
Oscillators";"Rhythm affords the ability to coordinate and synchronize, for which there are numerous examples in humans. One such example, group drumming, allows us to examine the dynamics of rhythmic coordination and synchrony in human groups. When investigating four individuals drumming to an isochronous rhythm individually, in a duet, and in a quartet, results showed that variability among the individual drummers was higher in the quartet condition than it was as individuals [1]. 
When presented with this experimental research paradigm, we sought to use nonlinear dynamics to model the phenomenon found during group drumming. To do so, we utilized the Kuramoto model of nonlinearly coupled oscillators, in which synchronous behavior emerges from a population of oscillators with a distribution of natural frequencies [2]. For the purpose of this research, we consider each oscillator in the model to be one drummer, and each drum hit occurs when the oscillation reaches a phase of zero. However, during group drumming, each individual is not continuously coupled as the original model suggests, which required us to modify the model to allow for pulse coupling (i.e., oscillators synchronizing at discrete points in time). 
We were able to modify the Kuramoto model by replacing the continuous coupling function with a pulse function to mimic the sound of a drum hit. A gamma function is used to simulate a drum hit’s acoustic envelope to which the individual drummers synchronize (Figure C). This adapted model is capable of simulating previous data (Figure A) [1], as shown in Figure B. Ongoing and future research will utilize a larger group size during drumming to better adjust the model to predict synchronous behavior among more performers and more data types.

References
[1] Dotov, D. & Trainor, L. (2019). Collective performance facilitates anticipatory synchronization in group drumming. Rhythm Perception and Production Workshop (RPPW). Conference conducted at the meeting of RPPW, Michigan, USA.
[2] Strogatz, S. H. (2000). From Kuramoto to Crawford: exploring the onset of synchronization in populations of coupled oscillators. Physica D: Nonlinear Phenomena, 143(1-4), 1-20";https://youtu.be/7TG_j3MLS_k;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%93107%20delasanta.mp4?vrtx=view-as-webpage;
119;2;8;Olivier Lartillot and Mats Sigvard Johansson;Automated beat tracking of Norwegian Hardanger fiddle music;"beat tracking
Hardanger fiddle
rule-based
metre";"Norwegian Hardanger fiddle music is typically played by a solo fiddler, without rhythmic accompaniment except for the musician’s discreet foot stomping. Some of its repertoire features an asymmetrical ternary meter, with an uneven proportion of durations between the three beats of each bar, and with varying degrees of fluctuation of those proportions throughout each piece. In addition, there is often no clear audible onset corresponding to the beat position. As a result, many listeners find it difficult to hear the beats without experience from playing or dancing, and the beat onsets cannot be properly tracked by state-of-the-art beat trackers.

The aim of this study is to develop a computational model of beat tracking of Hardanger fiddle music. Due to the rhythmic irregularity of the music, computational approaches relying on the detection of regular periodicities cannot be used. The proposed strategy adopts a cognitive perspective, modeling processes that progressively infer beats while scanning the music sequence chronologically. To each successive note is associated a tentative metrical position, which is determined based on a set of rules, using various input data such as (1) the ratio of the inter-onset interval (IOI) from the previous beat onset to the current note onset and the preceding inter-beat-onset interval and (2) the ratio of the IOI from the bar onset to the current note onset and the preceding inter-bar-onset interval. A sequential repetition of notes of same rhythmic value (such as eighth notes) induces a specific configuration guiding the subsequent extension of the sequence. Multiple beat tracking scenarios can coexist at particular moments in the tune for very short periods. In particular, the very first notes at the beginning of the tune may initially imply conflicting metrical structures and tempi. The conflicting parallel beat tracking scenarios are progressively extended note after note in parallel. A scenario ends whenever it reaches a dead-end situation where the music is in total contradiction. Multiple scenarios are fused when they are continued exactly the same way, and only the scenario deemed the most congruent is retained.

One particularity of Hardanger fiddle music is that beat onsets are not precise points in time but rather diffuse temporal extension, closely related to the notion of beat bin (Danielsen, 2010). Sometimes, multiple successive notes can all be considered as possible onsets for a given beat (Johansson, 2010; Stover et al., 2021). This multiplicity of beat onsets has been integrated into the model. 

Most of the analysis can be carried out using solely note onset time as input data, although more challenging cases occasionally require taking into account note duration or higher structure such as motivic repetition. This indicates that a proper beat tracker needs to be integrated as a module within a comprehensive music analysis framework, with bidirectional dependencies with the other modules of the framework.  The model has so far been tuned and tested on a couple of tunes only. Its application to the automated analysis of a larger corpus is under investigation.

Danielsen, Anne (2010). “Here, there, and everywhere. Three accounts of pulse in D'Angelo's 'Left and Right’.” In A. Danielsen (Ed.), Musical Rhythm in the Age of Digital Reproduction. Farnham: Ashgate/Routledge, UK.
Johansson, Mats (2010). “The Concept of Rhythmic Tolerance – Examining Flexible Grooves in Scandinavian Folk-fiddling.” In A. Danielsen (Ed.), Musical Rhythm in the Age of Digital Reproduction. Farnham: Ashgate/Routledge, UK. 
Stover, Chris; Danielsen, Anne & Johansson, Mats (2021). “Bins, Spans, Tolerance: Three Theories of Microtiming Behavior.” [under review in Music Theory Spectrum].";https://youtu.be/Fe_95MIwhQE;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%93119%20lartillot.mp4?vrtx=view-as-webpage;
3;2;9;Olivier Senn, Dawn Rose, Toni Bechtold, Lorenz Kilchenmann, Florian Hoesl, Rafael Jerjen, Antonio Baldassarre and Elena Alessandri;Preliminaries to a psychological model of musical groove;"Groove
Entrainment
Body Movement";"Background

Although groove has been discussed in musical communities, and in jazz and popular music studies for many years (Berliner, 1994; Danielsen, 2006), its relevancy to music psychology has emerged relatively recently. In this field, groove is understood as a pleasurable urge in listeners to move their bodies in synchrony with the rhythm of music (Janata et al., 2012). A landmark study by Madison (2006) sparked a massive surge of scholarship dedicated to investigate what in music makes us move. The first fifteen years of psychological groove research yielded over fifty relevant studies, more than twenty of these published in the last two years.

Aims

Drawing on this vast literature, the “Psychological Model of Musical Groove” (Senn et al., 2019) was developed as a theoretical framework for groove research. This allows for the contextualisation of existing groove studies, and enables the interpretation of results from a breadth of perspectives.

Main contribution

The model postulates causal relationships between a variety of mental processes (listener’s sense of temporal regularity, rhythmic interest, listening pleasure, and the urge for body movement) that have been found to be relevant to groove. It also describes how synchronised body movement feeds back into these mental processes and increases the experience of groove. Finally, the model acknowledges that personal and situational aspects affect the experience of groove: listeners’ groove experience is influenced by their musical taste, cultural background, familiarity with a repertoire, the concrete listening situation and other aspects that are not directly related to the music and its auditory properties. 

Conclusions and Implications

The model is preliminary in the sense that it is a work in progress. The hypotheses about the mental processes involved in groove have yet to be empirically tested. The authors aim to conduct research studying the merits of these hypotheses in the coming years.  The model will be updated and adapted based on the results of future research and on discussions within the groove research community.


Literature

Berliner, P. F. (1994). Thinking in jazz: The infinite art of improvisation. University of Chicago Press.

Danielsen, A. (2006). Presence and pleasure: The funk grooves of James Brown and parliament. Wesleyan University Press.

Janata, P., Tomic, S. T., & Haberman, J. M. (2012). Sensorimotor coupling in music and the psychology of the groove. Journal of Experimental Psychology. General, 141(1), 54–75. https://doi.org/10.1037/a0024208

Madison, G. (2006). Experiencing Groove Induced by Music: Consistency and Phenomenology. Music Perception: An Interdisciplinary Journal, 24(2), 201–208. https://doi.org/10.1525/mp.2006.24.issue-2

Senn, O., Rose, D., Bechtold, T., Kilchenmann, L., Hoesl, F., Jerjen, R., Baldassarre, A., & Alessandri, E. (2019). Preliminaries to a Psychological Model of Musical Groove. Frontiers in Psychology, 10(1228), 1–5. https://doi.org/10.3389/fpsyg.2019.01228";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%933%20senn.mov?vrtx=view-as-webpage;
66;2;10;Rolf Inge Godøy;Impulse-driven rhythm objects;"rhythm objects
sound objects
intermittent
multimodal
motor control";"Various recent research in movement science has converged in suggesting that human motor control is intermittent, i.e. that skilled and rapid body motion require piecewise pre-planning combined with point-by-point triggering of such anticipatory planned chunks of body motion (Loram et al. 2014). Know by various names such as open loop, feedforward, or serial ballistic, the idea of intermittent control is based on constraints of our motor system, first of all that the motor system is too slow to allow for continuous feedback control in demanding tasks, but also on the need for a contextual activation spreading of preparatory motion of the effectors (fingers, hands, vocal apparatus) for upcoming events, known as coarticulation. Based on our motion capture data of musical performance, the aim of this paper is to show how these constraints are manifest in what we call impulse-driven rhythm objects, i.e. in holistically conceived and perceived multimodal chunks of combined motion and sound.

Besides drawing on insights from movement science, this object focus also has a background in notions of objects and/or gestalts in music (Schaeffer 1966), with sound objects typically in the duration range of approximately 0.3 to 3 seconds, recognizing the overall shape of the sound object as crucial for both the generation and perception of musical features. With an added related idea of action gestalts in motor control (Klapp and Jagacinski 2011), we can speak of multimodal sound-motion objects where the sensations of sound are closely linked with sensations of body motion and effort.

To better understand the manifestation of such impulse-driven rhythm objects, it will be useful first to give an overview of the basic ideas of intermittent motor control and to provide some plausible theoretical framework for how such piecewise anticipatory motor control might work, and then go on to demonstrate how motion capture data testifies to rather strong consistencies in the performance, suggesting a high degree of pre-planning. Also, this motion capture data can demonstrate how the workings of coarticulation are manifest in preparatory motion (e.g. fingers/hands move ahead to optimal position for upcoming tone onsets (Godøy 2014)). This is typically evident in fast rhythm objects such as ornaments, where due to the required speed, it will not be possible to make corrections mid-course, but where error corrections will have to wait until the next instance of the ornament. Lastly, the element of effort in the performance of such rhythm objects may be indirectly inferred from the motion capture data, but better explored with EMG data, i.e. muscle activity data. As demonstrated with our own and other EMG data, this concerns in particular optimization of motion, where the holistic anticipatory control of motion chunks may contribute to minimizing energy cost and enhancing sense of fluency (Gonzales Sanchez et al. 2019).

References
Loram, I. D., van De Kamp, C., Lakie, M., Gollee, H., and Gawthrop, P. J. (2014). Does the motor system need intermittent control? Exercise and Sport Science Review, Vol. 42, No. 3, pp. 117-125.
Godøy, R. I. (2014). Understanding Coarticulation in Musical Experience. In M. Aramaki, M. Derrien, R. Kronland-Martinet & S. Ystad (Eds.), Sound, Music, and Motion. Lecture Notes in Computer Science Volume 8905 (pp. 535-547). Berlin: Springer. 
Gonzalez Sanchez, Victor Evaristo; Dahl, Sofia; Hatfield, Johannes Lunde & Godøy, Rolf Inge (2019). Characterizing movement fluency in musical performance: Toward a generic measure for technology enhanced learning.  Frontiers in Psychology.  ISSN 1664-1078.  10 . doi: 10.3389/fpsyg.2019.00084
Klapp, S. T., and Jagacinski, R. J. (2011). Gestalt Principles in the Control of Motor Action. Psychological Bulletin, Vol. 137, No. 3, pp. 443–462. 
Schaeffer, P. (1966). Traité des objets musicaux. Paris: Éditions du Seuil.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9366%20godoy.m4v?vrtx=view-as-webpage;
67;2;11;Sara D'Amario, Anna Niemand, Werner Goebl and Laura Bishop;Changes in rhythmic body motion in highly-trained piano-singing duos;"ensemble performance
interpersonal interactions
motion capture
joint actions";"Music ensemble performances feature complex displays of rhythmic body motions, supporting sound production, interpersonal communication and interaction, and music expressiveness. Previous investigations, mostly focused on instrumental ensembles and dyadic music-making tasks, suggest that body gestures might be related to participants' familiarity with each other and the music, piece structure, and empathic perspective taking (EPT) of the participants. Nevertheless, the nature of these relationships has not been fully understood yet. 

This research investigates changes in rhythmic body motion in semi-professional piano-singing duo performances, and how these changes relate to piece characteristics as well as musical roles and empathic profile of the performers. Specifically, this research focuses on the rhythm patterning of moment-to-moment changes, measured per bar, in quantity of motion.

We tested the hypothesis that greater within-performer variability would emerge at more climactic parts of the pieces, and that patterns of body motion would recur within the pieces (which had repetitive song structures). It was also expected that performers would have a measurable effect on each other’s body motion, such that individual musicians would move differently when playing the same pieces with different people. In particular, we tested whether pianists, in their role as accompanists, would be more affected by the singers, in their role as soloists, than vice versa. Finally, we conjectured that duos in which both musicians had low EPT scores would show lower quantities of body motion than duos with high scores, as suggested in studies analysing the impact of EPT on interpersonal coordination (Novembre, Mitsopoulos and Keller, 2019).

Advanced piano and singing students were recruited. Duos were formed based on the participants' EPT scores, pre-assessed using the Interpersonal Reactivity Index questionnaire. High and low EPT groups were created, and each musician participated in two laboratory sessions, each time paired with a co-performer from the same or the other EPT group. Participants rehearsed two different pieces, Faure's  Automne and Schumann's Die Kartenlegerin, for about 15 minutes each. Each duo performed the pieces one time before and three times after rehearsal. Motion capture of the musicians' upper bodies, eye-gaze, breathing, heart activity, video, audio, and piano MIDI recordings of the repeated performances were collected. Data collection is ongoing: data from 8 duos were collected so far, and 16 more duos are planned. Quantity of upper body motion was computed using motion capture recordings, to analyse changes in relation to musical roles, structural characteristics of the piece, and EPT profile. The quantity of motion, summed across markers (14 per musician) and performers per second, and then averaged per bar, was compared across piece sections, repeated performances, and within and across duos, by implementing multilevel linear-models. The dependencies between musicians within duos was measured using Granger causality analysis.  

The analysis is underway. Results from this research are expected to clarify: i) how motion-based information flow relates to the musical roles of the singer and pianist and the empathic profile of the musicians, and ii) how rhythms in quantity of motion are shaped by repetition in musical structure and expressive intensity. 

References: Novembre, G., Mitsopoulos, Z., & Keller, P. (2019). Empathic perspective taking promotes interpersonal coordination through music. Scientific Reports, 9: 12255.

Type of proposal: Spoken paper";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9367%20damario.mp4?vrtx=view-as-webpage;
114;2;12;Tzu-Han Zoe Cheng and John Iversen;Individual differences in spontaneous motor tempo correlate to neural responses in sensorimotor synchronization task;"Sensorimotor synchronization (SMS)
Individual difference
Motor-auditory interactions
Beta-band oscillations";"Sensorimotor synchronization (SMS) is a common phenomenon in human activities such as marching, clapping or dancing to the music. SMS requires precise temporal coordination of perception, prediction and action, but abilities vary from person to person. Wing and Kristofferson (1973) proposed that errors in SMS can be partitioned between motor execution error and timekeeper error; these errors may be different among Individuals with differing stability of intrinsic rhythms. It has further been suggested that SMS ability is related to more precise temporal encoding of sounds. So, stable tappers compared to unstable tappers, stable tappers may have better auditory encoding of the sound, representation of intrinsic timing, or better motor execution. Here, we seek to disentangle these possible contributions by investigating the neural correlates of behavioral performances: We seek to understand how individual differences in SMS relate to individual differences in temporal dynamics of auditory and motor processing and, as well as differences in dynamic relationships between auditory and motor processing. To do this, we isolated the maximally independent components (ICs) in each individual best corresponding with motor and auditory cortical activity, then observed the neural correlates of distinct SMS accuracy across individuals. Participants performed three tasks related to SMS: a) spontaneous tapping at their preferred tempo without auditory stimulus (i.e. spontaneous motor tempo, SMT task, instructed to tap as stably as possible); b) arrhythmic auditory stimuli (150 drum stroke stimuli with IOI randomized between 1 to 1.5 seconds, i.e. Random sound task); c) SMS with an isochronous stimuli (drum strokes, 600 ms IOI, instructed to synchronize as precisely as possible). We grouped the subjects (N=25) into stable and unstable tappers (n=12 in each group) based on the variability of the inter-tap interval (standard deviation of ITI) in the SMT task (median = 37.03, range from 14.36 ms to 205.04 ms). Event-related changes in spectral power for auditory and motor ICs and directional causal flow connectivity analysis (i.e. direct directed transfer function) in the beta-band (20 to 35 Hz) were computed respectively for the stable and unstable tappers. During SMS, preliminary results showed that stable tappers had significantly stronger beta desynchronization in motor ICs followed both sound and tap onsets, as well as stronger beta band causal flow from auditory and motor ICs. The comparison between stable and unstable tappers did not reveal any differences in Random sound tasks, suggesting auditory encoding is similar between the groups. These findings show that neural underpinnings, highlighting in ERSPs of the motor ICs, of SMS vary among individual’s intrinsic stability. This suggests that unstable and stable tappers may have similar auditory encoding, but show differences in activity related to motor execution and potentially a different representation of beats based on auditory-motor interactions.";https://youtu.be/kaxNfGf_BYg;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%93114%20cheng.mp4?vrtx=view-as-webpage;
130;2;13;Yuka Saito and Jun Kawahara;Beat patterns determine inter-hand differences of synchronization error  in a bimanual coordination tapping task.;"Negative mean asynchrony
synchronization
bimanual coordination
tapping
handedness";"A previous study (Fujii et al., 2011) indicated that participants tapped a beat pattern with the right hand earlier than other beat patterns with other effectors, whereas a different study (Ascharsleben & Prinz, 1995) demonstrated the right and left hands tapped beat patterns simultaneously without any synchronization errors in a bimanual-tapping task. We examined the hypotheses that the inter-hand difference of the synchronization error occurred due to the handedness or a specificity of the beat pattern recruited in Fujii et al.’s (2011) because they maintained the hand-beat assignments throughout the session. We predicted that if the beat pattern determines the difference, the beat pattern with longer tap intervals should be tapped earlier than the beat pattern with shorter intervals regardless of handedness. Otherwise, the beat patterns tapped by the non-dominant hand should be tapped earlier than those tapped by the dominant hand regardless of the beat patterns to be tapped. Participants’ tap timings were measured by two force sensing registers, one for the right and the other for the left index finger. Two different beats (i.e., half beat vs. base beat in Experiment 1 and double beat vs. base beat in Experiment 2) were assigned to the right and left index fingers and participants were instructed to tap the two different beats to auditory pacing signals (60 bpm and 100 bpm) with both hands. The half beat represents the beat pattern to be tapped once per two auditory pacing signals, the double beat represents the beat pattern to be tapped twice per an auditory signal, and the base beat represents the beat pattern with identical interval to auditory pacing signals. The comparison between the half beat and the base beat shows that the base beat was tapped earlier than the half beat at any tempi. We suggest that a beat pattern with shorter intervals resulted in earlier tapping. However, the beat with identical intervals to auditory pacing signals could be earlier than the beat pattern with different intervals as an alternative interpretation. An alternative explanation was excluded and the first interpretation was supported by Experiment 2 that adopted a double beat with which participants tapped twice per an auditory pacing signal. If the beat pattern with shorter intervals was tapped earlier than the beat pattern with longer intervals, the double beat should be tapped earlier than the base beat. If the beat pattern with the identical beat with auditory pacing signals was tapped earlier, the base beat should be tapped earlier. The results of Experiment 2 demonstrated that a shorter interval beat pattern was tapped earlier, although participants tapped the same beat as the base beat. The distribution of synchronization errors of each tap showed that the beat pattern with shorter intervals was tapped earlier even when participants tapped with single hand and it would support the model of central timing control of bimanual tapping (Vorberg & Hambuch, 1984; Hestermann et al., 2018).";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%93130%20saito.mp4?vrtx=view-as-webpage;
45;2;14;Zachary Melton, Roger Chaffin, Kayleigh Kangas-Dick, Kerry Marsh and Alexander Demos;Does interpersonal liking lead to dyadic synchrony?;"interpersonal synchrony
music synchrony
social affiliation";"Synchronizing with another person’s actions increases positive feelings about that person  (Hove & Risen, 2009; Demos et al, 2012) and about the self (Lumsden, Miles, & Macrae, 2014), and increases feelings of joint identity and “being on the same team” (Wiltermuth & Heath, 2009). We asked whether the converse is also true: Do participants synchronize more with an agreeable than with a disagreeable partner? To find out, we recorded the hand movements of a participant and confederate as they each shook a small maraca (an egg-shaped shell containing small beads), held in the palm of one hand. Participant and confederate were standing in opposite sides of a room, separated by a curtain, and told to shake their maracas at a steady pace. Unbeknownst to the participant, the confederate wore concealed earphones that prevented her from hearing sounds in the room and allowed her to shake her maraca at a steady pace of 107 bpm, in time with an auditory metronome pulse that was inaudible to the participant. Participants were told that the study assessed the reliability of the motion tracking device for potential use in gyms and explained the presence of the confederate and the playing of background music during the task as attempts to make things more realistic. The participant was asked to select the music. The confederate responded to the participant’s choice agreeably (“Yeah, that sounds great), for half of the participants, or disagreeably (“No, not really, but whatever), for the other half. This was our manipulation of agreeableness.
First, we recorded participants’ maraca-shaking without the presence of the confederate or music, to provide a baseline. Second, we brought in the confederate and turned on background music with a tempo of 128 bpm, instructing participants to “do what you did before”. Third, we asked participants to “try to move at the same rate as the other person”, as the music continued to play. Fourth, we checked participants’ ability to synchronize with the confederate by instructing them to synchronize with their partner without music playing. Finally, participants rated how much they liked the confederate and rated their interaction with her for stress, awkwardness, and pleasantness. We measured participants’ synchrony with the confederate and with the musical pulse. Participants spontaneously synchronized with the music and synchrony with the confederate decreased in conditions where music was present, replicating Demos et al. (2012). Synchrony was unaffected by the agreeableness of the confederate, even though perceptions of the confederate were strongly affected by the agreeableness manipulation: Participants reported liking the confederate more and rated the relationship more positively in the agreeable than in the disagreeable condition (each Cohen’s d > 1). Perceptions of the interaction were unrelated to the degree of synchrony (r’s < .35). Thus, more positive feelings about a partner did not elicit more synchrony. In conclusion, the relationship between synchrony and interpersonal liking appears to be unidirectional. While others have shown synchrony increases interpersonal liking, our results suggest that interpersonal liking does not increase synchrony.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9345%20melton.mp4?vrtx=view-as-webpage;
84;3;1;Adrian Simen Holm, Maja Dyhre Foldal and Tor Endestad;Temporal statistical learning and individual differences in rhythm perception;"rhythm perception
beat perception
individual differences
statistical learning";"What accounts for individual differences in rhythm perception, and what is the relationship between rhythm perception and other cognitive abilities? One way to explore these questions is to investigate the relation between the ability to detect statistical properties of temporal stimuli in the environment and individual differences in rhythm perception. Inspired by a statistical learning paradigm investigating pitch (Garrido et al., 2013; Garrido et al., 2016). We exposed participants to a stream of homogenous tones in which the inter-stimulus interval (ISI) between the tones varied in a Gaussian-like fashion (mean ISI = 1000 ms). Reaction time (RT) data revealed participants to be sensitive to statistically deviant intervals, demonstrating an ability to distinguish temporal stimuli based upon continuous variation. However, there was considerable variation in participants’ ability to detect these deviants. We hypothesized that participants who showed the greatest sensitivity to temporal statistical outliers (as indexed by RT data) would show greater proficiency in rhythm perception tasks.

To investigate this, participants also completed two rhythm perception tests, the Musical Ear Test (MET; see Wallentin et al., 2010) and the Computerized Adaptive Beat Alignment Test (CA-BAT; see Harrison & Müllensiefen, 2018). These tests investigate rhythm perception in rather different ways, allowing for more nuanced interpretations of the role temporal statistical learning (TSL) plays in rhythm perception. Furthermore, the MET also includes a test of melody perception, making it possible to delineate the role TSL plays in different kinds of musical abilities. Additionally, we administered the Goldsmiths Musical Sophistication Index (Gold-MSI; see Müllensiefen et al., 2014). Data from this questionnaire allow us to explore relationships between TSL and other factors that are relevant for individual differences in musicality, such as musical training.

We will present our results from these investigations.


References

Garrido, M. I., Sahani, M., & Dolan, R. J. (2013). Outlier responses reflect sensitivity to statistical structure in the human brain. PLoS Comput Biol, 9(3), e1002999. https://doi.org/10.1371/journal.pcbi.1002999

Garrido, M. I., Teng, C. L. J., Taylor, J. A., Rowe, E. G., & Mattingley, J. B. (2016). Surprise responses in the human brain demonstrate statistical learning under high concurrent cognitive demand. NPJ Sci Learn, 1, 16006. https://doi.org/10.1038/npjscilearn.2016.6

Harrison, P. M. C., & Mullensiefen, D. (2018). Development and Validation of the Computerised Adaptive Beat Alignment Test (CA-BAT). Scientific Reports, 8. https://doi.org/10.1038/s41598-018-30318-8

Mullensiefen, D., Gingras, B., Musil, J., & Stewart, L. (2014). The Musicality of 
Non-Musicians: An Index for Assessing Musical Sophistication in the General Population (vol 9, e89642, 2014). Plos One, 9(6). https://doi.org/10.1371/journal.pone.0089642

Wallentin, M., Nielsen, A. H., Friis-Olivarius, M., Vuust, C., & Vuust, P. (2010). The Musical Ear Test, a new reliable test for measuring musical competence. Learning and Individual Differences, 20(3), 188-196. https://doi.org/10.1016/j.lindif.2010.02.004";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9384-holm.mov?vrtx=view-as-webpage;
95;3;2;Anders Friberg, Torbjörn Gulz and Claes Wettebrandt;Tempo-aware tools for modelling swing timing in a jazz ensemble;"music performance
micro-timing
swing
swing ratio
beat-upbeat ratio
modeling
performance rules.";"Background 
Swing in jazz is an example of intricate micro-timing patterns among the musicians in an ensemble. An apparent feature is the long-short pattern of consecutive eighth notes, defined as the ratio between the long and the short note.  This ratio is denoted swing ratio (SR) or beat-upbeat ratio (BUR). However, these long-short patterns are different for the instruments in an ensemble (soloist, bass, drums). The instruments also exhibit systematic onset differences, the most prominent is the delay of the soloist in relation to the ride cymbal at beat positions. In order to understand swing timing, it is necessary to consider the swing ratio of the different instruments, the delays between the instruments both at beat and offbeat position, and the dependence of tempo.

Aim
The aim was to define computer tools that could be used to generate music examples according to the principles used in real performances. The default behavior of the tools uses averaged parameters from ecologically valid examples.  Thus, applying the tools with default values would generate an acceptable performance for any jazz example and tempo. From this, other timing patterns may be generated, such as modeling individual performances. The aim was not to model timing parameters on the note level but rather to provide adequate average values.

Tools
A set of tools (rules) were defined that each modifies a specific timing parameter. They were all defined in the performance rule system Director Musices (DM). There are three rules that perform the main modifications. Rule 1 adjust SR for all instruments according to ride cymbal measurements. Rule 2 sets the soloist beat delay relatively the cymbal. Rule 3 sets the bass beat delay relatively the cymbal. All three with a linear variation as a function of tempo. After this step the instruments have the intended offset at beat level, the resulting SRs, and are synchronized at the offbeat. Rule 4 can optionally be used for setting a delay for the offbeats (either soloist or bass). There are also a few rules for dynamics and utility purposes. In addition, one can apply previously defined rules that model random components in human performances (i.e. non-systematic micro-timing variations).

Results
The timing pattern generated by the rules could be matched to the timing data from single performances. Informal listening confirmed that the default settings resulted in plausible timing patterns and that different playing styles could be emulated. Several sound examples will be presented illustrating the function of each rule. 

Discussion
Informal comparison with the original examples revealed missing aspects that contributed to the groove. One was the dynamic accent patterns of the soloist. Also, the lack of expressive variation of the timing patterns made the generated examples duller. 
A possible application in research would be to conduct listening experiments with a factorial design in which each parameter can be varied systematically. Another possible application is in teaching, where the rules and onset measurements provide an objective way of discussing micro-timing in jazz.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9395%20friberg.mp4?vrtx=view-as-webpage;
54;3;3;Birgitta Burger and Clemens Wöllner;Keep the beat. Time-keeping in case of a professional drummer;"Movement
drumming
motion capture
synchronisation
time keeping";"Background
Keeping the time is an important skill when making music. Drummers, in particular classical drummers, are supposedly among the best tempo keepers, as they are to provide the temporal basis for the whole band or orchestra. While sensorimotor synchronisation mechanisms have been investigated to a large extent, it remains unclear how professional drummers internally keep a steady pace when playing.

Aim
This study aimed to investigate how a professional drummer is able to keep the tempo in a metronome-synchronisation phase and in a following tempo-continuation phase in different playing conditions relating to tempo, dynamics, and rhythmic complexity.

Method
A professional classically-trained drummer (male, age: 27) performed two different rhythms (simple, complex) at two different tempi (80, 140 BPM) on a drum kit (kick, snare, hihat) while being recorded with optical motion capture. Each performance lasted 30 seconds, with a metronome click track being provided for the first 15 seconds (metronome phase) and the drummer being instructed to keep tempo for the remaining 15 seconds without the metronome (continuation phase). He was further asked to play either loud, medium volume, or quiet, and with as much movement as possible, normal amount of movement, and as little movement as possible, resulting in 36 different combinations of dynamics-movement instructions. 

Results
Acceleration was derived from the motion capture data of the sticks. From these data, time points were extracted when the sticks hit the snare and the hi-hat respectively. These time series were analysed related to their 1) internal consistency (inter-hit difference, i.e., the temporal difference between each two consecutive hits) and 2) synchrony (hit-beat difference, i.e., temporal difference between each hit and the correct beat location). A consistency as well as a synchrony measure were derived by calculating the standard deviations of the difference series, separately for the metronome and the continuation phases. Results show that, for consistency, the standard deviations were smaller in the continuation phase than in the metronome phase, indicating the drummer was internally more consistent when the metronome was not present (snare: t(35)=3.47, p=.001; hi-hat: t(35)=3.91, p<.001). In case of synchrony, results show that the standard deviations were larger in the continuation phase than in the metronome phase, suggesting that the drummer met the target tempo more accurately when playing with the metronome (snare: t(35)=-4.71, p<.001; hi-hat: t(35)=-4.64, p<.001). Neither tempo, rhythmic complexity, nor the different playing conditions affected these general results.

Discussion
These results suggest that integrating an external time base (metronome) leads to more accurate keeping of the intended target tempo, but also to higher inter-hit variability, suggesting it being an effort to perform the hits at the temporally exact beat/metronome location. While playing without the external time base yielded less accurate tempo keeping, it indicated more consistent internal timing. This implies that the drummer was able to follow his ‘inner’ clock and play less inhibited when performing without the metronome. Overall, the drummer performed relatively constant despite the different conditions. These outcomes provide valuable insights into timing mechanisms of highly skilled performers.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9354%20burger.m4v?vrtx=view-as-webpage;
46;3;4;Eniko Ladanyi, Catherine Bush, Youjia Wang, Tiffany G. Woynaroski, Miriam D. Lense and Reyna L. Gordon;Relationships of infant vocabulary development with parent musicality, rhythm skills and home music environment;"infant vocabulary development
home music environment
rhythm
musicality
parent-child associations";"Previous research has shown associations between rhythm and language skills (Gordon, Shivers, et al., 2015; Woodruff Carr, White-Schwoch, Tierney, Strait, & Kraus, 2014) and moderate to high heritability of both rhythm- and language-related traits (Ullén, Mosing, Holm, Eriksson, & Guy, 2014; Hayiou‐Thomas, Dale, & Plomin, 2012). In addition, home music environment of the child and self-reported musicality of the parent were related to language skills of preschool-aged children (Politimou, Dalla Bella, Farrugia, & Franco, 2019). Motivated by these results, the current study aimed to: 1) investigate if the association of language development with music environment as well as with parent musicality extends to infancy, and 2) evaluate whether early language development is also associated with parent rhythm processing measured behaviorally.
Data were collected from n = 45 parents participating in the Family GAMEs study (Grammar And Music Exercises for the whole Family), a longitudinal study on various predictors of language development. As a part of this study, parents completed a rhythm discrimination test as a behavioral measure of rhythm skills (shortened version of the Beat-Based Advantage test; Grahn & Brett, 2007), a self-reported musicality questionnaire (Goldsmiths Musical Sophistication Index; Müllensiefen, Gingras, Musil, & Stewart, 2014) and a questionnaire on home music environment of the child (Music@Home; Politimou, Stewart, Müllensiefen, & Franco, 2018) when their infant was between 6 and 12 months. They also completed the MacArthur-Bates Communicative Development Inventories (Fenson, 2007) as a measure of expressive vocabulary development at 12 (n = 39), 18 (n = 42) and 24 (n = 21) months of age (n differs across ages due to attrition [12 and 18 months] and ongoing data collection [24 months]).
Our preliminary results showed a positive association of 24-mo vocabulary with parent self-reported musicality (r(19) = .47, p = .030) as well as with home music home environment (r(19) = .45, p = .042). Vocabulary at 18 months was also associated with music environment (r(40) = .38, p = .012). An exploratory analysis of associations with parents’ self-reported musicality and music environment subscales revealed a positive relation between vocabulary size at each age and parent-initiated singing, which tended to become stronger with age (12mo: r(37) = .32, p = .034; 18mo: r(40) = .47, p = .002; 24mo: r(19) = .56, p = .008). Parent rhythm skills measured behaviorally did not show a significant association with any of the child vocabulary measures.
These results suggest that parent musicality as well as the home musical environment during infancy are linked with language development as early as 2 years of age. After completing data collection, we plan to investigate the relative contribution of parent musicality and environment at different stages of language development with path models to better understand the interplay between these variables. Our results could motivate further research on enriched music environment as a potential protective factor in developmental language disorders.

References
Fenson, L. (2007). MacArthur-Bates Communicative Development Inventories. Brookes Publishing.

Gordon, R. L., Shivers, C. M., Wieland, E. A., Kotz, S. A., Yoder, P. J., & Devin McAuley, J. (2015). Musical rhythm discrimination explains individual differences in grammar skills in children. Developmental Science, 18(4), 635–644. https://doi.org/10.1111/desc.12230

Grahn, J. A., & Brett, M. (2007). Rhythm and beat perception in motor areas of the brain. Journal of Cognitive Neuroscience, 19(5), 893–906. https://doi.org/10.1162/jocn.2007.19.5.893

Hayiou-Thomas, M. E., Dale, P. S., & Plomin, R. (2012). The etiology of variation in language skills changes with development: A longitudinal twin study of language from 2 to 12 years. Developmental Science, 15(2), 233–249. https://doi.org/10.1111/j.1467-7687.2011.01119.x

Müllensiefen D., Gingras B., Musil J., & Stewart L. (2014). The musicality of non-musicians: An index for assessing musical sophistication in the general population. PLoS ONE, 9(2), Article e89642. https://doi.org/10.1371/journal.pone.0089642

Politimou, N., Dalla Bella, S., Farrugia, N., & Franco, F. (2019). Born to speak and sing: Musical predictors of language development in pre‐schoolers. Frontiers in Psychology, 10, Article 948. https://doi.org/10.3389/fpsyg.2019.00948

Politimou, N., Stewart, L., Müllensiefen, D., & Franco, F. (2018). Music@Home: A novel instrument to assess the home musical environment in the early years. PloS ONE, 13(4), Article e0193819. https://doi.org/10.1371/journal.pone.0193819

Ullén, F., Mosing, M. A., Holm, L., Eriksson, H., & Madison, G. (2014). Psychometric properties and heritability of a new online test for musicality, the Swedish Musical Discrimination Test. Personality and Individual Differences, 63, 87–93. https://doi.org/10.1016/j.paid.2014.01.057

Woodruff Carr, K., White-Schwoch, T., Tierney, A. T., Strait, D. L., & Kraus, N. (2014). Beat synchronization predicts neural speech encoding and reading readiness in preschoolers. Proceedings of the National Academy of Sciences of the United States of America, 111(40),
14559–14564. https://doi.org/10.1073/pnas.1406219111";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9346%20ladanyi.mov?vrtx=view-as-webpage;
39;3;5;Francesco Ganis, Sofia Dahl, Guilherme Schmidt Câmara and Anne Danielsen;Beat precision and perceived danceability in drum grooves;"danceability
micro-timing
groove
urge-to-move
rhythm
drumming performance";"Musicians can place the time-position of events with high precision and according to personal preference, genre and tempo [1]. For instance, the swing ratio is not kept constant, but it is systematically adapted to a global tempo [2]. In contemporary music, drummers can achieve a specific feel by manipulating the timing of rhythms in different ways and placing event onsets earlier or later compared to the time reference [1]. These small adjustments in time are also referred to as micro-timing variations.
The aim of this study was to investigate the influence of micro-timing variations in live-played rhythms on the perceived danceability and timing precision. The stimuli were chosen from Câmara et al. [1] where drummers were playing two different patterns with different timing styles (laid-back, pushed, on-beat). Two drummers’ performances were selected based on their reported average systematic timing. These 12 recordings were mixed with the instrumental backing track (bass and guitar) heard by the drummers to form the stimuli.
Forty participants (M = 28.23 years, SD = 11.80), 28 males and 12 females, with varying musical background were recruited via social media (Facebook pages, groups and direct messages to chat groups). Participants were sent a link to the online listening test using Google Forms with modifications that presented the stimuli as embedded videos. Each video started with a prompt to wear headphones followed by 4 bars of groove for a total of 11 seconds (with a static image). For each page, the participant was presented with a reference track (on-beat timing) and a “beat” track (laid-back or pushed timing) and asked to rate the perceived danceability from 1 (not danceable at all) to 5 (very danceable). Additionally, listeners were asked to compare the beat with the reference track and indicate whether this was pushed (ahead), laid-back (behind) or on-beat (synced with) the reference in terms of timing.
Preliminary results indicate that micro-timing variations affect the perceived danceability. On-beat patterns were rated with the highest danceability, followed by laid-back and pushed styles. The drummer that obtained the highest danceability rating for the laid-back performance is also the one that was mainly recognized as on-beat performer. As expected, identification of timing (ahead, behind or on) proved to be difficult. Using the instrumental backing track as a time reference could possibly have made the task even harder for untrained listeners. Future research could address this by comparing danceability ratings for the grooves mixed with different backing tracks. 

References
[1]	G. S. Câmara, K. Nymoen, O. Lartillot, and A. Danielsen, “Timing Is Everything…Or Is It? Effects of Instructed Timing Style, Reference, and Pattern on Drum Kit Sound in Groove-Based Performance,” Music Percept., vol. 38, no. 1, pp. 1–26, Sep. 2020, doi: 10.1525/mp.2020.38.1.1.
[2]	H. Honing and W. B. de Haas, “Swing Once More: Relating Timing and Tempo in Expert Jazz Drumming,” Music Percept., vol. 25, no. 5, pp. 471–476, Jun. 2008, doi: 10.1525/mp.2008.25.5.471.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9339%20ganis.mp4?vrtx=view-as-webpage;
125;3;6;George Sioros;A dynamic time warping method for the classification and quantification of expressive timing relations in musical signals;"Dynamic Time Warping
DTW
expressive timing
classification";"Background:

Musical signals are commonly represented as time series and their analysis is often focused on the similarity of their amplitude and shape as well as in their temporal characteristics. A widespread similarity measure for time series is the Dynamic Time Warping distance (DTW). The DTW algorithm finds the optimal alignment between the time series prior to measuring their amplitude difference to avoid artifacts that arise from the signals being distorted in time. Several variants of the algorithm have been developed over the last two decades that greatly improved its performance (e.g. [1, 2]). While most of those variants are centered in shape and amplitude differences, here we focus on the similarity of time series in the temporal domain and the recently proposed Time Alignment Measurement (TAM) [3]. We evaluate the TAM in the classification of music related signals that comprise rhythmic variations and a steady beat and propose an alternative method for the task that outperforms it.  

Method:

We conducted two classification experiments. In the first experiment, 30 time series were produced from a set of 10 simple piano melodies taken from a previous study [4]. The set consists of mechanical “deadpan” interpretations of the melodies and 2 additional algorithmic variations of each melody. The tempo and beat duration were kept constant throughout the music examples and the rhythmic variations took place within the beat duration in the form of pickups and syncopations. 

In the second experiment, we asked 21 participants to follow the motion of a sound in space with their hand, while their movement was recorded in a 3D motion capture system. The sound was oscillating between two points in three different motion patterns. While the period of the oscillation was constant producing the sensation of a steady beat, the patterns differ in the speed with which the sound traveled between the two points.

We implemented two Nearest Neighbor classifiers (1-NN) using: 1) the TAM distance between time series, 2) a simple Euclidean distance. 

Results and Conclusion

The TAM distance showed limited success in the tasks. In the first experiment, TAM outperformed the Euclidian classifier, although both classifiers had high error rates (TAM=50%, EUC=65%) . In the second experiment TAM achieved a lower error rate (41%), though, it was outperformed by the Euclidean classifier (e.r.=16%). The better performance of the Euclidean distance is not the result of a more effective measurement of timing differences in the signals but rather of the incidental differences in amplitude.
 
Motivated by the findings of our experiments, we developed a novel method for the quantification of timing similarities that uses a transformation of the time warping paths produced by the DTW to directly compare them. The method outperforms the other classifiers in the two experiments (error rates 7% and 5%).  Furthermore, we demonstrate how the statistical processing of the transformed warping paths delivers insights on the timing relations between the signals.  


References:

[1]	J. Zhao and L. Itti, “shapeDTW: Shape Dynamic Time Warping,” Pattern Recognit., vol. 74, no. c, pp. 171–184, 2018.
[2]	F. Petitjean, A. Ketterlin, and P. Gançarski, “A global averaging method for dynamic time warping, with applications to clustering,” Pattern Recognit., vol. 44, no. 3, pp. 678–693, 2011.
[3]	D. Folgado, M. Barandas, R. Matias, R. Martins, M. Carvalho, and H. Gamboa, “Time Alignment Measurement for Time Series,” Pattern Recognit., vol. 81, pp. 268–279, 2018.
[4]	G. Sioros, M. Miron, M. Davies, F. Gouyon, and G. Madison, “Syncopation creates the sensation of groove in synthesized music examples,” Front. Psychol., vol. 5, p. 1036, Sep. 2014.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%93125%20sioros.mp4?vrtx=view-as-webpage;
36;3;7;Haley Kragness, Idila Yogeswaran and Laura Cirelli;Children’s social attributions of synchronous movers;"synchrony
movement
development
affiliation";"Moving together synchronously is a prominent feature of cultural activities (i.e. rituals, musical engagement) across the globe. Moving in synchrony increases mutual aid, trust, and affection amongst those moving together, even in young children (e.g. Cirelli et al., 2014; Rennung & Göritz, 2016). Similarly, infants and adults expect synchronously-moving people to display more prosocial behaviours towards each other than asynchronously-moving people (e.g. Cirelli et al., 2018). These expectations suggest that synchrony is a cue for inferring social affiliation, but the little is known about the specific nature of that affiliation. Previous research has shown that children use social information – for instance, whether two people know vs. like the same songs – to make inferences about the relationship between those people (Soley, 2019). Here, we investigate how observing interpersonal synchrony guides children’s social expectations. Specifically, do children believe that people moving together are more likely to share knowledge, to share preferences, or to share both? 

After receiving instructions from a research assistant over Zoom, children (6 to 7 years of age, current N = 28, target N = 50) independently complete the experiment through a game-like online program. On each trial (16 total), children first watch a 7-second video of three novel characters (familiarization phase). All the characters move side to side. The target character in the middle moves synchronously at the same tempo (either 100 or 140 beats per minute) with either the character on their left or right, while the third character moves at a different tempo (asynchronously). Next (test phase), the child was asked two questions: which test character(s) share knowledge (“who also knows about ___?”) and which test character(s) share preferences (“who also likes ____?”) with the target character (“left, right, or both”). At the end of the experiment, participants’ recall for 6 “old” characters (3 sync and 3 async movers) and 6 “new” characters was assessed. 

We have two preregistered predictions: 1) synchrony will guide social expectations for group membership and friendship. Children will select the synchronous partner more often than chance in both the shared knowledge and shared preference conditions. 2)  synchrony will especially convey specific affiliation, in that the bias toward selecting the synchronous partner in the “shared preference” condition will be significantly stronger than in the “shared knowledge” condition. In an additional exploratory analysis, we will also investigate if children have superior visual memory for the synchronous characters, which would then suggest that the synchronous movement particularly attracted their attention. 

Results of the present study will allow us to acquire further knowledge regarding children’s use of interpersonal synchrony as a signal for understanding social relationships. Indeed, significant debate exists regarding the evolutionary roots of interpersonal synchrony and its effect on prosocial behavior. The present study will further the discussion through examining the kinds of social relationships children infer by observing synchrony. 


Note: We would like this submission to be considered for a poster.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9336%20kragness%20Children%E2%80%99s%20social%20attributions.mp4?vrtx=view-as-webpage;
73;3;8;Jan Stupacher, Cecilie Møller, Alexandre Celma-Miralles and Peter Vuust;The bass lays down the beat in polyrhythms;"polyrhythm
beat induction
finger tapping
bass frequencies";"To make sense of musical rhythms we group events and subdivide time; we create metrical structures. Polyrhythms contain two different metrical structures that compete against each other to become the fundament that allows us to perceive a beat. One factor influencing which structure is preferred as the metrical fundament is pitch. Bass frequencies are important for movement induction, such as tapping in time with the beat and dancing (Hove et al., 2020; Stupacher at al., 2016; van Dyck et al., 2013; Varlet et al., 2020). Rhythmic elements with lower pitch increase the sensitivity to timing variation on behavioral and neural levels (Hove et al., 2014), and EEG activity at meter-related frequencies increases with lower pitched sounds (Lenc et al., 2019). Here, we investigated the effect of low pitch on beat perception in polyrhythms.

Stimuli consisted of 2:3 and 3:4 polyrhythms with isochronous pulse trains at tempi of 90:135 and 90:120 BPM, respectively. In each polyrhythm, one of the pulse trains was a low-pitched marimba sound with a peak frequency of 262 Hz and the other a high-pitched marimba sound with a peak frequency of 1047 Hz. This manipulation was counterbalanced. In an online paradigm, we asked 80 participants to tap in time with the perceived beat of each stimulus. The dependent variable was defined as the tapping consistency—the mean vector length in circular statistics—in time with the slow pulse train minus the tapping consistency in time with the fast pulse train. Values close to -1 indicated that participants consistently tapped in time with the slow pulse train, whereas values close to 1 indicated that participants consistently tapped in time with the fast pulse train. We expected participants’ tapping responses to reflect a preference for the pulse trains with lower pitch.

As hypothesized, pulse trains with lower pitch were more likely to be perceived as the beat. This was indicated by two individual Wilcoxon tests showing significant effects of pitch in 2:3 and 3:4 polyrhythms (Z = -4.98, p < .001, r = .56; Z = -5.52, p < .001, r = .62). In line with our findings from two previous experiments, we additionally observed a preference for tapping in time with the pulse trains in which the subdivisions, i.e., the least common denominator, can be grouped binarily compared to ternarily. This was indicated by a preference for the 3 pulse train in both the 2:3 and the 3:4 polyrhythms.

Our findings suggest that preferences for synchronizing movement to rhythmic elements with lower pitch is not only evident in simple rhythms but also in polyrhythms. Physiological not mutually exclusive factors driving the bass superiority effect include tactile stimulation (Hove et al., 2020), vestibular stimulation (Todd & Lee, 2015), and encoding in the auditory pathway (Hove et al., 2014). Whether other factors influencing the bass-beat-movement connection are learned by exposure to music with important rhythmic information produced by bass instruments or innate and potentially driven by evolutionary pressures remain open questions.

References

Hove, M. J., Marie, C., Bruce, I. C., & Trainor, L. J. (2014). Superior time perception for lower musical pitch explains why bass-ranged instruments lay down musical rhythms. Proceedings of the National Academy of Sciences, 111(28), 10383–10388. https://doi.org/10.1073/pnas.1402039111

Hove, Michael J., Martinez, S. A., & Stupacher, J. (2020). Feel the bass: Music presented to tactile and auditory modalities increases aesthetic appreciation and body movement. Journal of Experimental Psychology: General, 149(6), 1137–1147. https://doi.org/10.1037/xge0000708

Lenc, T., Keller, P. E., Varlet, M., & Nozaradan, S. (2018). Neural tracking of the musical beat is enhanced by low-frequency sounds. Proceedings of the National Academy of Sciences, 115(32), 8221–8226. https://doi.org/10.1073/pnas.1801421115

Stupacher, J., Hove, M. J., & Janata, P. (2016). Audio features underlying perceived groove and sensorimotor synchronization in music. Music Perception, 33(5), 571–589. https://doi.org/10.1525/mp.2016.33.5.571

Todd, N. P. M., & Lee, C. S. (2015). The sensory-motor theory of rhythm and beat induction 20 years on: A new synthesis and future perspectives. Frontiers in Human Neuroscience, 9, 444. http://dx.doi.org/10.3389/fnhum.2015.00444 

Van Dyck, E., Moelants, D., Demey, M., Deweppe, A., Coussement, P., & Leman, M. (2013). The impact of the bass drum on human dance movement. Music Perception, 30(4), 349–359. https://doi.org/10.1525/mp.2013.30.4.349

Varlet, M., Williams, R., & Keller, P. E. (2020). Effects of pitch and tempo of auditory rhythms on spontaneous movement entrainment and stabilisation. Psychological Research, 84, 568–584. https://doi.org/10.1007/s00426-018-1074-8";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9373%20stupacher.mp4?vrtx=view-as-webpage;
42;3;9;Laura Bishop, Alexander Refsum Jensenius and Bruno Laeng;Musical and bodily predictors of mental effort in string quartet music: An ecological pupillometry study of performers and listeners;"ensemble performance
pupillometry
attention
music listening
music expression";"Certain musical features are attention-grabbing for listeners. These include ""surprising"" pitches that deviate from an established tonal context and notes that deviate from strict rhythmic regularity (i.e., microtiming). Attention also fluctuates in response to musical rhythms, reflecting listeners' entrainment with the music. For music performers, processing of these structural features must occur in addition to the planning and execution of a complex set of (sound-producing and expressive) action sequences. Motor production, therefore, places additional demands on performers' attention. The rhythm of listeners' and performers' moment-to-moment attention fluctuations can be captured using pupillometry. Psychophysiological research has shown that pupil dilations relate to increases in attention and cognitive arousal. In this study, we used pupil diameter as an index of mental effort (intensity of attention) in listeners and performers. Our aim was to show how the rhythm of attention fluctuations relates to changes in musical complexity, technical demands, and expressive difficulty. A secondary aim was to show how performers' cognitive arousal evolves across the course of a live concert in which different repertoire is played and the performers' musical roles vary. 

We captured pupil data from the members of a classical string quartet as they rehearsed and performed a concert (for live audience and an examiner) in our lab. Afterwards, the musicians provided ratings of the harmonic complexity, technical difficulty, and expressive difficulty of the music (per bar for each piece). The audio recordings from the quartet's concert performance were then played for an independent sample of musically-trained listeners, whose pupil data were also collected. We took a modelling approach to evaluate the effects of musical complexity (using performers' ratings of harmonic complexity, and a score-based measure of harmonic tension), bodily effort (using performers' head and arm motion, sound level, and performers' ratings of technical difficulty), and expressive difficulty (using performers' ratings of expressive difficulty) to listeners' and performers' pupil diameter. 

Our results show stimulating effects of bodily effort and expressive difficulty on performers' pupil diameters, and stimulating effects of expressive difficulty on listeners' pupil diameters. We also found negative effects of musical complexity on both performers and listeners, and negative effects of performers' bodily effort on listeners, which may reflect the complex relationships that these features share with other aspects of musical structure. The performers showed differences in average pupil diameter across the four pieces that they played during the concert. These differences were particularly notable for the violinists, who exchanged places halfway through the concert: both violinists showed more dilated pupils during their turns as 1st violinist than when playing as 2nd violinist. This finding aligns with earlier anecdotal evidence that 1st violinists (who traditionally have a leading role in a quartet) experience heightened cognitive arousal. In conclusion, our study shows how performers' and listeners' attention fluctuations form an irregular rhythm that shares a complex relationship with musical structure, yet is predictably informed by features of body motion and expressivity. This study also presents an innovative example of how pupillometry and motion capture techniques can be used in combination to evaluate cognitive processing in ecological conditions.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9342%20bishop.mp4?vrtx=view-as-webpage;
99;3;10;Leigh VanHandel, Zachary Lookenbill, Gerardo Lopez and J. Devin McAuley;Rhythmic complexity and tempo anchoring effects in tempo determination;"Tempo
Rhythmic complexity
Rhythm
Hypermeter";"Background
Tempo research has concentrated on preferred tempo, tempo memory, and tempo identification/discrimination. However, there has been very little research on the mental process I call tempo determination, or the active process of determining an appropriate tempo based on melodic, rhythmic, and harmonic cues.


Aims
This project investigates the role of rhythmic characteristics such as syncopation, rhythmic variability, and density in the tempo determination process and on perceived complexity. The hypothesis based on prior research was that participants would determine a slower tempo for more complex stimuli and a faster tempo for less complex. 


Methods
Rhythmic stimuli were both created for this study and drawn from previous work on rhythmic complexity (Povel and Essens 1985, Essens 1995), and were systematically varied by the amount of syncopation, rhythmic variability, and density (Patel and Daniele 2003; Gomez et. al 2005; Eerola et. al 2019). In addition, we used existing rhythmic complexity measures (Thul and Toussaint 2008; Shmulevich and Povel 2010; Hoesl and Senn 2018) to ensure stimuli represented a range of complexity based on their criteria. 

Subjects were assigned to a Fast (150bpm, 400ms IOI) or Slow (75bpm, 800ms IOI) starting tempo condition. They heard a repeating rhythmic pattern and manipulated the tempo in real time using a spin wheel. Each subject heard each rhythmic patterns with no metrical context (the No Context condition) and alternating with four isochronous pulses to establish a metrical context (the Context condition). Their task was to determine the appropriate tempo for each stimulus, and then to tap along with the rhythm at their determined tempo to clarify sub- or hypermetrical processing. Participants also provided a perceived complexity rating for each rhythmic pattern. 


Results
Perceived complexity did not have an effect on tempo determination, leading to a rejected initial hypothesis. Starting tempo condition and the characteristic of density showed significant effects: starting tempo was the primary predictor of final tempo, and rhythms with higher densities resulted in slower overall tempo selections. Other findings show that the slower starting tempo condition resulted in a wider range of determined tempos, and that there are effects of musical training on whether subjects tapped hypermetrically.


Conclusions and Implications
This experiment is part of a larger project that previously demonstrated that melodic characteristics have a significant overall effect on tempo determination. The results here suggest an intricate interrelation between rhythm and tempo determination, mainly that appropriate tempo appears to be governed by context rather than by a pattern’s rhythmic characteristics. The research has important implications for performance practice, pedagogy, and interpreting hypermetrical hearing.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9399%20VanHandel.mp4?vrtx=view-as-webpage;
65;3;11;Min Li, Dominic Ward, Ryan Stables, Chua, Howe, Quinn and Alan Wing;Synchronising with a violin duet;"Synchronisation
Timing
Ensemble
Modelling";The control of relative timing is evident in many social activities including synchronisation in music performance. The Linear Phase Correction Model of musical ensemble (Wing et al., 2014) suggests each player in a string quartet corrects the timing of their next note in proportion to the asynchrony with fellow players on the previous note. To examine melodic and rhythmic influences on this correction in a live musical setting, 12 participants played a violin melody part in synchrony with a violin duet who played the same melody and an accompaniment pitched below the melody. In half of the trials, the duet’s melody was played live, and in the other half it was a recording, which was not made explicit to the participants. In both cases, the accompaniment was always a recording. The timing structure of the duet was either simple (both parts in 2/4 time), or complex (melody in 2/4 and accompaniment in 6/8 time). After each trial, participants rated the perceived influence of melody and accompaniment on the timing of their playing. Timing performance, measured using note acoustic onsets, revealed higher correction gains in the linear phase correction model with the melody than with the accompaniment, and the effect was more apparent in the complex timing condition. Complementing this finding, ratings of perceived influence indicated greater influence of the melody than the accompaniment, especially in the simple timing condition. There was no effect of whether the melody was live or recorded. These patterns of correction and perceived influence indicate players take correction cues from the score part that is more similar to their own in terms of melody and rhythmic structure.;;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9365%20li.mov?vrtx=view-as-webpage;
110;3;12;Signe Hagner, Cecilie Møller and Peter Vuust;A bodily hierarchy of rhythm performance – effects of musicianship;"Rhythm production
Perception/action
Prediction
Coordination";"Entraining to a musical beat by means of body movement is a way to reinforce the metrical model predicted by the brain. These body movements assist in maintaining and exercising the underlying beat in a non-isochronous sequence. As such, they play a facilitatory role, creating a figure/ground relationship between the auditory rhythms and the metric body movements – a relationship between rhythm and meter also found in music theory. Moving to the beat not only takes place when we listen to music, but also when we produce it. Performing figure and ground simultaneously is a complicated action that involves coordination of different limbs. In the current study, we studied the dynamically changing roles of voice, hands and feet during simultaneous performance of rhythm and beat. 60 right-handed professional musicians, amateur-musicians and non-musicians each performed three short rhythms and their underlying beat in twelve different combinations of hands, feet and voice. 
	Results suggested the existence of a bodily hierarchy with the levels: (5) voice --> (4) right hand --> (3) left hand --> (2) right foot --> (1) left foot, indicating that participants preferred to perform the rhythm using a higher order limb than the limb keeping the beat. Importantly, the preferred roles of the limbs in the bodily hierarchy differed according to combination: While it was easier for them to use a hand to keep the beat while vocalizing a rhythm than doing the opposite, participants preferred to use a hand for performing the rhythm when combining it with a foot, which in that case had to stamp the beat. While performance generally increased with expertise, the hierarchical pattern was consistent in all three expertise groups. However, musicians were less affected by performing against the hierarchy than non-musicians. 
	The notion of a bodily rhythm/meter hierarchy between voice, hands and feet raises questions about our understanding of the interaction and lateralization of rhythm and meter in the brain. It shows that the conduct-support role of the different limbs is key in the coordination between voice and body; performance level of the same limb combination can differ considerably, depending on which limb takes the supporting role of the meter and the conducting role of the rhythm. Furthermore, results show how perception influences action by indicating that to musicians, the robustness of their metric prediction model transforms the task of combining rhythm and meter into one unified action where the meter is the implied ground of the rhythmic figure. To the non-musicians with less robust prediction models, the figure/ground relationship between the rhythm and its metrical model is not as well-established, making it a harder task of coordinating two equal components.";https://youtu.be/w6wkhDmDReM;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%93110%20hagner.mov?vrtx=view-as-webpage;
98;3;13;Tristan Loria, Aiyun Huang, Tara Henechowicz and Michael Thaut;Kinematic motion analysis of marimba performance: Rhythmic and spatial control of sound producing movements in experts and trainees;"motor control
motor skills
percussion
expert
novice";"Sound producing movements are often refined through years of training. Within a percussion context, a wide range of rhythmic motion in the shoulders, elbows, and hands is required. The present study sought to determine how the execution of sound producing movements differed between expert and trainee percussionists to shed light on temporal and spatial mechanisms underlying percussion performance (e.g., Eriksen et al., 2018). Fifteen participants took part in the study. The expert group (n = 3) were professional musicians and studio teachers with an average of 20 years of percussion experience. The trainee group (n = 12) had 8 years of percussion experience and were music students majoring in percussion in the Faculty of Music at the University of Toronto. Both groups played the first eleven measures from the second movement of J.S. Bach’s Sonata No. 1 in G Minor on marimba using four mallets at 72 bpm. Motion capture equipment measured the displacement of various limb segments including the shoulders, elbows, and hands during the performance at a sampling rate of 100 Hz (see Cutti et al., 2005; Murray, 1999). To examine movement similarities and differences across the performance, rhythm production and spatial motor outcomes were analyzed. Rhythm production was assessed by analyzing the velocity and acceleration profiles of the hands, whereas spatial motor outcomes were assessed via principal components analysis which extracted movement characteristics of the various limb segments that were critical for sound production (e.g., Gonzalez-Sanchez et al., 2019). Regarding rhythm production, hand movements of experts were executed at a greater overall velocity than trainees. Despite this, precise rhythmicity and oscillatory hand movements were preserved. That is, the velocity of expert hand movements was characterized by clusters of similar oscillatory movements separated by high amplitude changes in both velocity and acceleration resulting in internal rhythmic stability, whereas the trainees demonstrated continuous and less internally synchronous velocity profiles. These temporal rhythm production findings impacted spatial motor outcomes. Indeed, limb segment displacement showed that in experts, movements of the right shoulder depicted abduction to bring the shoulder towards the playing surface while extending the right elbows to a position near the proximal edge of the instrument when executing sound producing movements. In contrast, the trainee’s angled the right shoulder away from the playing surface and positioned the right elbow proximal to the torso during the performance (i.e., away from the playing surface). Critically, expert performance was characterized by movement coupling between the shoulders and elbows whereas trainee performance was associated with movement coupling between the elbows and hands. These unique control strategies were hypothesized to result from the different timing structures of hand movements between groups. More specifically, the enhanced internal rhythmic timing of oscillatory hand movements in experts yielded functional changes in limb displacement and coupling compared to trainees, thus highlighting temporal rhythmic control as a potential underlying mechanism to shape spatial motor sound production. Such findings may be applied towards enhancing approaches to motor learning in percussion to facilitate temporal and spatial control of developing motor skills.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9398%20loria.mov?vrtx=view-as-webpage;
93;3;14;Valentin Begel, Alexander P. Demos and Caroline Palmer;Delay-coupled modeling of spontaneous rate differences in turn-taking synchronization in social contexts;"Rhythm
synchronization
Entrainment
Group performance
Nonlinear dynamics";"Previous studies have shown large individual differences in how accurately people synchronize their movements with rhythmic sound, as well as differences in their spontaneous tapping rates. Less is known about how social turn-taking contexts affect synchronization. Knowledge of a partner's synchronization ability could help or hurt future synchronization, as perceiving another person's ability may influence one’s own performance. We address in a turn-taking context whether individuals' synchronization abilities change when they have knowledge of a partner's synchronization abilities. We used a unidirectional delay-coupling model to capture anticipatory synchronization as individuals synchronized melodies with a metronome, in the presence and absence of a partner. The model allows us to capture the degree of information coupling each partner gains from exposure to their partner while controlling for intrinsic frequency differences. 
24 participants (12 nonmusicians and 12 musicians) first produced a musical scale at a spontaneous (uncued) rate. Then they synchronized the musical scale with an auditory metronome set to their uncued rate or to their partner's rate (Solo task). In a Turn-taking task, two partners took turns every 8 beats, synchronizing the musical scale with a metronome cue set to their uncued rate or to their partner's uncued rate. Behavioral results indicated that in the Turn-taking condition only, all participants showed more asynchrony when the metronome was set to their partner's rate than to their own rate. On average, musicians showed smaller asynchronies than nonmusicians, but this difference was not significant. 
A delay-coupling model, with intrinsic frequencies set to each partner’s Solo rate and only the coupling strength (k) parameter allowed to vary, was fit to the tapping asynchronies. Based on best-fitting models (smallest RMSE), the time delay was fixed at 19 milliseconds across participants (representing synaptic transmission rates). As predicted, modeling revealed that when participants tapped at their own rate, k was always 0, meaning that no coupling strength was necessary. When participants tapped at their partner's rate, k increased significantly. These patterns held across Solo and Joint conditions. Also as expected, k increased as mean asynchronies decreased when participants tapped at their partner's rate. Partners with considerable differences in uncued rates exhibited greater coupling when they tapped at their partner's rate, further validating the role of coupling. As in the behavioral results, coupling tended to be higher for musicians than for nonmusicians, but this difference was not significant. 
Overall, participants had more difficulty synchronizing with the metronome when it was set to their partner's rate. The delay-coupling model captured individual differences in coupling strength due to these intrinsic frequency differences. Model implementations offer tests of how individual differences related to intrinsic frequencies impact synchronization in a social context and might provide an explanation for differences in social bonding.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9393%20begel.mp4?vrtx=view-as-webpage;
63;4;1;Aleksander Tidemann, Olivier Lartillot and Mats Johansson;Interactive tools for exploring performance patterns in hardanger fiddle music;"Computational music analysis
Interactive research tools
Traditional Scandinavian folk music
Hardanger fiddle performance patterns
MIRAGE research project";"Analyzing musical performances is a challenging and emergent field of computational music research, aiming to reveal performance patterns and link them to musical contexts (Lerch et al., 2020). There exists a modest amount of computational research on traditional Scandinavian fiddle performances, exhibiting a great deal of rhythmic and temporal variability despite the music's intimate relationship with various traditional dances (Johansson, 2017, p.58; Waadeland, 2000).

The MIRAGE research project is currently contributing to this scientific body utilizing a transdisciplinary approach. A way to mitigate challenges inherent to transdisciplinary approaches is to develop tailored technologies that seek to increase the availability of expertise and knowledge across disciplines.

By focusing on how we can design interactive computational tools to investigate performance patterns in traditional Norwegian folk music, this study aims to increase the availability of computational music research in the field of musicology. A toolkit has been prototyped, consisting of 2-3 standalone applications that explore the intricate timing patterns of Hardanger fiddle music. The applications look at how timing patterns of repeating motifs evolve throughout performances and feature abilities to investigate various musical properties (such as pitch, metrical position and dynamics) of repeating timing patterns. Additionally, several data visualization techniques are considered, such as an interactive and adjustable score representation of the performance and dynamic plotting capabilities.

The toolkit is currently under development and guided by continuous feedback from experts musicologists, a dialogue valuable for the eventual systematic evaluation of the tools. The project's current state suggests that a combination of object and visually-oriented programming environments, such as Max8 and JavaScript, prove optimal for building complex and interactive computational tools for exploring musical performance patterns. Preliminary results also seem to support previous research by indicating the conservation of timing profiles throughout occurrences of motivic patterns in Hardanger fiddle performances (Johansson, 2019, p.5). 

References

Johansson, M. (2017). Empirical Research on Asymmetrical Rhythms in Scandinavian Folk Music: A Critical Review. Studia Musicologica Norvegica, 43 (01), 58–89. doi: 10.18261/issn.1504-2960-2017-01-05

Johansson, M. (2019). Timing-sound interactions in traditional Scandinavian fiddle music: Preliminary and implications. EasyChair Preprint, 5.

Lerch, A., Arthur, C., Pati, A., & Gururani, S. (2020). An Interdisciplinary Review of Music Performance Analysis. Transactions of the International Society for Music Information Retrieval, 3(1), 221–245. https://doi.org/10.5334/tismir.53

Waadeland, Carl Haakon 2000. Rhythmic movements and movable rhythms. Syntheses of expressive timing by means of rhythmic frequency modulation. Doctoral dissertation. Department of Musicology, Norwegian University of Science and Technology, Trondheim.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9363%20tidemann.mov?vrtx=view-as-webpage;
79;4;2;Anastasiya Paltarzhitskaya;Perception of music duration: the effect of familiarity;"time perception
music
familiarity";"Introduction
Subjective perception of time is an exciting research topic involving several cognitive functions. We hypothesized that familiarity of music has a temporal dimension: 1) the duration of familiar musical pieces is readily assessed by the brain, which is a predicting machine (Friston and Kilner, 2006; Friston, 2010), while 2) mental processing of unfamiliar music could slow down perception of time because of the computational resources needed for their memory trace formation (Matthews & Meck, 2016; Pan & Luo, 2012).

Materials and methods 

The experiment consisted of two parts. During the first part, 31 participants (19 males; average age 25.06) were asked to compare the subjective duration of two consecutive musical pieces (randomly selected; each 10 s long) and report which one was longer. The task for the second part was to appraise the extent to which the tunes were familiar to the participants by using a scale from 1 to 4.  The tunes from children's cartoons and nursery rhymes were chosen as more familiar pieces, and sequences from solfeggio textbooks (based on notes from Ladukhin's (1980) and Friedkin's (1973) textbooks) served as unfamiliar ones. All melodies were balanced by major and minor, tempo, tonality, complexity and volume. 

Results

We found that the second piece tended to be perceived as a longer one, but only when unfamiliar music was presented first, that is, for the unfamiliar–unfamiliar and unfamiliar–familiar pairs.This result confirms the phenomenon known in the literature as time-order error (Hairston, 2007). But no such effect was found to be significant for the familiar–unfamiliar and familiar–familiar pairs. This finding suggests that the comparison of two pieces is affected by the higher processing demands needed to manipulate un unfamiliar piece of music in working memory.  

Conclusion

We conclude that working memory content (familiar versus unfamiliar music) affects perception of time. In addition, a very interesting point is the role of emotions in this time process (Noulhiane, Mella, Samson, Ragot, & Pouthas, 2007), because music often causes emotion experience.

1.	Friston, K. (2010). The free-energy principle: a unified brain theory?. Nature reviews neuroscience, 11(2), 127.
2.	Hairston, I. S., & Nagarajan, S. S. (2007). Neural mechanisms of the time-order error: an MEG study. Journal of Cognitive Neuroscience, 19(7), 1163-1174.
3.	Friston, K., Kilner, J., & Harrison, L. (2006). A free energy principle for the brain. Journal of Physiology-Paris, 100(1-3), 70-87.
4.	Matthews, W. J., & Meck, W. H. (2016). Temporal cognition: Connecting subjective time to perception, attention, and memory. Psychological bulletin, 142(8), 865.
5.	Noulhiane, M., Mella, N., Samson, S., Ragot, R., & Pouthas, V. (2007). How emotional auditory stimuli modulate time perception. Emotion, 7(4), 697.
6.	Pan, Y., & Luo, Q. Y. (2012). Working memory modulates the perception of time. Psychonomic Bulletin & Review, 19(1), 46-51.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9379%20paltarzhitskaya.mp4?vrtx=view-as-webpage;
69;4;3;Daniel Cameron, Chantal Carrillo and Laurel Trainor;Does the relationship between rhythmic complexity and groove change over development or with expertise?;"Groove
Complexity
Syncopation
Development
Expertise
Dance";"The extent to which listeners are inclined to move to a musical rhythm (groove) depends on how complex (e.g., syncopated) the rhythm is. In adults, the groove-syncopation relationship is thought to have an inverted-U shape, in which listeners want to move most to rhythms that have some syncopation—but not too much. However, we do not know whether the groove-syncopation relationship is fundamental and fixed vs. experience-dependent.

In two sets of online experiments using the drum break patterns (rhythms) developed and used previously by Maria Witek (2014), we tested whether the relationship between groove and syncopation differs between 1) children and adults, 2) dancers and non-dancers, and 3) dancers of different styles (e.g., ballet, jazz, contemporary, and hip-hop).

We tested 5- and 6-year-old children on two tasks. The first was a 2-alternative-forced choice task in which participants compared rhythms from 2 of 3 levels of syncopation (low, medium, and high) and decided which rhythm in a pair was better for dancing. To engage children, rhythms were associated with images of cartoon animals drumming. In a second task, the same children heard longer versions of the same rhythms and were instructed to dance along. Videos of dancing were rated for the extent of movement. Between the two tasks, we are able to assess—perceptually and behaviourally—whether children have a reliable relationship between groove and syncopation, and whether or not the perceptual relationship differs from that of adults. To allow comparison to adults, we tested a sample of adults online in a 2AFC task using the same subsets of stimuli (results showed the expected inverted-U relationship).

In the second set of experiments, trained dancers and non-dancers completed the same task used previously (Witek, et al., 2014): rating groove for 50 rhythm which ranged in syncopation from very low to very high. However, rather than rating each rhythm on groove and enjoyment, the dancers rated two types of groove: how much the rhythm made them want to move a) in a general way and b) in the specific style of dance with which they are most familiar. Each participant’s groove ratings provide a measure of optimal syncopation (peak of the quadratic fit). At the time of submitting this abstract, data collection is ongoing. Due to extensive experience moving to music, dancers may have a higher tolerance for syncopation (higher groove ratings and higher optimal syncopation) than non-dancers. And, because dance styles differ in their requirements for movements locked to the beat and the extent of syncopation in the accompanying music, we expect that optimal syncopation for groove will depend on dancers’ specific style of expertise, and on whether they are rating general groove vs. urge to dance in their style of expertise.

Together, these studies test the extent to which experience influences how the perception of groove is related to rhythmic complexity.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9369-cameron.mp4?vrtx=view-as-webpage;
106;4;4;Emily A. Wood, Dobromir Dotov, Andrew Chang, Dan Bosynak, Lucas Klein and Laurel Trainor;Group synchrony increases but information flow decreases as a string quartet learns unfamiliar music together;"Interpersonal coordination
Group performance
Nonverbal communication
Social cognition
Joint action
Kinematics
Granger causality
Cross-correlation";"Ensemble musicians must anticipate their partners’ actions to coordinate playing a piece together. One way musicians achieve this is by attending to sensorimotor signals in their partners’ body sway movements. Indeed, a musician’s body sway movements reveal their upcoming intent regarding phrasing, tempo, and dynamics, which helps their partners anticipate how and when to play next. 

In our lab, we have measured the body sway of expert musicians in small ensembles with motion capture. One way to analyze these data is with cross-correlation (CC), which describes the similarity, or synchrony, between the body sway time-series of interacting group members. Synchrony between musicians’ body sway is related to how well they coordinate together in difficult playing conditions1,2. However, this method does not capture the richness of interaction dynamics, during which musicians constantly adapt to and influence each other. Another method that considers bidirectional influence is Granger causality (GC), which provides a measure of information flow from one time-series to another. We have used GC in small ensembles to show that information flow was greater from assigned leaders to assigned followers than vice versa3, and that group information flow was greater when musicians played with emotional expression than without4. Here, we show how these body sway interactions change in an ensemble that learns to play unfamiliar music together. 

A professional string quartet played two unfamiliar pieces of music together eight times in succession while body sway motion data was recorded. The average CC and GC values between the body sway trajectories of each pair in the group on each trial were taken as measures of group synchrony and information flow, respectively. Linear mixed-effect modelling showed that group synchrony increased across trials, indicating that coordination improved as the group gained familiarity with the pieces. In contrast, group information flow decreased across trials, suggesting that the group relied on body sway to help them play together when the pieces were most unfamiliar, but this reliance decreased as they gained familiarity with playing the pieces together. In addition, group synchrony, but not information flow, was related to the quality of the performances. We are currently analyzing how our measures change within trials, such as during periods of expressive vs. steady timing. Overall, our studies show that body sway reflects nonverbal communication in musical ensembles.

References

1.	Keller, P. E. & Appel, M. Individual differences, auditory imagery, and the coordination of body movements and sounds in musical ensembles. Music Percept. 28, 27–46 (2010).
2.	Goebl, W. & Palmer, C. Synchronization of timing and motion among performing musicians. Music Percept. 26, 427–438 (2009).
3.	Chang, A., Livingstone, S. R., Bosnyak, D. J. & Trainor, L. J. Body sway reflects leadership in joint music performance. Proc. Natl. Acad. Sci. U. S. A. 114, E4134–E4141 (2017).
4.	Chang, A., Kragness, H. E., Livingstone, S. R., Bosnyak, D. J. & Trainor, L. J. Body sway reflects joint emotional expression in music ensemble performance. Sci. Rep. 9, 1–11 (2019).";https://youtu.be/68jZDXfM1o4;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%93106%20wood.mp4?vrtx=view-as-webpage;
37;4;5;Haley Kragness and Laura Cirelli;Tiny Dancers: Investigating auditory-motor development in home-recorded musical movements;"development
dance
auditory-motor
entrainment
groove
familiarity";"While spontaneous musical movements emerge in infancy and often co-occur with expressions of joy (Zentner & Eerola, 2010), it is not until later childhood that children are able to flexibly match their movements to the beat. Most investigations of children’s auditory-motor development occur under highly-controlled laboratory conditions, in which children are asked to tap or clap along to experimenter-selected stimuli. Such testing conditions may be too restrictive, and reduce motivation for motor engagement. In two studies, we investigate the influence of (1) song familiarity and (2) groove on children’s spontaneous motor and affective responses to music in the comfort of their own home. 

In both studies, parents video-recorded their children at home while they listened to an experimenter-created playlist. Parents could encourage children to dance before the music began, but were asked to refrain from speaking or dancing themselves during the music. In Study 1, 1.5- to 3-year-old children listened to their favorite song (as indicated previously by the parent) and an unfamiliar genre-matched song. A 1-min excerpt of each song played at 90, 120, and 150 bpm (pitch constant; order randomized). Data collection is nearly complete (current N = 69, target N = 75). In Study 2, 3- to 6-year-old children listened to a “high” and a “low” groove song (Janata et al., 2012). A 1-min excerpt of each song was presented at 120 and 170 bpm (order randomized). Data collection is nearly complete (current N = 70, target N = 80).

Trained coders watched the videos (sound off) and indicated when the children moved rhythmically, the tempo and energy of their movements, and positive affect. Preliminary results from a random subset of 8 participants from each study (to avoid the pitfalls of “data peeking”), indicate condition-level differences in both affective and motor responses. In Study 1, both movement and positive affect were more common in response to familiar songs. Tempo flexibility was apparent in both familiarity conditions. In Study 2, children moved more, moved with greater energy, and demonstrated more positive affect during high-groove excerpts. Results from a full sample will be presented, as well as analyses of movement tempo. 

These preliminary results suggest that both song familiarity and groove affected children’s movements and affective responses. Results point to the importance of both social and acoustic factors in children’s early motor responses to music, and suggest a robust association between musical movement and enjoyment in early childhood. Further, they demonstrate great promise for home-based methods investigating auditory-motor development, which could enable investigation of questions such as cross-cultural differences and child-sibling/child-parent interactions in musical movements.    

Note: We would like this submission to be considered for either a talk or a poster.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9337%20kragness%20tiny%20dancers.mp4?vrtx=view-as-webpage;
22;4;6;Jeremy Marozeau and Tanmayee Pathre;The effect of rhythmical pattern in the musical preference of cochlear implant users.;"Cochlear implant
Hearing loss
Music appreciation
emotion";"Most of us can find enjoyment in listening to a sad song. Although we can easily identify that a slow piece in minor mode conveys a sad emotion, this emotional message is uncorrelated with our appreciation of the music. In a recent study (Vannson et al., 2015, Trends in Hearing), we have identified a group of participants that did not seem to enjoy sad pieces: Cochlear Implant users.

The cochlear implant is a medical device developed to help people with severe-to-profound hearing loss regain the ability to perceive speech. However, this device is not well suited to convey musical information (Marozeau et al., 2014, Acoustica Australia). Different studies have outlined the limitations of the cochlear implant, CI, to transmit pitch, harmony, mode, and chord progressions. Despite these difficulties, our study has shown that CI users can discriminate between happy songs and sad songs. Given this lack of tonal perception, it would have been expected that CI listeners base their emotional judgment only on the tempo. However, statistical analysis revealed only a weak effect of this musical parameter. More surprisingly, the CI users showed a clear preference for the fast pieces in major mode instead of the slow pieces in a minor mode. In the following study, we are trying to understand how CI listeners can extract emotional information and why they seem to enjoy more happy pieces. 

Twelve normal-hearing, NH, listeners were asked to rate modified versions of the stimuli from the study of Vannson et al. In this latter study, the stimuli were short piano pieces composed to induce a specific emotion (Vieillard et al., 2008, Cognition and Emotion), while in the current study; the same pieces were performed on two congas. Consequently, NH listeners had access to only purely temporal information to form their emotional judgment.  Results show that the emotional and preference ratings of the congas pieces from NH listeners were very similar to the piano pieces' ratings from CI listeners. As the tempo only explains a small part of the variability, other temporal aspects such as the rhythmic pattern, syncopation, and pulsation were used to model the music emotion judgment. We could then conclude that although CI users have difficulties perceiving modes, they can rely on subtle temporal cues to extract music's emotional context.
We also argue that without harmonic information, musical enjoyment will rely mostly on the rhythmical structure of the piece.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9322%20marozeau.mp4?vrtx=view-as-webpage;
74;4;7;Jorg De Winne, Paul Devos, Marc Leman and Dick Botteldooren;Rhythm as a way to entrain people’s attention;"Rhythm
Entrainment
Attention
EEG
Audiovisual interaction";"Artificially intelligent (AI) systems evolve tremendously fast and get better every day. However, when used to interact with people, they still lack the degree of engagement and entrainment that characterizes human-human interaction. Direct access of the AI system to the brain functions of the person it is interacting with could possibly bridge this gap.  Rhythm could be of a major importance in these advances, as it is known that rhythmic signals, as well as music, are expected to give a strong human interaction. 
This study is a first step towards such an engaging and activating system and investigates the underlying processes that happen in the brain during audio-visual interaction. We hypothesize that a rhythmic sound can stimulate visual attention focusing more than a non-rhythmic sound. This better focused attention allows us to also consider that rhythm engages working memory storage. 
An experiment was conducted with 42 young adults (21 female), age 18 to 30 (M= 23.6, SD = 2.7). All of them were having or had had some form of higher education. The experiment started with a questionnaire, an audio-visual object classification task to determine the dominant modality and a standard oddball paradigm with both auditory and visual stimuli. These were followed by the main part of the experiment. 
Participants were presented with a sequence of five black target digits and five dark grey distractor digits mixed together in a random order, and with random timing for the distractor digits. They were asked to remember and report the black target digits. The color difference between black and dark grey was set to be just noticeable for every individual participant. In total 120 sequences were presented over four different conditions. A first condition presented the target digits with random interstimulus intervals (ISIs). A second condition presented the target digits with a fixed ISI, and preceded by five visual induction stimuli. The third condition again presented the target digits with random ISIs, but an auditory tone was presented concurrently with the targets. The last condition presented the target digits with fixed ISIs, preceded by five visual induction stimuli. In this condition both induction and target stimuli were presented with concurrent auditory tones. The induction stimuli induce the participant with a clear underlying beat, allowing him/her to make an accurate prediction on when the next target will appear. Based on our hypothesis, this should help focusing attention to the specific time of the target symbol. 
Together with behavioral responses, 64-channel EEG data was recorded during the experiment. The first preliminary results show some interesting trends. In the EEG response we observe strong inter-individual differences depending on whether the stimuli are presented with or without an underlying rhythm.  The lack of an overall mean effect indicates that personal characteristics have their influence and that we possibly deal with responders and non-responders. More in-depth analysis will investigate these personal factors that may make one more rhythm sensitive, and thus their effect on attention focusing and working memory storage.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9374%20de%20winne.mp4?vrtx=view-as-webpage;
91;4;8;Mari Romarheim Haugen and Anne Danielsen;Influence of tempo on non-isochronous note duration pattern in a performed samba groove;"Samba
Non-isochronous subdivisions
Tempo
Lower limit of duration
Groove
Performance";"Many musical styles are characterized by their non-isochronous (uneven) duration patterns at subdivision level (e.g., Benadon, 2006; Collier & Collier, 2002; Gerischer, 2006; Gouyon, 2007; Haugen and Godøy, 2014; Moelants, 2011; Naveda, 2011; Polak, 2010). It has been suggested that tempo might affect the relationship between such non-isochronous subdivisions. For example, some studies have found that non-isochronous duration patterns seem to become more even as tempo increases. It has been suggested that this effect might be due to perceptual constraints producing a lower limit for duration (e.g., Friberg and Sundström, 1997) or a reduction of rhythmic categories when two initially uneven short notes become so similar in fast tempi that they merge into one single category (Clarke, 1985). 

Previous rhythm studies of samba have revealed that a non-isochronous duration pattern on sixteenth note level is a prominent feature of the style. In the present study, we aim to investigate the influence of tempo on this non-isochronous duration pattern in a performed samba groove.

A percussionist and a dancer from São Paulo in Brazil participated in the study. The percussionist played a samba groove on a Brazilian hand drum called pandeiro and the dancer performed the dance in samba no pé style. The samba groove was performed at three different tempi: fast (133 bpm), preferred (100 bpm), and slow (69 bpm). The results are based on an analysis of the recorded sound. The temporal positions of the played sixteenth notes were detected using the MIRtoolbox (Lartillot and Toiviainen, 2007). 

The analysis showed the same non-isochronous duration pattern at sixteenth note level in all tempi, namely a systematic medium/long–short–medium/short–long duration pattern. That is, the rhythmic categories did not merge as a result of increase of tempo. All four rhythmical categories of subdivision are maintained, despite becoming very similar in duration in the fast tempo. Interestingly, we found that the second sixteenth note becomes relatively shorter and the fourth relatively longer as the tempo increases. This is in contrast with previous research which suggests that non-isochronous duration patterns become more even as tempo increases. In the fast tempo the short second sixteenth note was only 68 ms long, which is below the 80–100 ms range previously suggested as a lower limit for duration in other genres (Dittmar et al., 2015; Polak, 2017).

The results suggest that the non-isochronous duration pattern in samba becomes more pronounced when the tempo increases. We explain this by a need to maintain the specific samba ‘feel.’ The results also imply that the pattern has to be consistent at a categorical level across tempi: the groove pattern is a pattern of duration categories (long, short, etc.) rather than fixed percentages or ratios.  Ultimately, our results imply that humans are capable to differentiate between very similar durations and that even very fine-meshed rhythmical patterns can be perceived and reproduced with high accuracy, depending on the musical context.

References 

Benadon, F. (2006). Slicing the beat: Jazz eighth-notes as expressive microrhythm. Ethnomusicology, 50(1), 73–98.
Clarke, E. F. (1985). Structure and Expression in Rhythmic Performance. In P. Howell, R. West, & I. Cross (Eds.), Musical structure and cognition (pp. 209–236). London: Academic Press.
Collier, G. L., & Collier, J. L. (2002). A Study of Timing in Two Louis Armstrong Solos. Music Perception: An Interdisciplinary Journal, 19(3), 463—483. 
Dittmar, C., Pfleiderer, M., & Müller, M. (2015). Automated estimation of ride cymbal swing ratios in jazz recordings. Paper presented at the Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR), Málaga, Spain.
Friberg, A., & Sundström, A. (1997). Preferred swing ratio in jazz as a function of tempo. Speech, Music, and Hearing: Quarterly Status and Progress Report (TMH-QSPR), 4, 19–27. 
Gerischer, C. (2006). O suingue baiano: Rhythmic feeling and microrhythmic phenomena in Brazilian percussion. Ethnomusicology, 50(1), 99–119. 
Gouyon, F. (2007). Microtiming in “Samba de Roda” – Preliminary experiments with polyphonic audio. Paper presented at the SBCM 2007 Proceedings.
Haugen, M. R., & Godøy, R. I. (2014). Rhythmical Structures in Music and Body Movement in Samba Performance. In M. K. Song (Ed.), Proceedings of the ICMPC-APSCOM 2014 Joint Conference: 13th Biennial International Conference for Music Perception and Cognition and 5th Triennial Conference of the Asia Pacific Society for the Cognitive Sciences of Music (pp. 46–52). Seoul, South Korea: College of Music, Yonsei University.
Lartillot, O., & Toiviainen, P. (2007). A Matlab toolbox for musical feature extraction from audio. International Conference on Digital Audio Effects, 237–244. 
Moelants, D. (2011). The Performance of Notes Inégales: The Influence of Tempo, Musical Structure, and Individual Performance Style on Expressive Timing. Music Perception: An Interdisciplinary Journal, 28(5), 449—460. 
Naveda, L. (2011). Gesture in Samba: A cross-modal analysis of dance and music from the Afro-Brazilian culture. (Ph.D. thesis). Faculty of Arts and Philosophy, Ghent University, Belgium, Ghent, Belgium. 
Polak, R. (2010). Rhythmic Feel as Meter: Non-Isochronous Beat Subdivision on Jembe Music from Mali. Music Theory Online, 16(4), 26. Retrieved from http://www.mtosmt.org/issues/mto.10.16.4/mto.10.16.4.polak.html 
Polak, R. (2017). The lower limit for meter in dance drumming from West Africa. Empirical Musicology Review, 12(3-4), 205-226. doi:10.18061/emr.v12i3-4.4951.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9391%20haugen.m4v?vrtx=view-as-webpage;
64;4;9;Matt Moore and Molly Henry;An exhaustive search for twelve-unit metrically ambiguous rhythms;"polyrhythm
metrical ambiguity
bistability
beat grouping
entrainment";"Rhythmic stimuli are used to investigate a broad range of topics in timing and entrainment research. However, these stimuli are often either isochronous or are simple rhythms adhering to 4/4 time, the most prevalent metre in Western music. Simple rhythmic stimuli afford tight experimental and acoustic control. However, there are nevertheless many research questions which cannot be adequately addressed with rhythmically simple and unambiguous stimuli. For example, polyrhythms – two superimposed isochronous pulses with tempos related by a non-integer ratio – possess the crucial property of being metrically ambiguous (a form of bistability). That is, listeners will hear one of two metrical interpretations depending on which isochronous pulse they feel is the ‘beat’ (London, 2012). Metrical ambiguity is a prerequisite for experimental designs that attempt to bias listeners to hear one interpretation or another by varying, for example, context or tempo. As such, polyrhythms have been used to investigate rhythmic grouping behaviours in adults and infants (Jones et al., 1995; Phillips-Silver & Trainor, 2005). Unfortunately, there is an extremely limited pool of strict polyrhythms which are practical for use with non-specialists. 
The current study aimed to equip researchers with a large set of metrically ambiguous rhythms to facilitate more nuanced investigations of rhythmic grouping and metrical bistability. We generated every unique rhythm that can be constructed within a twelve-unit metrical grid (i.e., all possible combinations of eighth notes and rests in 12/8 time, excluding rotations of identical rhythms). Participants, recruited using the Prolific platform, were asked to tap the beat on their keyboards while the rhythms played on a loop. Participants heard a subset of the full set of 351 rhythms presented at both a fast and slow tempo. Rhythms were evaluated in terms of ambiguity and stability. Rhythms were categorised as ambiguous 1) when they were interpreted in different ways by the same participant (by-participant ambiguity), or 2) when they were interpreted in different ways by different participants but consistently within a participant (population ambiguity). Stability refers to how easily participants were able to find equal-sized beat groupings that evenly divided the twelve-unit grid; additive metre interpretations, whilst potentially interesting, are outside the scope of the present study and were consequently disregarded. Hierarchical clustering yielded eight prevalent groups of rhythms which varied in terms of stability and ambiguity. Application of a machine-learning approach is underway, with the aim of evaluating psychological (Povel & Essens, 1985) and music-theoretical (Lerdahl & Jackendoff, 1983) models of rhythm and metre, comparing their predictions against our own empirical findings.
 
References
Jones, M. R., Jagacinski, R. J., Yee, W., Floyd, R. L., & Klapp, S. T. (1995). Tests of attentional flexibility in listening to polyrhythmic patterns. Journal of Experimental Psychology: Human Perception and Performance, 21(2), 293–307. https://doi.org/10.1037/0096-1523.21.2.293
Lerdahl, F., & Jackendoff, R. (1983). A generative theory of tonal music. MIT Press.
London, J. (2012). Hearing in time: Psychological aspects of musical meter (2nd ed). Oxford University Press.
Phillips-Silver, J., & Trainor, L. (2005). Feeling the beat: Movement influences infant rhythm perception. Science, 308, 1430.
Povel, D.-J., & Essens, P. (1985). Perception of Temporal Patterns. Music Perception: An Interdisciplinary Journal, 2(4), 411–440. https://doi.org/10.2307/40285311";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9364%20moore.mp4?vrtx=view-as-webpage;
28;4;10;Heidi Bliddal, Christian Bech Christensen, Cecilie Møller, Peter Vuust and Preben Kidmose;Neural correlates of beat perception measured using ear-EEG:  Bringing EEG music studies into the concert hall;"Beat perception
Ear-EEG
Polyrhythm";"Ear-EEG is a promising novel technology that records electroencephalography (EEG), from electrodes inside the ear. This allows for a discrete and mobile recording of EEG and makes it possible to record EEG in natural environments (1). Nozaradan et al. (2011)(2) used scalp EEG to study neural responses to an isochronous sequence of sounds under three conditions: a control condition and two imagery conditions where participants were instructed to imagine accents on every second (march) or third (waltz) beat. A significant peak was found at the frequency of the imagined beat only in the matching imagery conditions. Since no physical accents were present in the stimulus, the peaks at meter-related frequencies indicate higher order processing of the sound sequence. The aim of the present combined scalp- and ear-EEG study (n=20) was to determine whether neural correlates of beat perception can be measured using ear-EEG. To investigate this, we used an adapted version of the Nozaradan paradigm, and an additional polyrhythm paradigm. We included both in order to compare the neural correlates of instructed, induced, and spontaneous beat perception. In the polyrhythm paradigm we used an ambiguous 2:3 polyrhythm preceded by no priming or by the same rhythm emphasizing the 2-beat or the 3-beat. Comparing different kinds of beat perception is particularly important here because there could be different underlying neuronal sources depending on the nature of the beat perception. Thus, the scalp EEG might measure some sources that the ear-EEG cannot. At the time of writing data collection was still ongoing. We conducted a pilot study which included two musicians using electrodes around the ear instead of the ear-EEG used in the ongoing data collection. Pilot data using only the electrodes around the ear, obtained with the Nozaradan paradigm, showed a significantly greater amplitude at the march-related frequency in the march imagery condition compared to the control condition for one participant (p <.005). The polyrhythm paradigm showed a significantly greater amplitude at the 3-beat frequency in the 3-beat condition, than the 3-beat frequency in the 2-beat condition in one participant (p< .04). These preliminary results are promising for the possibility of measuring the neural correlates of beat perception with ear-EEG.

References
1. Kappel SL, Rank ML, Toft HO, Andersen M, Kidmose P. Dry-Contact Electrode Ear-EEG. IEEE Trans Biomed Eng. 2019;66(1):150–8.
2. Nozaradan S, Peretz I, Missal M, Mouraux A. Tagging the neuronal entrainment to beat and meter. J Neurosci Off J Soc Neurosci. 13. juli 2011;31(28):10234–40.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9328%20bliddal.mp4?vrtx=view-as-webpage;
83;4;11;Noam Lederman, Simon Holland and Paul Mulholland;A principled approach to the development of drum improvisation skills through interaction with a conversational agent;"Drumming
Improvisation
Mixed-initiative interaction";"Abstract
Shedding is a term used to describe a musical conversation between drummers with the aim to improve their drumming vocabulary, gain confidence in real-time trading of musical ideas, develop an understanding for their original voice on the drum kit and enjoy the process of exploring creativity with a fellow drummer. However, in practice drummers have limited opportunities to play in real-time with other drummers. This research explores shedding activity in the form of mixed-initiative interaction between a human drummer and a conversational agent. 

The proposed agent embodies an inference system allowing it to navigate through transformations of a core phrase chosen by the user (the starting point of every shedding interaction in our model) with a design aim of conversing with the human drummer in a way that is perceived as meaningful, musical and inspiring. The transformations involve the agent taking a core phrase and adapting it in various ways, for example, by making changes to elements such as orchestration, metric modulation and phase shift. These elements offer dimensions of development in linear drumming, where a range of transformations of each element can be explored by the agent and human drummer. This research focuses on creating a reflective drumming agent that inspires the user by having a conversation rather than by teaching specific grooves (Senn, 2018) or drumming concepts. This paper focuses in particular on how this central shedding model can be enhanced and deepened based on a novel characterisation of rhythmic grouping and accent patterns.

Previous research has investigated creativity from a number of perspectives including computer modelling (Boden 1994, Cope 2005), communication (Davidson 2005) and perfection (Berger 1999). However, creativity in performance has received relatively little attention (Pinheiro 2010). More specifically, research into musical interaction activities with intelligent systems such as the Continuator (Pachet, 2003), Controlling Interactive Music (Brown, 2018) and Monterey Mirror (Manaris et al. 2018) present tools for contemporary music creation and co-creativity. However, our work suggests a musical framework with a reflective agent that aims to elicit creativity by encouraging the human drummer to observe and refine their creative process.
 
The inference system employed by the agent uses the core phrase and rules of transformations in order to converse with the human drummer in ways that are perceived as meaningful. Following several design prototyping studies using Wizard of Oz, we were able to refine the initial transformation model with three elements: linear drumming, accented and unaccented notes within rhythmic groupings and external inspiration for core phrases. This enhanced design will be tested in a pilot experiment with professional drummers in order to produce a more refined direction of exploration of drum improvisational skills development through shedding while considering implications of movement, perception and creativity. 




References 
Brown, A. R. (2018). Creative improvisation with a reflexive musical bot. Digital Creativity, 29(1), 5-18.
Green, L. (2002). How popular musicians learn: A way ahead for music education. Ashgate Publishing, Ltd..
Jackendoff, R. (2009). Parallels and nonparallels between language and music. Music perception, 26(3), 195-204.
Manaris, B., Hughes, D., & Vassilandonakis, Y. (2011, June). Monterey mirror: combining Markov models, genetic algorithms, and power laws. In Proceedings of the IEEE Conference on Evolutionary Computation.
Pachet, F. (2003). The continuator: Musical interaction with style. Journal of New Music Research, 32(3), 333-341.
Pinheiro, R. (2010) ‘The creative process in the context of jazz jam sessions’. Journal of Music and Dance. 1(1), pp. 1-5. Available at: http://www.academicjournals.org/journal/JMD/edition/January_2011
Senn, O., Kilchenmann, L., Bechtold, T., & Hoesl, F. (2018). Groove in drum patterns as a function of both rhythmic properties and listeners’ attitudes. PloS one, 13(6), e0199604.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9383%20lederman.mp4?vrtx=view-as-webpage;
50;4;12;Ragnhild Brøvig-Hanssen, Bjørnar Sandvik and Jon Marius Aareskjold-Drecker;Slow attacks, peculiar accents, and pumping grooves: perceptual interaction between timing and intensity in music production;"groove
rhythm
microrhythm
timing
sound
intensity
dynamics
perception
electronic dance music";This paper explores the extent to which the use of dynamic range processing (such as compression and sidechain compression) influences our perception of a sound signal’s temporal placement in music. Because compression reshapes the sound signal’s envelope (how it changes over time), scholars have previously noted that certain uses of compression can produce peculiar rhythmic effects. We seek to interrogate and complicate this notion by linking a description of the workings and effects of dynamic range processing to empirical findings on the interaction between sound and perceived timing. Access to original multitracks and project files for music recordings by esteemed Norwegian EDM producers allow us to surgically analyze their use of amplitude adjustment, compression, and sidechain compression and these effects’ impact on rhythm, including to aurally and visually compare individual tracks with and without sound-processing effects applied. We also analyze audio tracks by international EDM artists in order to demonstrate the broad range of possible applications of this effect. The analyses are supported by technical discussions of the workings of the relevant processing effects and to findings from perceptual experiments that examine multidimensional perceptual processing, including the interaction between time and shape/envelope, and between time and intensity. The analyses of the different EDM tracks demonstrated that especially sidechain compression affects the music in many possible ways, depending on the settings of the compressors’ parameters, as well as the rhythmic pattern and the sonic complexity of both the trigger signal and the sidechained signal. Dynamic range processing’s impact on groove and perceived timing indicates, in line with previous findings, that sound and timing interact in fundamental ways. Because of this interaction, then, we cannot limit ourselves to technical terms that describe how particular effects are achieved if we want to fully understand the grooves that are characteristic of EDM or other music. We must also consider how listeners experience these effects.;;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9350%20brovig-hanssen.m4v?vrtx=view-as-webpage;
32;4;13;Srishti Nayak, Daniel Gustavson, Youjia Wang, Jennifer Below, Reyna Gordon and Cyrille Magne;Validation of a novel speech rhythm perception test: convergence with musical rhythm and language and reading difficulties;"prosody perception
syllable emphasis
speech rhythm
musical rhythm
reading";"Sensitivity to speech rhythms is correlated with both linguistic skills (e.g., reading), and non-linguistic skills (e.g., musical rhythm). Here, we present the Test of Prosody via Syllable Emphasis (“TOPsy”) – and highlight its merits for studying links between speech rhythm, musical rhythm, and literacy in online and lab settings. In this 35-item speech rhythm perception test (adapted from Heggie & Wade-Woolley, 2018), participants identify the syllable carrying the primary emphasis in a multisyllabic word (e.g., “ci-vi-li-ZA-tion”) consisting of 3, 4, or 5 syllables. We present psychometric properties of the TOPsy based on assessment in an internet-based sample (n = 2511; 18-88 years). Participants completed the TOPsy, alongside a musical rhythm test (Ullen et al., 2014). They also answered questions about speech-language clinical histories and self-reported beat synchronization ability. Genetic materials were collected from a subset of participants (n = 1940) via mailed saliva collection kits.

Descriptive analyses show that TOPsy taps variability in speech rhythm perception across the population (mean = 76.23%, min = 0%, max = 100%, SD = 23.01%). Performance on the TOPsy was not significantly correlated with age or sex, as expected. Since results were positively skewed, with a high proportion of participants (~20%) responding correctly to all 35 items, scores were arcsine transformed for analyses. Test items from the TOPsy show excellent internal consistency (alpha = 0.93), with average inter-item correlations within the ideal range (average r = 0.28). That is, items were distinct but consistently measuring the same construct. Consistent with emerging evidence that speech rhythm and musical rhythm tend to covary (Morrill et al., 2015; Hausen et al., 2013), TOPsy scores and and musical rhythm test scores were correlated in our sample (r = 0.38, p <.0001), and a 1-item decrease in TOPsy score was associated with a 91.1% increase in the odds of self-reported inability to clap to a beat (p < .0001). Further, consistent with established links between speech prosody and reading (Holliman et al., 2017; Mundy & Carroll, 2016; Goswami, 2011), TOPsy performance was markedly poorer in individuals diagnosed with the reading-disorder Dyslexia, compared to others. Logistic regression clarified that a 1-item decrease in TOPsy accuracy was associated with a 6.6% increase in the odds of having received a dyslexia diagnosis (p <.0001), and a 1.8% increase in the odds of having  received speech-language therapy (p <.05). 

Given that lower TOPsy scores predict the presence of self-reported Dyslexia diagnosis, future directions include understanding the relationship between TOPsy scores and measures of reading-related abilities such as reading fluency and phonological awareness. Genetic data are also available on this sample, which we plan to use in future work to investigate the genetic underpinnings of speech rhythm and its relationship with musical rhythm, reading, and language abilities. We conclude that TOPsy is a promising tool for studying speech prosody perception, with good internal reliability, external validity, and ease of administration in both online and lab contexts. TOPsy can additionally act as a validated phenotype for future investigations into the biological basis of prosody using genetics and neuroimaging approaches. 


References

Goswami, U. (2011). A temporal sampling framework for developmental dyslexia. Trends in Cognitive Sciences, 15(1), 3–10.

Heggie, L., & Wade-Woolley, L. (2018). Prosodic awareness and punctuation ability in adult readers. Reading Psychology, 39(2), 188–215. 

Holliman, A. J., Gutiérrez Palma, N., Critten, S., Wood, C., Cunnane, H., Pillinger, C. (2017). Examining the independent contribution of prosodic sensitivity to word reading and spelling in early readers. Reading and Writing, 30, 509–521. 

Mundy, I., & Carroll, J. (2016). Which prosodic skills are related to reading ability in adulthood. In J. Thomson & L. Jarmulowicz (Eds.), Linguistic Rhythm and Literacy (Vol. 17, pp. 51–76). John Benjamins Publishing Company.

Ullén, F., Mosing, M. A., Holm, L., Eriksson, H., & Madison, G. (2014). Psychometric properties and heritability of a new online test for musicality, the Swedish Musical Discrimination Test. Personality and Individual Differences, 63, 87–93.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9332%20nayak.mp4?vrtx=view-as-webpage;
62;4;14;Xinyue Wang, Birgitta Burger and Clemens Wöllner;Tempo-manipulated biological motion: From kinematics to perception;"Tempo manipulation
Biological motions
Kinematic features
Tempo judgment";"Background
Slowed down or sped up scenes have been often found to be associated with distortions in human perception. Slow motion excerpts from films, ballets or sports led to duration underestimation and lower emotional arousal (Wöllner, Hammerschmidt, & Albrecht, 2018). It remains unclear how tempo manipulation in relation to changes in kinematic features as shown in point-light displays (PLDs) lead to differences in the perception of human motion. 

Aim
This study attempted to investigate which kinematic features in PLDs are changed, when a) the same movement was performed at different tempi and b) the movement was tempo-manipulated (i.e. accelerated and decelerated to match the other respective tempi). In particular, it was asked how the features contributed to perceived naturalness, perceived passage of time (PoT), and emotional arousal.  

Method
62 participants were recruited worldwide for an online experiment (29 females, 1 gender undisclosed; Mean age = 29.23, SD of age = 8.83). A total of 18 stimuli were synthesized PLDs of three tempi (86, 130, 195BPM) in original and tempo-manipulated versions (i.e. accelerated and decelerated), all in conditions with and without drumbeats at the same paces. Participants were asked to rate the PLDs on several perceptual scales. The procedure was repeated twice. 

Results
One-way ANCOVAs, controlling for the outcome tempo, revealed that the effect of tempo manipulations was significant on the average movement speed (p = .05) and complexity (p = .03). Post-hoc tests revealed that accelerated movements (i.e., manipulated versions) showed significantly higher complexity (p = .03) than the decelerated ones. No statistical difference between the original and tempo-manipulated PLDs were observed in other movement features. 

Linear mixed models indicated the contributions of selected movement features on PoT, arousal level, and the perceived naturalness under the same tempo condition. PoT could be mainly predicted by the fluidity of body movements (p < .001). Emotional arousal and perceived naturalness were affected the most by the average movement speed and fluidity (for all p < .001), followed by movement complexity (arousal: p < .001, naturalness: p = .01) and the variability of jerk (arousal: p = .04).

Conclusion
While the motion features in the PLDs used here exhibited different levels of complexity in accelerated and decelerated performance versions, the tempo manipulated PLDs did not differ significantly from the originals. This might be due to the insufficient variations in the underlying movement. Stimuli of the original slow and fast tempi was rated the most natural and least arousing. The greater the extent of manipulation, the less natural and more arousing it was perceived. Low arousal resulting from high motion fluidity could relate to smooth movements being linked to tenderness (Burger, Saarikallio, Luck, Thompson, & Toiviainen, 2013), whereas its connections to slow PoT and low naturalness call for more investigations.

References
Burger, B., Saarikallio, S., Luck, G., Thompson, M. R., & Toiviainen, P. (2013). Relationships between perceived emotions in music and music-induced movement. Music Perception, 30(5), 517–533. https://doi.org/10.1525/MP.2013.30.5.517
Wöllner, C., Hammerschmidt, D., & Albrecht, H. (2018). Slow motion in films and video clips: Music influences perceived duration and emotion, autonomic physiological activation and pupillary responses. PLoS ONE, 13(6), e0199161. https://doi.org/10.1371/journal.pone.0199161";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9362%20wang.mp4?vrtx=view-as-webpage;
90;4;15;Alexis MacIntyre, Ian Cross and Sophie Scott;Task irrelevant auditory metre shapes visual-motor sequential learning;"Sequential learning
serial reaction time
rhythm
metre";"The ability to learn and reproduce sequences is fundamental to language acquisition, and deficits in sequential learning are associated with developmental disorders such as specific language impairment. Individual differences in sequential learning are usually investigated using the serial reaction time task (SRTT), wherein a participant responds to a series of regularly timed, seemingly random visual cues that in fact follow a repeating deterministic structure. Although manipulating inter-cue interval timing has been shown to adversely affect sequential learning, the role of metre (the patterning of temporal salience) remains unexplored. The current experiment consists of an SRTT adapted to include task-irrelevant auditory rhythms conferring a sense of metre. We predicted that (1) participants’ (n = 36) reaction times would reflect the auditory metric structure; (2) that disrupting the correspondence between the learned visual sequence and auditory metre would impede performance; and (3) that individual differences in sensitivity to rhythm would predict the magnitude of these effects. We found that participants' responses to visual cues systematically differed on the basis of the cues' position in the auditory metre. Altering the relationship between the trained visual sequence and auditory metre slowed reaction times, irrespective of individual sensitivity to rhythms. This factor was, however, predictive of reaction times over all. We demonstrate the influence of auditory temporal structures on visual-motor sequential learning in a widely used task where metre and timing are rarely considered. The current results indicate rhythm as a possible latent factor underpinning individual differences in sequential learning, with potential for diagnostic applications.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/posters/%E2%9C%9390%20macintyre-task-irrelevant.mp4?vrtx=view-as-webpage;
