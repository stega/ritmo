;;;;Videos are uploaded here: https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/;Videos are named with the EasyChair number followed by the first author's last name. ;;;;;;
;;;;Thomas/Wenbo: The columns with session name and chair is mostly relevant for the website (which Stephen is working on);;;;;;;
;EasyChair#;Session name;Session Chair;authors;title;keywords;abstract;YouTube link;Vortex link;Comment;
;38;Entrainment Symposium (E3);Hugo Merchant;Caroline Palmer and Alexander Demos;Explaining entrainment and synchrony with predictive coding and dynamical systems;"entrainment
anticipatory synchronization
predictive coding
dynamical systems";"Background: Humans entrain easily to intricate musical sequences. Entrainment processes allow us to perceive and produce quasi-rhythmic sound sequences. When musicians time their productions to synchronize or align with sound, they must anticipate future events in order to prepare movements, as movement preparation is often slower than the rate of musical events. A striking feature is the tendency to act sooner than the sound with which one synchronizes; musicians produce tones about 30-50 ms sooner than a regular auditory beat, and nonmusicians anticipate even sooner (50-80 ms) (Repp & Siu, 2015). We contrast two prominent theories that offer different mechanisms for this anticipatory synchronization. One is predictive coding (PC) and the other is dynamical systems (DS). We discuss their assumptions and computational differences, and applications to the case of musical synchronization.  

Findings: Several variables influence our ability to synchronize actions with musical sound. Musically trained individuals show smaller asynchronies (difference between produced tone onsets and observed stimulus onsets) than untrained individuals. The more sensory feedback available from self-generated and external auditory outcomes, the smaller the asynchrony error. Finally, the more regular the sequence, the smaller the asynchrony variability. Mean and variance measures of asynchrony are used by PC and DS models in different ways, to be discussed.	

Predictive Coding. Early precursors to predictive coding proposed internal modeling of future events, based on representations of the mapping between perception and action (Clark, 2013). Internal models of motor commands (Wolpert et al., 1995) and mental simulations of partners' joint actions (van der Steen & Keller, 2013) compare model predictions with stimulus input, and the internal model is adjusted for discrepancies. Recent PC theories also compute the difference between internal models and perceived events (Friston, 2018), with a goal to minimize the entropy (the inverse of predictability) of predictions relative to future outcomes. We will discuss musical applications of PC models that rely on Bayesian inference to update internal models (Heggli et al., 2019; Vuust & Witek, 2014). 

Dynamical Systems. Synchronization arises in DS theories when oscillators that are self-sustaining couple (share information, causing them to adapt). The self-sustaining nature of oscillators is supported by neural representations of musical pulse from MEG recordings that continue in the absence of the stimulus (Tal et al., 2017). When an oscillator is momentarily disturbed by input, it soon returns to its original frequency (Large at al, 2002). In the presence of a rhythmic stimulus, an oscillator will resonate (adapt its phase and/or frequency to the stimulus) when the oscillator's natural frequency is close to the stimulus frequency, or the coupling between oscillator and stimulus is strong. We will discuss musical applications of DS models that rely on time-delayed coupling to explain anticipatory synchronization (Demos et al., 2019; Roman et al., 2019). 
PC and DS models offer explanations of synchrony and entrainment that differ in where the computations arise and how prior likelihoods and internal models are used. Machine learning and mathematical tools may help clarify the utility of each theory.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/38-palmer.mp4?vrtx=view-as-webpage;Video uploaded to Vortex (Tuesday);
;88;Entrainment Symposium (E3);Hugo Merchant;Troby Ka-Yan Lui, Jonas Obleser and Malte Wöstmann;Exploring the temporal rhythmicity in distractor suppression;"Distraction suppression
Temporal rhythmicity
Auditory Attention
Working memory";Task-relevant cognitive processes, such as attention and working memory, are susceptible to distraction. While attention has been consistently demonstrated to operate in a rhythmic manner, the temporal rhythmicity in distractor processing is less well studied. We will present two recent studies on whether our cognitive vulnerability to distraction is rhythmic. First, using an Irrelevant-Speech task, we probed the rhythmicity of working memory interference by speech distractors of variable onsets during memory retention. Participants (N = 23) were asked to memorise a sequence of nine digits presented in random order. During the ensuing retention period, an irrelevant sentence was presented as a distractor at a random onset drawn from a uniform distribution. Results showed that the onset time of the distractor rhythmically modulated recall accuracy of the digit sequence and the distractor-evoked N1 event-related potential (ERP) component in the electroencephalogram (EEG). Critically, accuracy and N1 amplitude were co-modulated by a ~2.5 Hz rhythm. Second, we investigated how temporally-varied distractor tones impact neural and behavioural responses in a pitch discrimination task. Participants (N = 30) compared the pitch of two target pure tones and ignored a 25-Hz modulated tone sequence presented in between the targets. Both perceptual sensitivity (d’) and the distractor-evoked ERP amplitude were modulated by time of distractor onset. Correlation analyses revealed a correspondence between behavioural and neural distractor-evoked responses. We discuss the implications of these results for the view that similar to attentional selection, distractor suppression follows a temporal trajectory.;;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/88%20lui.mp4?vrtx=view-as-webpage;;
;97;Entrainment Symposium (E3);Hugo Merchant;Edward Large and Ji Chul Kim;Resonance, synchronization, and entrainment: What, why, and how?;"oscillation
rhythms
resonance
synchronization
entrainment
nonlinear systems";"Entrainment can be defined as the response of one or more oscillations to an external rhythm, bringing the oscillation(s) into synchrony with the rhythm. In behavioral entrainment individuals coordinate movements with stimuli such as isochronous rhythms, music, and speech. Neuronal entrainment, in which action potentials or local field potentials entrain to rhythmic stimuli, is important in the perception of music and musical rhythm, in the perception and intelligibility of speech, and in attention more generally.
In synchronization, oscillations are coupled bidirectionally and exert mutual influence on one another. Behavioral synchronization can be observed in the coordination of limbs and in coordination among individuals. Synchronization of anatomically distributed neuronal networks is necessary for behavioral synchronization, and more generally is thought to underlie large scale functional integration during complex activities such as speaking or reading.
In classical physics, resonance refers to the response of a (linear) system that does not exhibit a stable limit cycle in the absence of a rhythmic stimulus. In synchronization of nonlinear oscillators, resonance refers to an integer ratio frequency relationship, i.e. k:m, where k and m are integers. A linear system cannot exhibit a stable limit cycle, and only responds at stimulus frequencies. However, some nonlinear systems do not exhibit stable limit cycles, but do produce integer ratios and combination frequencies when stimulated. Thus, resonance is the most general category that includes both linear and nonlinear systems, systems with and without spontaneous oscillations, and systems that exhibit both phase-locking (1:1) and mode-locking (k:m). 
This is why we call our approach “neural resonance theory,” not “neural entrainment theory.” In this talk, I will detail what the phenomena are, describe why they are important, and discuss how to identify each in your data.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/97-large-1030.mp4?vrtx=view-as-webpage;Video uploaded to Vortex Wednesday morning (we're going for the 10:40);
;129;Entrainment Symposium (E3);Hugo Merchant;Dobromir Dotov, Abraham Betancourt, Jorge Gámez and Hugo Merchant;Neural resonance in the primate premotor cortices during the motor preparation stage of a synchronization task with discrete periodic stimuli;"animal model
anticipation
entrainment
neural dynamics
resonance";"Humans and other animals need to coordinate their actions with predictable structure present in the environment. The neural substrates of such skill can be investigated by training primates to tap along with a periodic stimulus. Single cells in the medial areas of the premotor cortex (MPC) encode the duration and serial order of rhythmic intervals. Properties of the population state trajectory such as its amplitude are also related to the interval of rhythmic tapping (Gámez, Mendoza, Prado, Betancourt, & Merchant, 2019). Such state trajectories are observed because the activities of individual cells are correlated, resulting in lower-dimensional dynamic manifolds over populations of neurons. We investigated the spiking profile of single cells from MPC and their population dynamics while two monkeys (Macaca mulatta) were in the passive, perceptual stage of a listen-then-synchronize task. The periodic stimuli consisted of discrete visual or auditory signals with a sub-second interval. Population activity exhibited an oscillatory profile with an anticipatory rise time and peaks locked to the stimulus. Importantly, the amplitude of each cycle increased from baseline to maximum over successive beats until the monkey began tapping. From a formal perspective, this is consistent with a resonance model of regularity detection rather than with models positing that intrinsically periodic activity is being phase-aligned with the stimulus. Additionally, we investigated the dimensionality of population activity by way of the loadings from principal component analysis. We found evidence both for high-dimensional activity with power-law scaling as reported previously in visual cortices but also for a low-dimensional oscillatory component emerging over background activity.

References
Gámez, J., Mendoza, G., Prado, L., Betancourt, A., & Merchant, H. (2019). The amplitude in periodic neural state trajectories underlies the tempo of rhythmic tapping. PLoS Biology, 17(4), e3000054.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/129-dotov.mp4?vrtx=view-as-webpage;Video uploaded to Vortex (Tuesday afternoon);
;52;"Entrainment (E1) 
";Jessica Grahn;Mattia Rosso, Marc Leman and Lousin Moumdijan;Neural entrainment meets behavior: the stability index as a neural outcome measure of auditory-motor coupling;"Entrainment
Auditory-motor coupling
Rehabilitation
EEG
Eigendecomposition";"Understanding rhythmic behavior in the context of coupled auditory and motor systems has been of interest to neurological rehabilitation, in particular to facilitate walking. Recent work based on behavioral measures revealed an entrainment effect of auditory rhythms on motor rhythms. In this study, we propose a method to compute the neural component of such process from an EEG signal. 
A simple auditory-motor synchronization paradigm was used, where 28 healthy participants were instructed to tap along to metronomes. The computation of the neural outcome measure was carried out in two blocks. In the first block, we used Generalized Eigendecomposition to reduce the data dimensionality to the component which maximally entrained to the metronomes. The scalp topography pointed at brain activity over the contralateral sensorimotor regions. In the second block, we computed instantaneous frequency from the analytic signal of the extracted component. This returned a time-varying measure of frequency fluctuations, whose standard deviation provided our ‘stability index’ as neural outcome measure of auditory-motor coupling. Finally, the proposed neural measure was validated by conducting a correlation analysis with a set of behavioural outcomes from the synchronization task: resultant vector length, relative phase angle, mean asynchrony and tempo matching.
Significant moderate negative correlations were found with the first three measures, suggesting that the stability index provided a quantifiable neural outcome measure of entrainment, with selectivity towards phase-correction mechanisms. 
We address further adoption of the proposed approach especially with populations where sensorimotor abilities are compromised by an underlying pathological condition. The impact of using stability index can potentially be used as outcome measure to assess rehabilitation protocols, and possibly provide further insight into neuropathological models of auditory-motor coupling.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/52%20rosso.mp4?vrtx=view-as-webpage;;
;61;"Entrainment (E1) 
";Jessica Grahn;Benjamin Morillon and Arnaud Zalta;Prominence of delta oscillatory rhythms in the motor cortex and their relevance for auditory perception;"audio-motor coupling
groove
entrainment
natural rhythms
human neurophysiology";"Temporal predictions are fundamental instruments for facilitating sensory selection, allowing humans to exploit regularities in the world. Recent evidence indicates that the motor system instantiates predictive timing mechanisms, helping to synchronize temporal fluctuations of attention with the timing of events in a task-relevant stream, thus facilitating sensory selection. Accordingly, in the auditory domain auditory-motor interactions are observed during perception of speech and music, two temporally structured sensory streams. I will present a behavioral and neurophysiological account for this theory and will detail the parameters governing the emergence of this auditory-motor coupling, through a set of behavioral and magnetoencephalography (MEG) experiments.
I will first review the prominence of delta (~2 Hz) oscillatory rhythms in the motor cortex and show that they constraint the interaction between motor and auditory systems. At this rate –and this rate only– overt rhythmic movements sharpen the temporal selection of auditory stimuli, thereby improving performance. 
I will next show that the implication of the motor system during auditory perception depends also on the temporal predictability of the sensory stream. Behaviorally, the feeling of groove induced by a melody –i.e. the wanting to move during passive listening– strongly depends on its temporal predictability. MEG results reveal that auditory and motor regions have a distinctive sensitivity to auditory temporal dynamics, with motor areas being more flexible in their ability to track temporal information.
Together, these findings are compatible with active sensing theories, emphasizing the prominent role of motor areas in sensory processing, which are notably implicated in the analysis of contextual temporal information.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/61%20morillon.mp4?vrtx=view-as-webpage;;
;9;"Entrainment (E1) 
";Jessica Grahn;Jonathan Cannon and Aniruddh Patel;The shared predictive roots of motor control and beat-based timing;"Beat-based timing
Motor system
Basal ganglia
Active inference
Neuroscience
Neurophysiology
Predictive processing";"fMRI results have shown that the supplementary motor area (SMA) and the basal ganglia, most often discussed in their roles in generating action, are engaged by beat-based timing even in the absence of movement. Some have argued that the motor system is “recruited” by beat-based timing tasks due to the presence of motor-like timescales, but a deeper understanding of the roles of these motor structures is lacking. Reviewing a body of motor neurophysiology literature and drawing on the “active inference” framework [1], I argue that we can see the motor and timing functions of these brain areas as examples of dynamic sub-second prediction informed by sensory event timing. I hypothesize that in both cases, sub-second dynamics in SMA predict the progress of a temporal process outside the brain, and direct pathway activation in basal ganglia selects temporal and sensory predictions for the upcoming interval -- the only difference is that in motor processes, these predictions are made manifest through motor effectors. Convergent evidence suggests that dopaminergic signaling may represent the certainty of these predictions, modulating the balance of top-down and bottom-up influences. If we can unify our understanding of beat-based timing and motor control, we can draw on the substantial motor neuroscience literature to make leaps forward in the study of predictive timing and musical rhythm.

[1] Adams, R. A., Shipp, S., & Friston, K. J. (2013). Predictions not commands: Active inference in the motor system. Brain Structure and Function, 218(3), 611–643.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/9%20cannon%20The%20shared%20predictive%20.mov?vrtx=view-as-webpage;;
;16;Entrainment (E2) ;Benjamin Morillon;Maja Foldal, Sabine Leske, Alejandro Blenkmann, Tor Endestad and Anne-Kristin Solbakk;Attentional modulation of alpha- and beta oscillations during perception of auditory rhythms – an EEG study;"Alpha-beta oscillations
Auditory selective attention
Electroencephalography
Lateralization
Temporal prediction";Hemispheric lateralization of alpha (7-13 Hz) and beta (14-24 Hz) oscillations is known to index voluntary allocation of spatial attention. However, it is largely unknown how attention adapts to temporal dynamics in sound stimuli. To address this issue, we investigated how hemispheric lateralization of alpha- and beta-oscillations is influenced by temporal structure and predictability in auditory stimuli. We recorded EEG while healthy adult participants listened to rhythmic tone sequences. Dichotic tone presentation allowed manipulation of spatial-temporal attention, as specific tones were presented to separate ears, also causing the time between tones to differ between the ears. Specifically, each ear received isochronous stimuli, but at different presentation rates. Together, this formed rhythmic sequences comprising a 3- versus 4-beat meter. Participants responded to stimulus-onset-asynchrony (SOA) deviants (-90 ms) for given tones in the attended rhythm. The purpose of inserting SOA deviants was to manipulate predictability, which remained high in blocks where these were inserted rarely, and decreased in blocks containing more frequently inserted SOA deviants. The results revealed lateralization of beta-power, according to attention direction, reflected as ipsilateral enhancement and contralateral suppression. No effect was found for the alpha-band. The beta-lateralization effect was larger in high- vs. low-predictability conditions, and synchronized more strongly to the temporal structure of attended than unattended tones. The present study demonstrates a role of beta-oscillations during attention to rhythmic stimuli that have varying temporal structures. Differently from previous studies, we presented stimuli as continuous rhythms containing both relevant and irrelevant temporal structures. Hence, the alignment of beta-lateralization with the temporal structure of attended sounds index voluntary control of attention to sound timing.;;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/16%20foldal.mov?vrtx=view-as-webpage;;
;56;Entrainment (E2) ;Benjamin Morillon;Fleur L. Bouwer, Johannes J. Fahrenfort and Heleen A. Slagter;Neural entrainment underlies beat-based, but not pattern-based temporal expectations in rhythm;"Expectations
Prediction
EEG
Entrainment
Beat
Pattern";"The brain continuously forms expectations about the timing of incoming information to optimize sensory processing. Such temporal expectations can be studied exceptionally well in musical rhythm, which can contain multiple types of structure that elicit temporal expectations. Rhythms often contain both a regular beat (eliciting “beat-based” expectations), and predictable patterns of absolute temporal intervals (eliciting “pattern-based” expectations). While beat-based expectations are thought to result from entrainment of low-frequency cortical oscillations to rhythmic input (Haegens & Zion Golumbic, 2018), it is unclear whether entrainment also underlies pattern-based expectations (Bouwer, Honing, & Slagter, 2020; Rimmele, Morillon, Poeppel, & Arnal, 2018). Here, we examined behavioral responses and EEG activity in silent periods following rhythmic sound sequences that allowed for beat-based or pattern-based expectations, or had random timing. In Experiment 1 (32 participants), we obtained ratings of how well probe tones presented at various times in the silence fitted the previous rhythm. While beat-based expectations affected fitness ratings for at least two beat-cycles, the effects of pattern-based expectations subsided after the expected time point in the silence. The effects of both beat-based and pattern-based expectations correlated with musical experience. In Experiment 2 (27 participants), using EEG, we found a CNV following the final tones of pattern-based and random sequences, but not beat-based sequences, suggesting that climbing neuronal activity may specifically reflect pattern-based expectations. Moreover, using a frequency tagging approach, we found enhanced power at the beat frequency for beat-based sequences not only during listening, but also during the silence, confirming a theoretical prediction of entrainment models. For pattern-based sequences, we observed enhanced power at pattern-related frequencies only during listening, but not during the silence. An additional analysis of the EEG data using multi scale entropy (MSE) as an index of signal regularity (Kloosterman, Kosciessa, Lindenberger, Fahrenfort, & Garrett, 2020) confirmed that the beat-based sequences elicited the most regular signal in the silence period. Finally, we show that multivariate pattern decoding may be a useful time-resolved alternative to spectral analyses in probing temporal expectations with EEG. Taken together, our findings suggest different mechanisms for expectations based on a regular beat and a rhythmic pattern, with the former but not the latter relying on neural entrainment.

Bouwer, F. L., Honing, H., & Slagter, H. A. (2020). Beat-based and memory-based temporal expectations in rhythm: similar perceptual effects, different underlying mechanisms. Journal of Cognitive Neuroscience, 32(7), 1221–1241. doi: 10.1162/jocn_a_01529

Haegens, S., & Zion Golumbic, E. (2018). Rhythmic facilitation of sensory processing: A critical review. Neuroscience and Biobehavioral Reviews, 86(March), 150–165. doi: 10.1016/j.neubiorev.2017.12.002

Kloosterman, N. A., Kosciessa, J. Q., Lindenberger, U., Fahrenfort, J. J., & Garrett, D. D. (2020). Boosts in brain signal variability track liberal shifts in decision bias. ELife, 9, 1–22. doi: 10.7554/ELIFE.54201

Rimmele, J. M., Morillon, B., Poeppel, D., & Arnal, L. H. (2018). Proactive sensing of periodic and aperiodic auditory patterns. Trends in Cognitive Sciences, 22(10), 870–882. doi: 10.1016/J.TICS.2018.08.003";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/56%20bouwer.mp4?vrtx=view-as-webpage;;
;116;Entrainment (E2) ;Benjamin Morillon;Tomas Lenc, Peter E. Keller, Manuel Varlet and Sylvie Nozaradan;Selective enhancement of metric periodicities in brain response to naturalistic music: reproducibility and transformation beyond low-level auditory processing;"EEG
rhythm
meter
frequency-tagging
naturalistic music";Music makes us move and temporal coordination of this movement is usually guided by a periodic pulse-like meter that can be perceived from the musical rhythm. Mounting evidence suggests that the internal representation of metric pulses relates to prominent periodicities in brain activity, as recorded in human participants using electroencephalography (EEG). However, this hypothesis has only been tested with brain responses to rhythmic stimuli specifically designed for experiment purposes. Here we show that prominent metric periodicities can also be captured in neural responses to naturalistic music, i.e., spectrotemporally rich auditory stimuli originally designed for dance and entertainment. Selective neural responses at metric periodicities were observed in response to music pieces inducing meter perception (as indexed after the EEG recording by capturing spontaneous finger tapping to the same musical stimuli). Importantly, this was the case even when there were no prominent cues to metric periodicities in the acoustic stimulus or in the responses simulated with a biologically-plausible cochlear model, thus controlling for stimulus-driven low-level confounds. However, metric periodicities were not prominent in brain responses elicited by music that did not induce stable meter perception. Finally, comparison across two EEG sessions recorded on two consecutive days revealed significant effects of familiarity. The prominence of metric periodicities in neural activity was highly reproducible across sessions, with the exception of responses to music that lacked prominent acoustic cues to the meter, in which metric periodicities were significantly enhanced on the second day. Together, these results further our understanding of the processes that transform internal representations from faithful tracking of the acoustic input towards periodic pulses that are fundamental to adaptive coordination behaviors in musical contexts.;;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/116%20lenc.mp4?vrtx=view-as-webpage;;
;85;"Entrainment (E4) 
";Bruno Laeng;Anna Fiveash, Simone Dalla Bella, Reyna Gordon and Barbara Tillmann;The multidimensionality of rhythm: Separable components of rhythmic abilities;"rhythm production
rhythm perception
rhythmic skills
BAASTA
rhythmic battery
individual differences
memory";"Humans have a remarkable capacity for perceiving and producing rhythm. Rhythmic competence is often viewed as a single concept, with participants who perform well or poorly on a single “rhythm task”. However, research is revealing a variety of sub-processes and competencies involved in rhythm perception and production which can be selectively impaired or enhanced. Our goal in the current study was to measure performance across a range of rhythm tasks to investigate whether different patterns of performance would emerge across tasks and individuals. Such patterns could reveal multiple, separable rhythmic competencies that may draw on distinct neural mechanisms. We tested 31 participants on nine rhythm perception and production tasks selected from different test batteries, including the battery for the assessment of auditory sensorimotor and timing abilities (BAASTA; Dalla Bella et al., 2017), the beat alignment test (BAT, Iversen & Patel, 2008, adapted from the BAASTA), the beat-based advantage task (BBA, Gordon et al., 2015) and two tasks from the Burgundy best musical aptitude test (BbMAT: perceptual synchronization and metric regularity; Bigand & Lalitte). 

Based on the literature, we expected to observe distinctions between (1) perception and production tasks, and (2) memory-based and beat-based rhythm perception tasks. Two principal component analyses (PCA) were run: one for perception, and one for perception and production. For the perception-only PCA, separate dimensions captured performance on the memory-based rhythm task (BBA) compared to beat-based rhythm tasks (BAT; BbMAT-synchronization; anisochrony detection). These dimensions were confirmed when the production tasks (unpaced tapping, synchronization-continuation, paced tapping - music, paced tapping - metronome) were added. Additionally, a separation was observed between production tasks, the memory-based rhythm perception task, and beat-based rhythm perception tasks. Hierarchical cluster analyses revealed distinct clusters of participants with different patterns of rhythmic competencies. The perception-only PCA revealed three clusters: participants who performed (1) poorly across memory-based and beat-based rhythm perception tasks (i.e., poor perceivers), (2) selectively well on the memory-based rhythm perception task (i.e., good memory-based perceivers), and (3) selectively well on beat-based rhythm perception tasks (i.e., good beat-based perceivers). Adding the production tasks did not change the cluster of poor perceivers. A small cluster of “poor tappers” also emerged, and a final cluster consisted of participants who performed well across all types of tasks. 
The current study suggests that production tasks, memory-based rhythm perception tasks, and beat-based rhythm perception tasks correspond to distinct dimensions of performance. Clusters of participants emerged with different patterns of rhythmic competencies, supporting the divergence of rhythmic skills and potential differences in underlying neural mechanisms supporting these skills. Based on these results, future studies investigating rhythm processing should include three short tasks that cover: (1) rhythm production (e.g., tapping to metronome/music), (2) beat-based rhythm perception (e.g., BAT), and (3) memory-based rhythm (e.g., BBA). If it is not possible to include a production task, the BAT may also be sufficient to capture beat-based timing, as it also contributed to the production dimension within the combined PCA. Implications for underlying neural mechanisms, links to pathology, and future research will be discussed. 

References 
Bigand, E., & Lalitte, P. (n.d.). Burgundy best Musical Aptitude Test. http://leadserv.u-bourgogne.fr/~cimus/
Dalla Bella, S., Farrugia, N., Benoit, C.-E., Begel, V., Verga, L., Harding, E., & Kotz, S. A. (2017). Baasta: Battery for the assessment of auditory sensorimotor and timing abilities. Behavior Research Methods, 49(3), 1128–1145. https://doi.org/10.3758/s13428-016-0773-6
Gordon, R. L., Shivers, C. M., Wieland, E. A., Kotz, S. A., Yoder, P. J., & Devin McAuley, J. (2015). Musical rhythm discrimination explains individual differences in grammar skills in children. Developmental Science, 18(4), 635–644. https://doi.org/10.1111/desc.12230
Iversen, J. R., & Patel, A. D. (2008). The Beat Alignment Test (BAT): Surveying beat processing abilities in the general population. In K. Miyazaki, M. Adachi, Y. Nakajima, & M. Tsuzaki (Eds.), Proceedings of the 10th International Conference on Music Perception and Cognition (pp. 465–468). Causal Productions.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/85%20fiveash.mp4?vrtx=view-as-webpage;;
;23;"Entrainment (E4) 
";Bruno Laeng;Anna Kasdan, Fabrizio Pizzagalli, Alexander Chern, Alyssa Scartozzi, Andrea Burgess, Sonja Kotz, Stephen Wilson and Reyna Gordon;Identifying a brain network for musical rhythm: A functional neuroimaging meta-analysis and systematic review;"meta-analysis
fMRI
rhythm
music";"Across multiple methodologies, a cortico-subcortical brain network including the basal ganglia, supplementary motor area (SMA), temporal cortices, cerebellum, and premotor cortex has been implicated in musical rhythm processing (Kotz et al., 2018). Much of the work contributing to such findings, however, has been conducted in small samples and it is unknown the extent to which each of these regions may be differentially recruited based on factors such as task design, stimulus complexity, or musical experience of the participants. We conducted a systematic review and meta-analysis of functional magnetic resonance imaging (fMRI) studies of musical rhythm processing using the Seed-based d mapping software (Albajes-Eizagirre et al., 2019). The goals of the meta-analysis were two-fold: to identify brain regions important for general musical rhythm processing and to identify regions modulated by rhythmic complexity. Critically, we parsed studies within the general musical rhythm meta-analysis by the type of baseline condition – studies either employed an unconstrained “rest/silence” baseline (9 experiments) or a low-level perceptual control baseline (e.g., non-beat or irregular rhythmic sequences) that controlled for auditory, motor, attentional, or other cognitive processes (9 experiments). 

We found that musical rhythm is largely represented bilaterally in the brain. The putamen bilaterally and the left inferior frontal gyrus, pars opercularis/ventral precentral gyrus emerged as significant clusters for the low-level control baseline subset of studies. A much larger network including these regions in addition to the cerebellum, SMA, inferior parietal lobule, precentral gyrus, superior and middle temporal gyri, and anterior insula bilaterally appeared for the silent baseline subgroup analysis. In the rhythm complexity meta-analysis (16 experiments), regions activated for more versus less complex rhythms were the bilateral SMA and anterior/mid cingulate gyrus region as well as the left cerebellum. Descriptive categorization of studies included in the meta-analysis indicated there was significant heterogeneity in terms of task type, stimulus type, and terms used to describe the stimuli (e.g., metric, non-metric, regular, irregular). Most commonly, studies used simple sound sequences (e.g., pure tone or woodblock rhythms) and employed perception (as opposed to production) tasks.

Results suggest that a brain network more specific to rhythm processing, and one not confounded by basic auditory or motor processes for example, consists of the putamen bilaterally and the left inferior frontal gyrus. Considering the differences in activation patterns based on baseline condition has important implications for future design of fMRI experiments investigating musical rhythm. Our findings advance the neural basis of musical rhythm and provide groundwork for understanding how the neurobiology of rhythm interfaces with other cognitive domains, including language and aesthetics.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/23%20kasdan.mp4?vrtx=view-as-webpage;;
;123;"Entrainment (E4) 
";Bruno Laeng;Reyna Gordon, Else Eising, Maria Niarchou, Research Team, Genlang Qtl-Gwas Working Group, Lea Davis, Nancy Cox and Simon Fisher;Examining shared genetic architecture between musical rhythm and language traits;"beat synchronization
genetics
language
reading
genome-wide association study
genetic correlation";"Individual differences in musical rhythm ability are related to phonological awareness, reading skill, spoken grammar, prosodic awareness, and second language acquisition within individuals with typical language development. Moreover, individuals with speech-language disorders such as dyslexia, developmental language disorder, and stuttering are more likely to have atypical rhythm skills. We hypothesized that shared genetic architecture may account for phenotypic associations between musical rhythm ability and language traits. To test this hypothesis, we used summary statistics from a large-scale genome-wide-association study (GWAS) on a musical rhythm phenotype (beat synchronization) from our collaboration with 23andMe, Inc. in 606,825 individuals (Niarchou et al., 2021, BioRxiv), and summary statistics from GWAS meta-analyses on five quantitative language traits conducted in the GenLang consortium in 11,176 to 19,946 individuals of European ancestry from 9 to 14 cohorts. LD-score regression genetic correlations were calculated between musical rhythm and each of five language-related phenotypes: non-word repetition, phoneme awareness, spelling, word reading, and non-word reading. All five language-related phenotypes showed significant positive genetic correlation with musical rhythm, with the highest genetic correlations with phoneme awareness (rg = 0.28, se = 0.05, p=1.02x10-7) and nonword repetition (rg = 0.38, se = 0.08, p=2.57x10-6), thus showing  genetic pleiotropy between rhythm and language-related phenotypes. Our findings support the Atypical Rhythm Risk Hypothesis (Ladanyi et al., 2020) and suggest that shared genetic architecture may be driving phenotypic associations between language and music. Ongoing work is examining if polygenic risk scores for rhythm can predict individual differences in spoken grammar ability and other spoken language traits.

Niarchou, M., Gustavson, D. E., Sathirapongsasuti, J. F., Anglada-Tort, M., Eising, E., Bell, E., McArthur, E., Straub, P., 23andMe Research Team, McAuley, J.D., Capra, J.A., Ullen, F., Creanza, N., Mosing, M., Hinds, D., Davis, L.K., Jacoby, N., Gordon, R. L. (2021) Unravelling the genetic architecture of musical rhythm: a large-scale genome-wide association study of beat synchronization. bioRxiv 836197 https://doi.org/10.1101/836197

Ladányi E, Persici V, Fiveash A, Tillmann B, Gordon RL (2020). Is atypical rhythm a risk factor for developmental speech and language disorders? WIREs Cognitive Science. e1528. (1-32) https://doi.org/10.1002/wcs.1528";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/123%20gordon.mp4?vrtx=view-as-webpage;;
;76;SMS (S3);Edward Large;Olivia Xin Wen and Molly Henry;Characterizing individuals’ oscillator flexibility: An online slider paradigm for temporal extrapolation;"oscillator flexibility
extrapolation
timing judgment
online slider
individual difference";"Background: The psychophysical task of mentally extrapolating a rhythm through a silent gap has been used to evaluate quantitative models of rhythm perception[1,2,3]. For example, patterns of timing-judgment errors can be used to estimate phase- and period-correction parameters for oscillator models[1]. We aim to use this task to estimate an unstudied oscillator property – flexibility: a flexible oscillator will maintain the stimulus dynamics through the silent gap, while an inflexible oscillator will decay quickly back to its own endogenous dynamics that might not match the stimulus. Thus, modeling timing-judgment errors under different experimental manipulations will allow us to estimate individuals’ oscillator flexibility. 
Aims: In 5 experiments, we validated and tested the versatility of an online slider paradigm to improve the efficiency and resolution of the extrapolation task over and above laborious in-lab approaches[2,3,4,5,6,7,8,9,10]. 
Method: Based on Manning and Schutz (2013, Experiment 1), stimuli in our base experiment comprised 13 context clicks presented with a 500ms inter-onset-interval (IOI), a silent gap of 3 IOIs, and a final click at one of six offsets: ±6%, ±18%, or ±30% re: “on time” based on extrapolation of the context IOI through the gap. Participants (N=134) rated the timing of the final click on a continuous slider with extremely early labeled on the left and extremely late on the right. Building on the base experiment, four experiments manipulated one parameter each: IOI (400, 500, 600ms), context length (13, 9, 5 clicks), gap length (3, 7, 11 IOIs), and offset range (±6%, ±18%, ±30%, ±42%, ±54%, ±66). Participants were first trained on two extreme offsets in each condition, and were instructed to avoid body movement and vocalization throughout.
Results: In all 5 experiments, participants accurately rated offset timing, replicating in-lab results[4]: earlier and later ratings corresponded to earlier and later offsets, respectively. This pattern generalized to a wider range of offsets, where participants used a larger slider range, proportional to offset timing. Both context-length and gap-length manipulations affected judgments of final-click timing. An earlier bias occurred for fast tempi (400 & 500ms) and short gap length (3 IOIs) relative to slow tempo (600ms) and long gap lengths (5 & 7 IOIs), suggesting an effect of global pacing of the experiment on local trial-by-trial timing judgments. On the other hand, timing judgments were unaffected by context length because all context lengths reinforced the same tempo and thus global temporal context. By examining a subset of participants (N=12) that took part in all but the base experiment, we demonstrated high reliability of timing ratings across sessions. 
Conclusion and Implications: We demonstrated that a continuous slider-rating paired with a temporal extrapolation task is reliable across multiple online studies and replicated in-lab results using a standard psychophysical method. The paradigm is adaptable to a wide offset range, affords better resolution in timing judgments than binary data, and is sensitive to influences of various experimental manipulations on offset-timing judgments. The improved efficiency and convenience of online data collection make the slider paradigm a promising tool to better study many timing-related topics.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/76%20wen-henry.mp4?vrtx=view-as-webpage;;
;111;SMS (S3);Edward Large;Harjo de Poel, Dobromir Dotov, Marije Harmsen, Liset van der Hulst and Claudine Lamoth;Synchronizing to adaptively variable oscillations;"entrainment
sensorimotor synchronization
chaotic behavior
complexity
human dynamic clamp";"Experimental sensorimotor synchronization tasks often involve isochronous stimuli. From different perspectives there are indications that inherent biological characteristics such as a natural degree of variability (e.g., Dotov et al., 2017) and mutual (rather than unidirectional) interdependence (e.g., Kostrubiec et al., 2015) facilitate entrainment. We pose that adaptively variable stimuli foster an agent’s rhythmic adaptability, which promotes sensorimotor and/or rhythmic behaviour more competently than moving to rhythmic sources that are/appear immutable and unnatural. Here we illustrate this alongside an auditory-motor tracking experiment with sound signals of periodic nature (P), complex nature (C), and complex but adaptive nature (AC).
A grouped acute learning design with pre-post-test comparison was adopted (Dotov & Froese, 2018). Thirty participants were randomly assigned to the P, C or AC learning group. They practiced in 40 trials to synchronize their sonified movement (i.e., oscillations of a hand-held IMU device were transformed into sound oscillations) with another auditory oscillating signal, depending on their group (see illustration).
 
For group P this involved a stimulus sound of which the pitch oscillated in a fixed sinusoidal manner. For group C, the stimulus pitch oscillated in a fixed, though less predictable manner, as modelled by a chaotic oscillator. For the AC group, chaotic stimulus oscillations were (weakly) coupled to the IMU movement signal. Crucially, this implied that when participants managed to anticipate the complex structure of the stimulus it also adapted in real-time to their movement oscillations. Hence, when they managed to deal with the complexity of the stimulus, their task became less complex because of the bidirectional adaptation. 
		
All three groups were compared pre- vs. post-learning based on windowed cross-correlation and between-signal RMSE in the two non-adaptive stimulus tasks P and C (5 times each, administered in counterbalanced order). These analyses suggested that all groups showed improvement in both the periodic and chaotic task (= general learning effect), and that the P group showed the least improvement. Notably, whereas the P and C group showed typical between-subject variation, in the AC group participants in fact showed quite homogeneous improvement. Furthermore, for AC the cross-correlations lags converged towards zero rather than negative asynchrony (as often observed for isochronous tasks). Considering the setting choices taken in this experiment (coupling strength, degree of complexity, etc.), these outcomes provide promising incentives that adaptively variable stimuli indeed foster rhythmic sensorimotor performance and merits further empirical endeavours in this regard. 


Dotov, D., & Froese, T. (2018). Entraining chaotic dynamics: a novel movement sonification paradigm could promote generalization. Human movement science, 61, 27-41.

Dotov, D. G., Bayard, S., de Cock, V. C., Geny, C., Driss, V., Garrigue, G., Bardy, B. & Dalla Bella, S. (2017). Biologically-variable rhythmic auditory cues are superior to isochronous cues in fostering natural gait variability in Parkinson’s disease. Gait & posture, 51, 64-69.

Kostrubiec, V., Dumas, G., Zanone, P. G., & Kelso, J. S. (2015). The virtual teacher (VT) paradigm: learning new patterns of interpersonal coordination using the human dynamic clamp. PloS one, 10(11), e0142029.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/111%20de-poel.mp4?vrtx=view-as-webpage;;
;30;SMS (S3);Edward Large;Lauren Fink, Prescott Alexander and Petr Janata;The influence of metronome adaptivity and auditory feedback on group tapping;"sensorimotor synchronization
adaptive metronome
group tapping
groove
synchrony";"When humans interact together in motoric synchrony, they are more likely to exhibit cooperative behavior, successful joint actions, trust of others, and altruism (Valdesolo, Ouyang, & DeSteno, 2010; Wiltermuth & Heath, 2009). While such benefits occur during pure motor synchrony – for example, walking in step together – using music to organize movement is a powerful temporal cue and culturally relevant activity. The use of music, or even just a metronome, can enhance cooperative behavior (Kniffen, Yan, Wansink, & Schulze, 2017; Wiltermuth & Heath, 2009), as well as feelings of synchronization or connection with others (Fairhurst, Janata, & Keller, 2013, 2014; Hove & Risen, 2009; Zhang, Dumas, Kelso, & Tognoli, 2016). Importantly, however, the degree to which individuals can synchronize with each other depends on their ability to perceive the timing of others’ actions and produce motor movements accordingly. Here, we introduce an adaptive device that can be used to enhance synchronization abilities within and across individuals. 
Unlike previous adaptive metronomes, ours is implemented on Arduino Uno circuit boards, allowing for negligible temporal latency between player input and adaptive sonic output. Across five experiments, we 1) replicate previous single-person adaptive metronome findings (Fairhurst, Janata, & Keller, 2013), 2) extend the use of an adaptive metronome to a multi-person scenario, and 3) analyze the effects of auditory feedback on tapping performance and subjective ratings of performance quality and emotions. In all experiments, player synchronization with the metronome was significantly enhanced with 25-50% adaptivity, compared to no adaptation. In group experiments with auditory feedback, synchrony remained enhanced even at 70-100% adaptivity; when feedback was removed, synchrony at these adaptivity levels returned to near baseline. Subjective ratings of being in the groove, in synchrony with the metronome, in synchrony with others, liking the task, difficulty of the task, and influence over the metronome all could be reduced to one latent factor, which we have termed, enjoyment. The same factor structure replicated across all experiments. In predicting participants’ enjoyment score, we found a significant interaction between auditory feedback and metronome adaptivity. Specifically, participants experienced an increase in enjoyment at optimal levels of adaptivity when they received auditory feedback, and a severe decrease in enjoyment at higher levels of adaptivity when they had no auditory feedback. Exploratory analyses relating personality factors to tapping performance will also be discussed. 

References: 
Fairhurst, M. T., Janata, P., & Keller, P. E. (2013). Being and feeling in sync with an adaptive virtual partner: brain mechanisms underlying dynamic cooperativity. Cereb Cortex, 23(11), 2592-2600. doi:10.1093/cercor/bhs243
Fairhurst, M. T., Janata, P., & Keller, P. E. (2014). Leading the follower: an fMRI investigation of dynamic cooperativity and leader-follower strategies in synchronization with an adaptive virtual partner. Neuroimage, 84, 688-697. doi:10.1016/j.neuroimage.2013.09.027
Hove, M. J., & Risen, J. L. (2009). It's all in the timing: Interpersonal synchrony increases affiliation. Social Cognition, 27(6), 949-961. 
Kniffen, K., Yan, J., Wansink, B., & Schulze, W. (2017). The sound of cooperation: Musical influences on cooperative behavior. J. Organiz. Behav., 38, 372–390. doi:10.1002/job.2128 
Valdesolo, P., Ouyang, J., & DeSteno, D. (2010). The rhythm of joint action: Synchrony promotes cooperative ability. Journal of Experimental Social Psychology, 46(4), 693-695. doi:10.1016/j.jesp.2010.03.004
Wiltermuth, S. S., & Heath, C. (2009). Synchrony and Cooperation. Psychological Science, 20(1), 1-5. 
Zhang, M., Dumas, G., Kelso, J. A., & Tognoli, E. (2016). Enhanced emotional responses during social coordination with a virtual partner. Int J Psychophysiol, 104, 33-43. doi:10.1016/j.ijpsycho.2016.04.001";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/30%20fink.mp4?vrtx=view-as-webpage;;
;24;SMS (S4);Marc Leman;Thomas Kaplan, Jonathan Cannon, Lorenzo Jamone and Marcus Pearce;Modelling entrainment as continuous bayesian inference using learned rhythmic prototypes;"categorical rhythm perception
cognitive modelling
bayesian inference
rhythmic prototypes";"Grammar-like models of musical rhythm have been used to demonstrate how a listener's metrical expectations resemble temporal patterns learned from score-based musical rhythms (Lerdahl & Jackendoff, 1983; Temperley, 2010; Van der Weij, 2020). Outside of notated representations of musical rhythm, it has been proposed that these schematic expectations help to categorise non-uniform rhythms (Honing, 2013), yet it remains unclear how patterns of expressive timing influence both the formation of these schematic expectations and entrainment in the context of these expectations.

	In order to investigate this question, we propose that the dynamic perception of musical rhythms can be expressed---at a cognitive level---as the problem of continuously aligning the auditory rhythm (event-based in continuous time) with rhythmic prototypes (symbolic sequences) learned through musical enculturation. Here we present a computational model capable of such entrainment: a model of auditory expectation based on statistical learning (Pearce, 2018) is used to derive a temporal expectation template, which is deployed in a process of variational Bayesian inference that continuously estimates posterior probabilities of onset phase in a stimulus rhythm (Cannon, 2021). Crucially, the latter model maintains precise timing expectations with respect to schematic expectations provided by the former model.

	As a preliminary assessment of the model, we simulate the iterated reproduction of random three-interval rhythms in accordance with the method used by Jacoby & McDermott (2017), to demonstrate how our model, just like participants, is biased towards perception of prototypical integer-ratios between intervals. Learned posterior distributions of event timing in a given iteration are used to create the stimulus rhythm in the subsequent iteration, as an approximation of finger-taps. Two models were configured with different cultural corpora, to highlight the influence of differing rhythmic prototypes: German folksongs (Schaffrath & Huron, 1995), and Turkish Makam music (Karaosmanoğlu, 2012). Both models converge on rhythms related by simple small-integer ratios (e.g. 1:1:1, 1:1:2, 2:1:1), yet also diverge at some more complex ratios (e.g. 2:2:3). This divergence reflects differences in schematic expectations acquired through simulated enculturation, and contributes further evidence towards the proposal that music-cultural environments shape the rhythmic prototypes which constrain rhythm perception (Polak et al., 2018).

	The model presented serves as a basis for analysing the rhythmic prototypes which continuously guide our listening to auditory rhythms, and how these depend on training and culture. Further to existing work that demonstrates how patterns of rhythm production can  be related to the frequency of patterns in (symbolic) music corpora using Bayesian inference (Sadakata et al., 2006), the model allows detailed analysis of how these prototypical patterns are dynamically inferred and aligned with an auditory rhythm. In future work we will use this model to explore the dynamic perception of musical rhythms with natural timing fluctuations and micro-rhythmic features, where metrical expectations vary in temporal precision (Danielsen, 2018).


References:
- Cannon, J. (2021). Expectancy-based rhythmic entrainment as continuous Bayesian inference. bioRxiv. 
- Danielsen, A. Pulse as Dynamic Attending. Analysing Beat Bin Metre in Neo Soul Grooves. In:The Routledge Companion to Popular Music Analysis. Routledge, 2018, pp.179–189.
- Honing, H. Structure and Interpretation of Rhythm in Music. In:The Psychology of Music. Elsevier, 2013, pp. 369–404.
- Jacoby, N., & McDermott, J. H. (2017). Integer ratio priors on musical rhythm revealed cross-culturally by iterated reproduction. Current Biology, 27(3), 359–370.
- Karaosmanoğlu, M. K. A Turkish makam music symbolic database for music information retrieval: SymbTr. In:Proc. of the 13th Int. Society for Music Information Retrieval Conf. 2012, 223–228.
- Lerdahl, F., & Jackendoff, R. (1983). An overview of hierarchical structure in music. Music Perception, 1(2), 229–252.
- Pearce, M. T. (2018). Statistical learning and probabilistic prediction in music cognition: Mechanisms of stylistic enculturation. Annals of the New York Academy of Sciences, 1423(1), 378–395.
- Polak, R., Jacoby, N., Fischinger, T., Goldberg, D., Holzapfel, A., & London, J. (2018). Rhythmic prototypes across cultures. Music Perception, 36(1), 1–23.
- Sadakata, M., Desain, P., & Honing, H. (2006). The Bayesian way to relate rhythm perception and production. Music Perception, 23(3), 269–288.
- Schaffrath, H., & Huron, D. (1995). The Essen Folksong Collection in kern format. [Retrieved 2020]. https://kern.humdrum.org/cgi-bin/browse?l=essen
- Temperley, D. (2010). Modeling common-practice rhythm. Music Perception, 27(5), 355–376. 
- Van der Weij, B. (2020). Experienced listeners: Modeling the influence of long-term musical exposure on rhythm perception (Doctoral dissertation). Amsterdam Institute for Logic, Language and Computation.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/24%20kaplan.m4v?vrtx=view-as-webpage;;
;43;SMS (S4);Marc Leman;Manuel Anglada-Tort, Peter Harrison and Nori Jacoby;REPP: A robust cross-platform solution for online sensorimotor synchronization experiments;"Sensorimotor synchronization
rhythm
movement
timing
online experiments";"Sensorimotor synchronization (SMS), the rhythmic coordination of perception and action, is a fundamental human skill that supports many behaviours, from daily repetitive activities to the highest forms of behavioural coordination, including music and dance (Repp 2005). Research on SMS has been mostly conducted in the lab using finger tapping paradigms, where participants typically tap with their index finger to a rhythmic sequence of auditory stimuli. However, these experiments require equipment with high temporal fidelity to capture the asynchronies between the time of the tap and the corresponding cue event. Thus, SMS is particularly challenging to study with online research, where variability in participants’ hardware and software can introduce  all kinds of delay in latency and jitter into the recorded timestamps. For example, SMS experiments performed online can introduce delays in the order of 60 to 100 ms and vary considerably between platforms, browsers, and devices (Anwyl-Irvine et al., 2020). 
 
Here we present REPP (Rhythm ExPeriment Platform), a novel technology to measure SMS in online experiments that can work efficiently using the built-in microphone and speakers of standard laptop computers. This study aims to validate this technology and demonstrate how it can be implemented in online-based experiments to ensure high data quality while minimizing recruitment costs. REPP uses a free-field recording approach: specifically, we play the prepared stimulus through the laptop speakers and simultaneously record the resulting audio signal along with participants’ tapping response using the built-in microphone. The resulting recording is analyzed using signal processing techniques to extract and align timing cues with high temporal accuracy. This analysis is fully automated and customizable, enabling researchers to monitor online experiments in real time and to implement a wide variety of SMS paradigms. 
 
We validated REPP in a total of four experiments. We first demonstrated that it achieves high temporal accuracy using an external calibration setup (Experiment 1). In particular, we estimated the average latency and jitter of REPP to be within 2 ms. In a laboratory experiment (Experiment 2; N = 20), we then found that REPP achieves a high test-retest reliability (r = .87 and ρ = .81) and a high concurrent validity (r = .94 and ρ = .80). In an online study (Experiment 3), we confirmed that the test-retest reliability of REPP remains high when using a larger sample of participants recruited online (N = 166; r = .80 and ρ = .81). Finally,  we were able to replicate a relatively more complex tapping paradigm using REPP in an online large-scale study (N = 133): estimating perceptual priors for simple rhythms via iterated reproduction of random temporal sequences, following the paradigm of Jacoby and McDermot (2017). 
Together, these experiments show that REPP is well equipped to collect large-scale tapping datasets in a wide variety of SMS experiments online. This novel technology can therefore open new avenues for research on SMS that would be nearly impossible in the laboratory, reducing experimental costs while  massively increasing the reach, scalability and speed of data collection.

References
Anwyl-Irvine, A., Dalmaijer, E., Hodges, N., & Evershed, J. (2020). Online Timing Accuracy and Precision: A comparison of platforms, browsers, and participant's devices. Pre-print. https://doi.org/10.31234/osf.io/ jfeca
Jacoby, N., & McDermott, J. H. (2017). Integer ratio priors on musical rhythm revealed cross-culturally by iterated reproduction. Current Biology, 27(3), 359-370 
Repp, B. H. (2005). Sensorimotor synchronization: a review of the tapping literature. Psychonomic Bulletin & Review, 12(6), 969-992.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/43%20anglada-tort%20harrison%20jacoby.mp4?vrtx=view-as-webpage;;
;127;"Entrainment (E5) 
";Daniel Cameron;Zachary Lookenbill and Leigh VanHandel;Effect of metrical primes on perceived complexity of 2:3 and 3:4 polyrhythms;"Polyrhythm
Meter
Complexity
Perception";"Previous empirical studies concerning the perception and production of polyrhythms have isolated the rhythms from a metrical context to observe the meter associated with each rhythm and different performance strategies employed by musicians. These studies show the important effect pitch and tempo have on identifying meter (Handel & Oshinsky, 1981; Handel & Lawson, 1983). For example, listeners tend to tap the 3-pulse layer in a 2:3 polyrhythm except at fast tempos where the 2-pulse layer is preferred, and listeners often tap the lower pitched stream of the 3:4 polyrhythm (Handel and Oshinsky 1981). Additional studies demonstrate an integrated model of performance, where the placement of onsets of one stream is determined in relation to the interval between onsets of the other stream. This model can be thought of as a figure and ground stream (Peters and Schwartz, 1989; Pressing, Summers, and Magill, 1996; Krampe et. al., 2000). The current study extends prior research on polyrhythms by focusing on the metrical context in which the rhythms are presented, emulating more realistic musical environments where the listener is already entrained to a meter once the polyrhythm enters. This emphasis is aimed at investigating modes of listening or production that might aid a musician in performing polyrhythms in various metrical contexts. 
The current methodology employs metrical primes that suggest one of the two possible meters in 2:3 and 3:4 polyrhythms. Participants rate the complexity of the polyrhythmic pattern and how well the rhythm fits with the metrical prime. Similar to Handel & Oshinsky (1981) and Handel & Lawson (1983), the aim is to observe any preference for a particular meter, but in this case, perceived metrical fit and complexity will be observed rather than tapping behavior. My hypothesis is that a metrical prime will overwhelm any previous preference for meter, meaning listeners will adjust their metrical interpretation of the polyrhythm to align with the primed meter. Additionally, I predict patterns placed in simple meters will be perceived as less complex than in compound meters, since compound meter requires more subdivisions than simple meter. Results indicate listeners do have a preference of meter in both 2:3 and 3:4 polyrhythms. Listeners also rated the complexity of the stimuli lower in simple meters for 2:3, but this relationship is not clear in 3:4. Discussion considers the implication of the results on music pedagogy and performance of polyrhythms. 

References
Beauvillain, C. (1983). Auditory perception of dissonant polyrhythms. Perception & Psychophysics, 34(6), 585-592. https://doi.org/10.3758/BF03205915
Eerola, T., Himberg, T., Toiviainen, P., & Louhivuori, J. (2006). Perceived complexity of western and African folk melodies by western and African listeners. Psychology of Music, 34(3), 337–371. https://doi.org/10.1177/0305735606064842
Fidali, B C., Poudrier, È., & Repp, B. H. (2013). Detecting perturbations in polyrhythms: effects of complexity and attentional strategies. Psychological Research 77, 183–195. https://doi.org/10.1007/s00426-011-0406-8
Handel, S., & Lawson, G. R. (1983). The contextual nature of rhythmic interpretation. Perception & Psychophysics, 34 (2), 103-120. https://doi.org/10.3758/BF03211335
Handel, S., & Oshinsky, J. S. (1981). The meter of syncopated auditory polyrhythms. Perception and Psychophysics, 30(1), 1-9. https://doi.org/10.3758/BF03206130
Jones, M. R., Jagacinski, R. J., Yee, W., Floyd, R. L., & Klapp, S. T. (1995). Tests of attentional flexibility in listening to polyrhythmic patterns. Journal of Experimental Psychology: Human Perception and Performance, 21(2), 293-307.	 https://doi.org/10.1037/0096-1523.21.2.293
Keller, P., & Burnham, D. (2005). Musical Meter in Attention to Multipart Rhythms. Music Perception 22(4), 629-666. https://doi.org/10.1525/mp.2005.22.4.629
Krampe, R. T., Kliegl, R., Mayr, U., Engbert, R., & Vorberg, D. (2000). The fast and the slow of skilled bimanual rhythm production: parallel versus integrated timing. Journal of experimental psychology: Human perception and performance 26(1) 206–233.	 https://doi.org/10.1037/0096-1523.26.1.206
Moelants, D., & Noorden, L. V. (2005). The Influence of Pitch Interval on the Perception of Polyrhythms. Music Perception: An Interdisciplinary Journal 22(3), 425-440. https://doi.org/10.1525/mp.2005.22.3.425
OneMotion. (n.d.). Drum Machine. https://www.onemotion.com/drum-machine/
Peters, M., & Schwartz, S. (1989). Coordination of the Two Hands and Effects of Attentional Manipulation in the Production of a Bimanual 2:3 Polyrhythm. Australian Journal of Psychology 41(2), 215–224. https://doi.org/10.1080/00049538908260084
Poudrier, È. (2017). Tapping to Carter: Mensural Determinacy in Complex Rhythmic Sequences. Empirical Musicology Review 12(3-4), 277-315. http://dx.doi.org/10.18061/emr.v12i3-4.5814
Pressing, J., Summers, J., & Magill, J. (1996). Cognitive Multiplicity in Polyrhythmic Pattern Performance. Journal of Experimental Psychology: Human Perception and Performance 22(5), 1127–1148. https://doi.org/10.1037/0096-1523.22.5.1127";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/127%20lookenbill.mp4?vrtx=view-as-webpage;;
;121;"Entrainment (E5) 
";Daniel Cameron;Alexandre Celma-Miralles, Cecilie Møller, Jan Stupacher and Peter Vuust;Neural entrainment to an ambiguous polyrhythm within distinct metrical contexts;"polyrhythm
beat perception
neural entrainment
EEG
frequency-tagging";"The temporal events of music are perceived as rhythmic patterns within a metrical framework (Honing, 2013; London, 2012). Our brain processes these rhythms by extracting and predicting an underlying periodicity (i.e. the beat) and its groupings and subdivisions (i.e. the meter; Honing, 2012; Fitch, 2013). Polyrhythms are temporal structures in which more than one rhythmic stream occurs at the same time. For instance, the 3:4 polyrhythm combines a ternary (3) and a quaternary (4) pulse train that provides the listener with an ambiguous percept with rhythmic tension (Vuust, Gebauer, and Witek, 2014). Here, we used the 3:4 polyrhythm to study neural aspects of beat perception in different metrical contexts.

We recorded the EEG of 16 participants listening to a 3:4 polyrhythm presented at a rate of 90:120 BPM. To control for the natural tendency to tap the 3 pulse train in this polyrhythm (Møller, Stupacher, Celma-Miralles and Vuust, in prep), the amplitudes of the 4 pulse train were increased by 8 dB. To assess the effect of metrical context, we included experimental conditions where the polyrhythm was preceded by an 8-second drum beat playing either the 3 or the 4 pulse train. Using a frequency-tagging approach (Nozaradan, 2014), we analyzed the steady-state evoked potentials elicited during listening to the polyrhythm.  Specifically, we compared the frequencies related to each meter (i.e. 1.5 Hz and 2 Hz) to  each other and to the frequencies present in the stimulus envelope (Nozaradan, Peretz, and Mouraux, 2012). These comparisons revealed that neuronal populations enhanced neural amplitudes at the meter-related frequencies, especially when the polyrhythm was presented in a metrical context emphasizing the 3-pulse train.

Participants’ preferred beat was assessed behaviourally in a separate session after the EEG-recordings, and the tapping responses showed that half of participants perceived the 4 pulse train as representing the beat. However, no differences between ternary- and quaternary-tappers appeared in neural entrainment, likely due to the counterbalanced position of the blocks providing different metrical contexts.

In sum, the results show that neural entrainment to an ambiguous polyrhythm can be modulated by providing a metrical context to structure the incoming events of the polyrhythm, which is in line with the predictive coding of music (Vuust, Gebauer, and Witek, 2014; Vuust and Witek, 2014).

REFERENCES
Fitch, W. (2013). Rhythmic cognition in humans and animals: distinguishing meter and pulse perception. Frontiers in systems neuroscience, 7, 68.
Honing, H. (2012). Without it no music: beat induction as a fundamental musical trait. Annals of the New York Academy of Sciences, 1252(1), 85-91.
Honing, H. (2013). Structure and interpretation of rhythm in music. In D. Deutsch (Ed.), The psychology of music (p. 369–404). Elsevier Academic Press. https://doi.org/10.1016/B978-0-12-381460-9.00009-2
London, J. (2012). Hearing in time: Psychological aspects of musical meter. Oxford University Press.
Nozaradan, S. (2014). Exploring how musical rhythm entrains brain activity with electroencephalogram frequency-tagging. Philosophical Transactions of the Royal Society B: Biological Sciences, 369(1658), 20130393.
Nozaradan, S., Peretz, I., & Mouraux, A. (2012). Selective neuronal entrainment to the beat and meter embedded in a musical rhythm. Journal of Neuroscience, 32(49), 17572-17581.
Vuust, P., Gebauer, L. K., & Witek, M. A. (2014). Neural underpinnings of music: the polyrhythmic brain. Neurobiology of interval timing, 339-356.
Vuust, P., & Witek, M. A. (2014). Rhythmic complexity and predictive coding: a novel approach to modeling rhythm and meter perception in music. Frontiers in psychology, 5, 1111.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/121%20celma-miralles.mp4?vrtx=view-as-webpage;;
;27;"Entrainment (E5) 
";Daniel Cameron;Erica Flaten and Laurel J Trainor;Waltz or march? Evidence for infants’ top-down meter processing of an ambiguous rhythm;"Rhythm perception
Beat & meter
Top-down processes
Infant development
EEG";"Infants typically have daily exposure to the music of their culture, and sensitivity to beat and rhythm are present in early development. From rhythmic patterns, people extract the beat – the regular temporal intervals we tap or dance to. Beats can be perceptually grouped to form meters. For example, a repeating 6-beat rhythm pattern can be perceived as three groups of 2 (march meter) or two groups of 3 (waltz meter). A previous study from our lab found that when 7-month-olds were presented with a repeating ambiguous 6-beat rhythm, they showed electroencephalographic (EEG) steady-state responses at all three main frequencies present in the stimulus, representing beat, duple and triple levels (Cirelli et al., 2016). However, this study did not address whether infants could be primed to hear the rhythm in one meter or the other. One approach to examine top-down processes is to analyze event-related potentials (ERPs). Specifically, the mismatch negativity (MMN) ERP component is automatically elicited at around 100 - 200 ms after stimulus onset by occasional unexpected ‘deviant’ sounds in a sequence of standard sounds (Näätänen & Alho, 1995). Infants often show a positive mismatch response (MMR) between 200 and 400 ms after deviant onset. Though the mismatch response is automatic, it can be modulated by top-down processes, such as internal perceptions of meter. For example, deviants on metrically strong, as opposed to weak beats elicit larger amplitude mismatch responses in infants (Winkler et al., 2009) and adults (Bouwer et al., 2014). 
While previous studies show infants can discriminate different metrical structures, it is not known whether they can maintain a metrical interpretation through top-down processes in the case of an ambiguous rhythm. 13 infants were primed to hear the 6-beat ambiguous pattern in duple meter, and 11 in triple meter, through loudness accents either on every second or every third beat. Periods of priming (4 repetitions of the pattern with accents) were inserted before sequences of test trials, consisting of 16 repetitions of the ambiguous unaccented pattern. To examine MMR, occasional pitch deviants were placed on either beat 4 (strong beat in the triple interpretation; weak in duple) or beat 5 (strong in the duple; weak in triple) of the unaccented test trials. Analyses revealed an interaction between priming type (double, triple) and deviant beat position on MMR amplitudes (defined as the deviant-standard amplitude difference). Specifically, MMR was larger for beat 5 than beat 4 in the duple group, with a trend in the opposite direction for the triple group   . In line with our hypothesis, this shows infants can use internal processes to maintain a metrical interpretation of an ambiguous rhythm. Further, at frontal right sites, MMR was generally larger for those in the duple group compared to the triple group, which may reflect a processing advantage for duple meter. These results indicate infants can impose a top-down internally generated metrical structure on ambiguous auditory rhythms, an ability that would aid early language and music learning. Analyses with a comparative adult sample are currently under-way.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/27%20flaten.mp4?vrtx=view-as-webpage;;
;102;Medical (Med);Laurel Trainor;Prisca Hsu, Emily Ready and Jessica Grahn;The effects of music training, dance training and Parkinson's disease on beat perception and production abilities;"Beat perception and production
Music
Dance
Parkinson's Disease
Motor entrainment
Synchronization
Timing
Sensorimotor integration";Humans naturally perceive and move to a musical beat, entraining body parts to the complex auditory stimuli through clapping, tapping and dancing. Yet the accuracy of this seemingly effortless behavior varies widely across individuals. Beat perception and production abilities can be positively impacted by past experiences, such as music and dance training, and are negatively impacted by progressive neurological changes in Parkinson’s Disease (PD). In this study, we assessed the combined effects of past music or dance training and early-stage PD to determine whether the positive effects of rhythm-based training in healthy adults on beat processing abilities are altered in PD. Musicians and dancers have morphological brain differences in areas directly engaged during their training, such as increased gray and white matter networks in temporal and motor areas. These neurological correlates may be preserved to facilitate motor timing abilities despite basal ganglia degeneration in PD. Thus, we examined whether PD patients with previous music and dance training demonstrated better beat perception and production abilities compared to PD patients without training. We used the Beat Alignment Test (BAT) to assess beat perception and production abilities among a pre-existing sample of 458 participants (278 healthy younger adults, 139 healthy older adults, and 41 people with early-stage PD), across varying levels of music and dance training. In general, participants with over three years of music training had more accurate beat perception than those with minimal training (p<.001). Interestingly, PD patients with over three years of music training demonstrated beat production abilities comparable to healthy adults (all p-values >.05) while PD patients with minimal music training performed significantly worse (p<.01). No dance training effects were found. The finding that musically trained PD patients perform similarly to healthy adults during a beat production task, while untrained patients do not, may broadly inform the role of music training in preserving certain rhythmic motor timing abilities in early-stage PD. These results suggest that, despite underlying motor timing deficits that are inherent to PD, patients with a musical background may be able to approximate sensorimotor synchronization abilities similar to that of a healthy adult during rhythm-based tasks.;;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/102%20hsu.mp4?vrtx=view-as-webpage;;
;68;Medical (Med);Laurel Trainor;Aleksandra Koprowska, Maja Serman, Torsten Dau and Jeremy Marozeau;Training synchronization with musical beat to improve speech perception in individuals with hearing loss;"beat perception
speech intelligibility
hearing loss
auditory rehabilitation";"Training auditory skills through music is one of the possible interventions for individuals suffering from difficulties understanding speech in noise. Such training can be particularly helpful for those people affected by a hearing loss, to whom hearing aids do not provide satisfactory benefits. Among various possible forms of musical training, rhythm-based tasks were identified as the most promising option. Research investigating the relationship between musical skills and speech comprehension revealed that individuals with good rhythm perception and production skills tend to obtain better speech recognition thresholds (SRTs) in background noise. Especially sensitivity to musical beat showed a strong correlation with the SRTs. The ability to extract and track temporal regularities in the speech stream appears relevant for speech perception as it allows the listener to form accurate predictions about the timing of upcoming speech tokens, thus, enhancing anticipatory attention. However, it is not known if rhythm-based training can improve this ability and, consequently, speech understanding. Furthermore, it has not been investigated yet, if such training would be suitable for hearing-impaired listeners. 
To address these questions, we designed a training program aimed to improve the perception of and synchronization with the musical beat. The program implemented as a mobile application comprised 18 training sessions, where the task was to listen to musical excerpts and tap along. Hearing-aid users with mild to moderate, symmetric, age-related hearing loss were recruited and divided into two groups. Half of the participants underwent the 18-day long training program at home. The other half of the participants served as an active control group, which for 18 days completed daily exercises based on listening to audiobooks. To assess the effects of both interventions, all participants completed Danish speech-in-noise tests (DANTALE II and HINT), beat perception and production assessment (Beat Alignment Test) as well as a questionnaire about self-perceived hearing abilities (SSQ12) before and after the training period. 
The results of this study will allow evaluating the efficacy of the proposed training program as a potential rhythm-based auditory rehabilitation tool for individuals with hearing loss.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/68%20koprowska.mp4?vrtx=view-as-webpage;;
;117;Medical (Med);Laurel Trainor;Olivia Boorom, Valerie Muñoz, Camila Alviar Guzman, Christopher Kello and Miriam Lense;Hierarchical acoustic structure during parent-child interactions of toddlers with typical development and autism spectrum disorder;"autism spectrum disorders
parent-child interaction
hierarchical structure";"Background: Timing is essential for successful social interactions, as exemplified by interactional variables like interpersonal synchrony and parental responsiveness, which support the development of communication and language skills in children with and without ASD (Siller & Sigman, 2002; Hudry et al., 2013). The importance of timing is also evident in caregivers’ vocalizations to young children: compared to adult-directed speech, infant-directed speech exhibits greater hierarchical temporal structure, or clustering of acoustic events across multiple linguistic timescales (e.g., syllables nested within phrases nested within utterances), which likely supports children’s attention to the speech signal and their developing communication skills (Falk & Kello, 2017). However, hierarchical temporal structure has not yet been examined across a dyadic interaction, with both parent and child contributing to the acoustic signal, and has not been applied to a clinical population.

Objectives: In consideration of the transactional nature of parent-child interactions, we examined acoustic temporal clustering of parent-child interactions in dyads of toddlers with and without ASD.

Methods: 53 parent-child dyads (19 ASD; 34 TD (22 nonverbal age-matched, 12 language age-matched)) were audio-recorded during a 10-minute parent-child free play activity. Peak amplitude events in audio recordings were used to compute Allan Factor variances, which reflects event clustering at multiple time scales (from a 0.0146 second scale to a 30 second scale). Quadratic slopes were fit across each dyad’s Allan Factor functions to quantify acoustic clustering patterns across timescales. Slopes of nonverbal age-matched, language-matched TD dyads, and ASD dyads were compared using Welch two-sample t-tests.

Results: Overall, the slopes derived from Allan Factor analysis of dyadic interactions were significantly lower in the TD nonverbal age matched cohort (M = 0.67, SD = 0.078) than the ASD cohort (M = 0.75, SD = 0.077), t(38.2) = -3.38, p < 0.01. However, the slopes were comparable in the ASD and language matched cohort (M = 0.74, SD = 0.077), t(26.63) = 0.26, p = 0.80. The magnitude of slopes indicates greater hierarchical clustering (steeper Allan Factor functions) of the acoustic signal in the dyads with children with ASD versus nonverbal age matched children with TD, particularly in longer timescales corresponding to phrase-level speech, but similar levels of hierarchical clustering when compared to the language-matched dyads.

Conclusions: This is the first study to examine multiscale acoustic temporal structure across both partners in a dyadic interaction, as well as in a clinical population of toddlers with ASD. Results add to evidence that temporal clustering is impacted by social communicative factors with greater hierarchical temporal structure in parent-child dyads of toddlers with ASD versus TD nonverbal age-matched toddlers. The similar degree of acoustic clustering between the ASD and language-matched sample indicate that higher slopes in the ASD cohort may be related to adaptations in interpersonal vocal dynamics based on the vocal skill of the communication partner. Future analyses will investigate characteristics, such as turn-taking and vocal durations, that may impact acoustic temporal clustering.

Falk, S., & Kello, C. T. (2017). Hierarchical organization in the temporal structure of infant-direct speech and song. Cognition, 163, 80–86. https://doi.org/10.1016/j.cognition.2017.02.017

Hudry, K., Aldred, C., Wigham, S., Green, J., Leadbitter, K., Temple, K., … McConachie, H. (2013). Predictors of parent-child interaction style in dyads with autism. Research in Developmental Disabilities, 34(10), 3400–3410. https://doi.org/10.1016/j.ridd.2013.07.015

Siller, M., & Sigman, M. (2002). The Behaviors of Parents of Children with Autism Predict the Subsequent Dev...: EBSCOhost. Journal of Autism and Developmental Disorders, 32(2), Vol 32, No 2.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/117%20boorom.mp4?vrtx=view-as-webpage;;
;75;Music (M2);Laura Bishop;Marc Leman;Co-regulated timing in music ensembles: a Bayesian listener perspective;"co-regulated timing
music perception
BListener";Co-regulated timing in a music ensemble rests on the human capacity to coordinate actions in time. Here we explore the hypothesis that humans predict timing constancy in coordinated actions, in view of timing their own actions in line with the others. An algorithm (BListener) is presented that predicts timing constancy, using Bayesian inference about incoming timing data from the music ensemble. Smoothness and regularization parameters are explained and illustrated. The algorithm is then applied to a timing analysis of real data, first, to a choir consisting of four singers, then, to a dataset containing performances of duet singers. Global features of timing constancy, such as fluctuation and stability, correlate with human subjective estimates of the music ensembles' quality and associated experienced agency. The results suggest that computational modelling of co-regulated timing can lead to powerful insights and applications. In future work, BListener could serve as component in an artificial musician that plays along with human musicians in a music ensemble.;;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/75%20leman.mp4?vrtx=view-as-webpage;;
;70;Music (M2);Laura Bishop;Carlos Eduardo Cancino-Chacón, Silvan Peter, Emmanouil Karystinaios and Gerhard Widmer;Towards quantifying differences in expressive piano performances: Are Euclidean-like distance measures enough?;"performance modeling
expressive piano performance
generative model evaluation";"Introduction

Computational models of expressive music performance can be used to generate an expressive (i.e., human-like) performance of a piece given its score ore  (Cancino-Chacón et  al.,  2018). For models of piano performance, these generated performances would typically include deviations in performance parameters such as tempo/timing, dynamics and articulation. A central question with generative computational models that produce some sort of artistic output is how to evaluate the quality of this output. Unfortunately, large scale evaluation of such models through listening tests would be too time consuming in the context of common research practice (Bresin & Friberg, 2013).

Advances in machine learning have led to a renewed interest in generative data-driven models of expressive performance (Cancino-Chacón  et  al.,  2017;  Jeong  et  al.,  2019; Maezawa et al., 2019). Evaluation of such models usually involves quantitatively comparing performances generated by these models to performances by (expert) human pianists. This comparison is done by measuring the reconstruction error, i.e., the distance between curves of performance parameters (tempo, dynamics, etc.) generated by the model and those of reference human performances, using a metric (distance measure) such as the Euclidean distance or its derivatives (such as the mean squared error). It is easy to see that such an approach might present several issues:

1.  Quantitative metrics of closeness do not necessarily imply perceptual nor musical (or aesthetic) closeness: not all errors (i.e., points/notes at which the generated performance is not identical to the human performance) would be perceived by listeners as equally important. For example, speeding up at the end of a phrase (instead of slowing down) might come as more unexpected/unmusical than simply playing the entire piece a little bit faster, yet both cases might result in the same numerical distance.

2.  Choice of the reference human performance(s) to compare the output of the model to: pianists interpret music in different but musically equally valid ways.

In this study we investigate the limitations of reconstruction error-like functions to evaluate models of expressive piano performance using a validity/reliability framework.

Methods

Using two datasets of precisely measured performances of classical piano music (recorded on computer controlled grand pianos), we present tests that indicate that evaluating performance models using commonly used distance measures do not necessarily satisfy even moderate reliability and validity requirements. We compare multiple metrics (Euclidean, cosine, Lp norm, etc.), including metric learning (i.e., using data-driven methods to learn appropriate distance measures), as well as aggregating distances at different time-scales, and different transformations/scalings of the curves of performance parameters.

We complement this experiment with a listening experiment in which participants are asked to identify randomly generated (i.e., unmusical) performances from human performances. This experiment aims to validate the use of the validity/reliability framework to compare distance measures for measuring the similarity between human performances.

Expected Results

Data collection is ongoing. We expect the results to show the limitation of standard measures of similarity between performances and serve as a basis for discussing implications for quantitative evaluation of generative models of performance.

Furthermore, we aim to derive recommendations for the evaluation of computational models of expressive performance. Future work will include investigating the contribution and interaction of different performance parameters (tempo, dynamics, articulation, etc.) to the perception of similarity in performances.

Acknowledgments

This research has received support from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme under grant agreement No. 670035 (project “Con Espressione”) and by the Research Council of Norway through its Centers of Excellence scheme, project number 262762 and the MIRAGE project, grant number 287152.

References

Bresin,  R.,  &  Friberg,  A.   (2013).   Evaluation  of  Computer  Systems  for  Expressive Music Performance.  In A. Kirke & E. R. Miranda (Eds.), Guide to Computing for Expressive Music Performance (pp. 181–203).  London, UK: Springer-Verlag.

Cancino-Chacón, C., Gadermaier, T., Widmer, G., & Grachten, M.  (2017).  An Evaluation of Linear and Non-linear Models of Expressive Dynamics in Classical Piano and Symphonic Music. Machine Learning, 106(6), 887–909.

Cancino-Chacón,   C.,   Grachten,   M.,   Goebl,   W.,  &  Widmer,   G.(2018). Computational  Models  of  Expressive  Music  Performance:    A  Comprehensive  and Critical  Review. Frontiers in Digital Humanities, 5,   25

Jeong, D., Kwon, T., Kim, Y., Lee, K., & Nam, J.  (2019).  VirtuosoNet:  A Hierarchical RNN-based System for Modeling Expressive Piano Performance.  In Proceedings of ISMIR 2019 (pp. 908–9015).  Delft, The Netherlands.

Maezawa, A., Yamamoto, K., & Fujishima, T.  (2019).  Rendering Music Performance with Interpretation Variations using Conditional Variational RNN.  In Proceedings of ISMIR 2019(p. 855-861).  Delft, The Netherlands";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/70%20cancino-chacon.mp4?vrtx=view-as-webpage;;
;81;Music (M2);Laura Bishop;Alan Wing, Ryan Stables, Mark Elliott, Maria Witek and Massimiliano Di Luca;Synchronisation in a virtual quartet.;"Synchronisation
Music ensemble
Quartet simulation
Linear Phase Correction Model
Feedback correction gain
Timing variability";The Linear Phase Correction Model of musical ensemble (LPC) (Wing et al., 2014) suggests each player in a string quartet corrects the timing of the next note as a proportion of the asynchronies with the other three players on the previous note. In the present study, we used a quartet simulation which implemented the LPC model, to examine synchronisation of a participant performing as violin 1 with three virtual players playing violin 2, viola and cello. We were interested to determine whether the participant’s timing would be affected by the timing variability and correction gain of the three virtual players. The simulation involved a performance of an excerpt from Haydn Opus 74 No 1(4). Participants tapped on a midi-interfaced drum pad to produce a succession of automatically sequenced notes comprising the (violin 1) melody with the accompaniment notes produced by the virtual players on violin 2, viola and cello. The virtual player note onsets were timed according to the LPC model with pre-specified correction gains and timing variability. We used a two-by-two design, comparing the effects of setting violin 2 variability lower or higher than viola and cello crossed with correction gain set lower or higher than viola and cello. Analysis of the note onset times according to the bGLS method (Jacoby et al 2016) revealed that participants reduced their correction gain with respect to violin 2 when violin 2 variability was higher or correction gain was higher. We suggest these changes reflect sensitivity and adaptivity of participants performing as violin 1 to the timing characteristics of the other three players in the virtual ensemble.  Further research is needed to determine whether similar effects would be observed if the participant were playing a real instrument with the virtual ensemble.;;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/81-wing.mp4?vrtx=view-as-webpage;VIdeo uploaded to Vortex (monday afternoon);
;13;Music (M1);Olivier Senn;Nori Jacoby, Rainer Polak and Justin London;Extreme precision in rhythmic interaction is enabled by role-optimized sensorimotor coupling: Analysis and modeling of West African drum ensemble music;"sensorimotor synchronization
rhythm & timing
music performance
joint action
rhythm modeling";"Background

Synchronized human behaviors such as music-making require sophisticated adaptive error correction mechanisms (Repp & Su, 2013; Timmers et al., 2014; Wing et al., 2014). Malian Jembe drum ensemble music features extremely tight synchronization at rapid rates (Polak, Jacoby, & London, 2016; Clayton et al. 2020), and the various parts of the ensemble have distinct musical roles, affording study and modelling of more complex patterns of information flow and error correction.

Aims

Through timing analyses of a large corpus of drum performances, and by testing coupling models based upon that performance data, we document the high degree of synchrony achieved by Malian drummers and show they employ optimal error correction strategies.

Methods

Four Jembe ensembles performed 72 takes of the piece “Suku.” Onset asynchronies were measured as their deviation from prototypical timings inferred from each performance.  Influence among ensemble members was assessed via a causal coupling model.  Simulated performances systematically varied the coupling relationships among the players to assess coupling optimization.

Results

Instrumental Role, Ensemble, and Lineup (where players switch roles) were all highly significant factors (p<.001), but differed in terms of their explained variance, covering 56%, 15.8% and 2.1% of the variance, respectively.  Coupling analysis showed that the non-variative accompaniment role (Jembe 2) exerts the strongest influence on the other instruments’ timing.  In a two-dimensional parameter space that mapped overall coupling strength and its allocation amongst the musical roles, the observed coupling values were not significantly different from the optimal location within the space (p=0.47 via Wilcoxon Rank-Sum Test).

Conclusion

Synchronization in the Malian drum ensemble music we studied is related to the specific musical role of each drum in the ensemble to a much higher degree than to idiosyncratic differences among particular performers and ensembles. It is asymmetrically distributed amongst the ensemble roles and inversely related to the variability/flexibility of each role: the most variable and socially prominent role (lead-drum) adapts to the least variable one (accompaniment), similar to the relation between soloist and rhythm section suggested by qualitative jazz research (Berliner, 1994; Monson, 1996).  Moreover, in the Malian ensembles this asymmetry is optimized, given the different musical tasks that each drummer must perform. 

Implications

Previous research using European music ensembles presumed that error correction either is achieved by ensemble members mostly adapting to a lead-role, suggesting a coherent concept of leadership (Badino, et al., 2014; D’Ausilio, et al. 2012; Keller, 2014; Vanzella, et al., 2019), or distributed evenly across ensemble members (Elliot, et al., 2014; Goebl & Palmer 2009; Timmers, et al., 2014; Wing, et al., 2014), though these studies also suggest that neither strategy is operative in its most basic form. Our findings show that the social dynamics of ensemble synchronization in West-African and European musical genres differ in fundamental ways, underscoring the importance of cross-culturally comparative and culturally decentered research perspectives. More broadly, we demonstrate that music ensemble performance and perception is a complex social-musical interaction, and should be studied as such.

References

Badino L, D’Ausilio A, Glowinski D, Camurri A, Fadiga L. 2014 Sensorimotor communication in professional quartets. Neuropsychologia 55, 98-104. doi:10.1016/j.neuropsychologia.2013.11.012
Berliner P. 1994 Thinking in jazz. The infinite art of improvisation. Chicago: University of Chicago Press.
Clayton M, Jakubowski K, Eerola T, Keller PE, Camurri A, Volpe G, Alborno P. 2020 Interpersonal Entrainment in Music Performance. Music Perception 38, 136–194. (http://dx.doi.org/10.1525/mp.2020.38.2.136).
D’Ausilio A, Badino L, Li Y, Tokay S, Craighero L, Canto R, Aloimonos Y, Fadiga L. 2012 Leadership in Orchestra Emerges from the Causal Relationships of Movement Kinematics. PLoS ONE 7:e3575
Elliott MT, Chua WL, Wing AM. 2016 Modelling single-person and multi-person event-based synchronisation. Current Opinion in Behavioral Sciences 8:167–174. doi:10.1016/j.cobeha.2016.01.015
Goebl W, Palmer C. 2009 Synchronization of timing and motion among performing musicians. Music Perception 26(5), 427-438. doi:10.1525/mp.2009.26.5.427
Keller P. 2014 Ensemble performance: interpersonal alignment of musical expression. In Expressiveness in music performance: Empirical approaches across styles and cultures (ed. Fabian D, Timmers R, Schubert E), pp. 260-282. Oxford: Oxford University Press.
Monson I. 1996 Saying Something: Jazz Improvisation and Interaction. Chicago: University of Chicago Press.
Polak R, Jacoby N, London J. 2016 Both isochronous and non-isochronous metrical subdivision afford precise and stable ensemble entrainment: A corpus study of Malian jembe drumming. Frontiers in Auditory Neuroscience, doi:10.3389/fnins.2016.00285
Repp BH, Su YH. 2013 Sensorimotor synchronization: A review of recent research (2006-2012). Psychonomic Bulletin & Review 20, 304-452.
Timmers R, Endo S, Bradbury A, Wing AM. 2014 Synchronization and leadership in string quartet performance: A case study of auditory and visual cues. Frontiers in Psychology 5,645. doi:10.3389/fpsyg.2014.00645
Vanzella P, Balardin JB, Furucho RA, Augusto G, Morais Z, Janzen TB, Sammler D, Sato JR. 2019 fNIRS responses in professional violinists while playing duets: Evidence for distinct leader and follower roles at the brain level. Frontiers in Psychology 10,164. doi:10.3389/fpsyg.2019.00164
Wing AM, Endo S, Bradbury A, Vorberg D. 2014 Optimal feedback correction in string quartet synchronization. Journal of The Royal Society Interface 11, 20131125. (http://dx.doi.org/10.1098/rsif.2013.1125).";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/13%20jacoby%20polak%20london.mp4?vrtx=view-as-webpage;;
;71;Music (M1);Olivier Senn;Guilherme Schmidt Câmara, George Sioros and Anne Danielsen;Mapping timing and intensity strategies in drum-kit performance via hierarchical clustering and phylogenetic visualizations;"Drumming
Performance
Timing
Groove
Microrhythm
Onset
Intensity";"Background and Aim
 
Findings from performance timing experiments have shown that drummers are able to systematically play stroke onsets significantly earlier and later than an instructed on-the-beat performance (Câmara et al., 2020; Danielsen et al., 2015), and purportedly able to further control the degree of onset asynchrony between the various constituent instruments of the drum-kit (Câmara and Danielsen, 2019). Previous investigations have focused on comparing average statistical trends of onset timing between timing styles across entire groups of drummers. In this study, we map performance strategies present at the individual participant level and categorize the different archetypical ways in which drummers express different timing styles. We focus on the onset asynchrony and intensity of strokes between drum-kit instruments, and in relation to a metrical grid, and hypothesize that drummers employ consistent strategies to achieve different desired timing styles, choosing different instruments (snare/kick/hi-hat) in the rhythmic pattern to generate in-sync, late, and early timing performances.
 
Methods 

In a previous experiment (Câmara et al., 2020), twenty-two drummers were instructed to play a basic “back-beat” pattern along to a metronome and a pre-recorded instrumental track in three different timing conditions: laid-back, on-the-beat, and pushed. Here, we conduct a hierarchical cluster analysis of various onset and intensity features in this data set, combined with phylogenetic tree visualizations to provide an overview of the strategies used by the drummers to distinguish laid-back/pushed from on-the-beat performances. Furthermore, we encode the features of the onset or intensity clusters into microtiming archetypes that visually summarize the general characteristics of the drummers’ strategy in each cluster. 
 
Results 

Overall, three overarching onset strategies were used to distinguish pushed/laid-back from on-the-beat performances: (1) strong “general earliness/lateness” strategies: most instruments are consistently played earlier/later in time relative to the grid; (2) subtler “early/late flam” strategies: most instruments are played synchronously with the grid but at least one instrument is played distinctively early/late ; and (3) even subtler “ambiguously early/late compound sound” strategies: two instruments are played synchronously in relation to each other as a compound sound, but one instrument is played synchronous with the grid and the other is played early/late. While no clear intensity manipulation patterns emerged to exclusively distinguish laid-back/pushed timing, they serve as a means of enhancing or diminishing the effect of intentionally produced asynchronies.
 
Conclusion  

Results indicate that performers utilize a range of inter-instrument onset timing and intensity relationships to express microrhythmic feel in groove performance, that is, different drummers use different means to achieve the same desired feel.
 
Implications 

The novel methods developed in this study may be applied to analysis of commercial recordings to provide insight into the idiomatic timing–sound strategies of influential performers and/or genres/styles more generally.
 
References 

Câmara, G. S., & Danielsen, A. (2019). Groove. In A. Rehding & S. Rings (Eds.), The Oxford handbook of critical concepts in music theory (pp. 271–294). Oxford University Press. 
Câmara, G. S., Nymoen, K., Lartillot, O., & Danielsen, A. (2020). Timing Is Everything . . . or Is It? Effects of Instructed Timing Style, Reference, and Pattern on Drum Kit Sound in Groove-Based Performance. Music Perception, 38(1), 1–26.
Danielsen, A., Waadeland, C. H., Sundt, H. G., & Witek, M. A. G. (2015). Effects of instructed timing and tempo on snare drum sound in drum kit performance. Journal of the Acoustical Society of America, 138(4), 2301–2316.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/71%20camara.mp4?vrtx=view-as-webpage;;
;47;Music (M1);Olivier Senn;Fred Hosken;The perception and discrimination of different musical “feels”;"microtiming
expressive timing
empirical research
Just Noticeable Differences";"Background
Are listeners able to discern different musical “feels”? Performers can operationalize feel descriptors like “laid back” and “pushed,” nuancing the loudness, timbre, and timing of their performances in systematic ways (e.g., Câmara et al., 2020a, 2020b) and some DAW plug-ins have feel settings (e.g., Soundtoy’s “EchoBoy” has a “feel” dial that ranges from “rushin” to “draggin”). But if we are to talk meaningfully about “feel” and its relation to microtemporal events, we need to understand whether listeners have the ability to perceive these subtle performance details.

Aims
This study evaluates the ecological validity of music information retrieval analyses of microtemporal musical phenomena in “groove” music. Drawing on data about microtiming gathered from analyses of real drum performances, this experimental study ascertains whether the nuances are perceptible to general audiences or if they fall beneath the limits of Just Noticeable Differences. The hypothesis is that the deviations of the magnitude found in real performances are indeed perceivable.

Methods
Three separate phases of the experiment tested: whether listeners can discern different feels 1) in general, 2) between pushed and laid back, and 3) between tight and loose. Each of the studies used an ABX paradigm presented online to 36 people. In each of 24 trials, participants listen to three unique basic “boom tish” drum grooves that are generated so that they have specific timing profiles and then make a choice about which of the first two performances the third one sounds most like. The timing profiles, which are assigned to the bass drum and the snare drum (hi-hats are metronomic), are created by drawing onset times from normal distributions with predefined properties: a “pushed” profile involves the mean onset occurring before the metronome while “laid back” involves the mean onset occurring after. The amount of variance to these profiles is also controlled such that, for example, a virtual drummer who tends to “push” also performs in either a “tight” (low variance) or “loose” (high variance) manner. Drum grooves are synthesized at 80 and 110 BPM. Statistical analyses utilize signal detection theory (Boley & Lester, 2009).

Results
Study 1: Of the 28 analyzable participant responses, all but two participants have d’ values whose 95% confidence intervals do not overlap with a score of 0. As such, for the vast majority of participants, they are able to distinguish between drum performances that have different timing profiles. Studies 2 and 3, which honed in on particular feel contrasts, showed a more complex picture with participants succeeding in differentiating between profiles, but finding the task a challenge and with wide variation by participant. Interestingly, participants appear better able to discern between performance timing profiles at 110 BPM than at 80 BPM — e.g., study 1: paired t-test: t(19) = 3.160, p < .01, d = 0.706. There appeared to be no effect of musical training or of listening habits.

Conclusion
This study demonstrates that listeners are able to discern between drum performances that have different timing profiles. The results of the study give confidence to analyses of musical “feel” and other microtemporal phenomena, supporting the claims that these subtle changes in timing are perceivable and not below the threshold for the Just Noticeable Difference, and therefore they may have meaningful aesthetic and embodied consequences for listeners. Future directions include investigating listeners' qualitative associations with these profiles, whether they use vocabulary like ""tight"" or whether they have other meaningful descriptors.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/47%20hosken.mp4?vrtx=view-as-webpage;;
;53;Music (M3);Lauren Fink;Toni Amadeus Bechtold and Maria Witek;That’s a different type of groove! – How musician’s strategies change groove experiences;"popular music
groove
body movement
bodily sensations";"Background

Groove is a fundamental aspect of modern popular music. Music psychology understands groove as a pleasurable urge to move to music. This urge to move has been examined in various listening experiments in the last decades with the aim to determine how groove comes about, and which musical properties are associated with it. However, groove creators’ perspectives, practices, intensions, expertise and understanding of the concept remain underrepresented in this research. It is further unclear how exactly it feels to experience groove and whether this feeling is the same for all kinds of music, as detailed qualitative studies that venture into these subtleties are rare.

Aims

The study aims to learn how musicians and producers understand groove, which strategies they use to promote it, and what reactions they intend to elicit in listeners. Grounded in this knowledge, we develop a theory of how groove experiences feel for different types of music and how this feel relates to the musical properties.

Methods

Following a method of theory-generating interviews, 5 semi-structured interviews were conducted with musicians, composers and producers. Aside from questions on the topic, 6 short music excerpts of various styles were used as an elicitation method and to substantiate the conversation. The interviews were analysed in a 4-step procedure of coding, thematic comparison, conceptualization and theoretic generalisation.

Results

The analysis revealed a three-step procedure of a) compositional, performance- or production-related groove strategies by the musicians that evoke b) intended reactions, which together with the strategies lead to c) a specific groove experience. This groove experience has two necessary aspects - bodily and emotional movement – but how exactly these are felt depends on one hand on the individual listener, and on the other hand on the exact groove strategies. In other words, different musical styles, which require specific context-dependent interpretations and rules, intend different forms of movements and feelings. Therefore, the experience of groove changes based on the music, and it is possible to distinguish different types of the groove experience. These types can be described based on the associated bodily sensations, of which more than 30 were described in the interviews, for example relaxed, sticky, nervous, vigorous, or breathing grooves.

Conclusions

The expert’s assessment of what creates groove is largely congruent to what has been established in previous listening experiments. However, the qualitative approach combined with a set of stimuli revealed important new insights: there are different types of intended groove experiences, and the experience changes based on the strategies that are used to create it. Therefore, it seems appropriate to allow for more relativity in the theoretical framework of groove research instead of working towards a universal groove formula, and consequently speak of groove experiences instead of “the” groove experience.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/53%20Bechtold.mp4?vrtx=view-as-webpage;;
;60;Music (M3);Lauren Fink;Connor Spiech, Bruno Laeng, Georgios Sioros, Anne Danielsen and Tor Endestad;More than meter's the eye: Divergent roles of pickups and syncopation in groove;"groove
pupillometry
pickups
syncopation
predictive coding
metric complexity";One important aspect of groove is a pleasurable compulsion to move to musical rhythms. This has been shown to vary along an inverted U-curve with increasing metric complexity (e.g., syncopation, pickups). This has been explained within the framework of predictive coding where moderate levels of metric complexity drive us to move in an attempt to reduce sensory prediction errors and accurately model the temporal structure of the rhythm (Koelsch, Vuust, & Friston, 2019). Here, we tested this directly using pupillometry for its ability to index arousal and cognitive effort. Previous studies on groove have demonstrated greater pupil dilations in response to high microtiming asynchronies (Skaansar, Laeng, & Danielsen, 2019) and syncopated drum patterns (Bowling, Graf Ancochea, Hove, & Fitch, 2019) compared to straight rhythms. To our knowledge, however, ours is the first rigorously controlled study of pupil size changes over a broader range of metric complexity that encompasses both pickups and syncopations in order to investigate the neurophysiological correlates of groove. Previous studies had either neglected the upper end of complexity (particularly the differences in musicality related to repetition) or ignored the role of pickups altogether. Here we replicate the canonical inverted-U relationship between metric complexity and groove ratings, including findings demonstrating that this effect is enhanced by musical ability using the Computerized Adaptive Beat Alignment Test (Harrison & Müllensiefen, 2018) rather than reports of musical training and practice (Matthews, Witek, Heggli, Penhune, & Vuust, 2019). Importantly, these results map nicely onto the pupil drift rate, suggesting that groovier rhythms hold attention longer than ones rated less groovy. Moreover, we found divergent but complementary effects of syncopations and pickups on groove ratings and pupil size, respectively, extending previous findings which tended to focus solely on syncopations by discovering a distinct predictive mechanism in the brain related to pickups. Specifically, the brain appears to pre-emptively deploy attention to pickups in order to enhance the prediction errors of pickups, thus augmenting the predictive scaffolding that we use to guide the movements needed to suppress the subsequent prediction errors arising from syncopations. This thus lends correlative support to the predictive coding account where groove is envisioned as an embodied resolution of precision-weighted prediction error (Vuust & Witek, 2014).;;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/60%20spiech.mp4?vrtx=view-as-webpage;;
;33;SMS (S2);Justin London;Elodie Augier, Naila Boudiaf and Christian Graff;From sperm-whale clicking to human finger-tapping: Etho-mimetics of active reception rhythms;"active reception
Probe
finger tapping
Sequence of Pulse Intervals (SPI)
Time time series
Cetacean
Ethomimetics
one-dimensional signal
Local filtered signal
Electric fish
Neuroethology
Sperm whale";"Motor rhythmic activities are involved in sensorimotor processes. Active reception is the process by which an agent extracts information about the surroundings from self-emitted probe signals. In nature, human haptics, mammal echolocation and fish electrolocation are among the most documented functions. Technology has developed radar, sonar and magnetic resonance imaging set-ups. Sperm whales (Physeter macrocephalus) use an echolocation system based on clicks emitted at variable rhythms. During navigation, their clicking rate increases when they arrive in their foraging zone, and turns into bursts called ""creaks"" when attaining the squids they feed on (Miller et al. 2004). Analogous accelerations in the emission of transient probe signals are observed in other animals that use active reception such as dolphins (Doh et al, 2018), echolocating bats (Britton & Jones 1999) and mormyrid electric fish (Avril & Graff 2007). The analogy can be attributed to convergent evolution.
We wanted to see if humans, enabled to use an echolocation system, would spontaneously show similar phases of sequences of pulse intervals when approaching a target. In our etho-mimetic paradigm, human participants - eyes closed - navigated around on a large table by tapping on it with a thimble at the tip of their index finger, and by listening to the specific sound feedback produced on different areas. Directly modeled from sperm whale ethology, the remote part of the table top was covered by a layer of thin paper mimicking the deep-sea hunting zone. Distributed over it, a number of (“prey”) targets consisted in paper disks stuck over the layer. The specific prey orientation was figured by a thickened diameter of the disk, parallel vs orthogonal to the table width.
The rhythms of probing were analyzed through the inter-tap interval durations, analogous to the sperm whales click intervals. Results show that humans are readily able to use echolocation for fine perception tasks. Along the successive phases, the tapping rate changed according to the associated functions (navigating, searching, discriminating). The rhythms were much comparable with those of other animals using active reception (pulse electric discharges, sonic clicks). In particular, a drastic speed-up occurred at the time of determining the target’s orientation, similar to sperm whales’ “creaks” before attacking a squid, discharge “bursts” of mormyrid foraging on a worm and bats’ “final buzz” before an insect catching. Furthermore, the speed-up profile along files was proportionally the same for all participants. Our etho-mimetic approach highlights spontaneous human rhythm features in an analogous situation and confirms our understanding of animal behavior. It illustrates the relationship between time and space by the advantage of a higher sampling rate for a finer spatial resolution.

Avril, A., & Graff, C. (2007). Active electrolocation of polarized objects by a pulse-discharging electric fish, Gnathonemus petersii. Journal of Comparative Physiology A, 193(12), 1221-1234.
Britton, A. R., & Jones, G. A. R. E. T. H. (1999). Echolocation behaviour and prey-capture success in foraging bats: laboratory and field experiments on Myotis daubentonii. Journal of Experimental Biology, 202(13), 1793-1801.
DOH, Y., DELFOUR, F., AUGIER, E., GLOTIN, H., GRAFF, C., & ADAM, O. (2018). Bottlenose dolphin (Tursiops truncatus) sonar slacks off before touching a non-alimentary target. Behavioural processes, 157, 337-345.
Miller, P. J., Johnson, M. P., & Tyack, P. L. (2004). Sperm whale behaviour indicates the use of echolocation click buzzes ‘creaks’ in prey capture. Proceedings of the Royal Society of London. Series B: Biological Sciences, 271(1554), 2239-2247.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/33%20augier.mp4?vrtx=view-as-webpage;;
;131;SMS (S2);Justin London;Niels Chr. Hansen, Carolyn Ee, Peter Vuust and Peter Keller;Oxytocin effects on sensorimotor synchronization;"sensorimotor synchronization
rhythm
oxytocin
cohesion
allostasis";"BACKGROUND: The prominence of joint music making in sports, war, and religion (McNeill, 1997) suggests adaptive functions of music-induced social cohesion (Fitch, 2005; Savage et al., 2020). Yet, the neurobiological underpinnings of music's alleged status as a ""biotechnology of group formation"" (Freeman, 2000) remain unknown. The neuropeptide oxytocin is implicated in multifaceted aspects of social behaviour—including pair bonding, parental caregiving, memory, and interpersonal coordination (Kendrick et al., 2017). Findings that oxytocin is sometimes released after singing (Grape et al., 2002; Keeler et al., 2015) and passive music listening (Nilsson, 2009) and increases synchrony with an unresponsive human partner (Gebauer et al., 2016) calls for further enquiry. 

AIMS: This study aims to replicate and extend oxytocin’s effects on sensorimotor synchronization and investigate its modulation by sex, task difficulty, partner identity belief, and information flow between dyad members.

METHODS: Following a randomized, double-blind, placebo-controlled, within-participant protocol compliant with standardized guidelines (Guastella et al., 2013), 33 non-musician male dyads (n=66) and 31 non-musician female dyads (n=62) received oxytocin (24IU) or placebo nasal spray during two 90-mins sessions taking place on separate days. After a 30-mins waiting period, participants situated in separate testing booths completed 12 blocks of five trials each. Wearing headphones, they were instructed to drum along to isochronous (120 bpm) or tempo-changing (~100-150 bpm, following smooth curves with varying period) sound sequences generated by their human partner or an adaptive computer partner employing 10% phase correction to imitate humanlike behaviour (cf. Mills et al., 2015). Participants were sometimes deceived to believe they were interacting with a human when, in fact, their partner was a computer. In the two conditions with truly interhuman drumming, sound output was either uni- or bi-directionally coupled (cf. Gebauer et al., 2016).

RESULTS: Linear mixed-effects modelling of preliminary data from 60 male participants, with stepwise simplification of the random-effects structure (Barr et al., 2013), indicates that oxytocin may impair drumming accuracy and precision, especially in unidirectional interpersonal tapping with a designated “leader” and “follower” (p=.029) and when changing tempo together with a computer partner believed to be human (p=.001). Hierarchically regressing the subject-level effects on parameters from the Adaptation and Anticipation Model (Harry & Keller, 2019) fitted to the empirical data showed that oxytocin increases timekeeper noise alongside higher-order variables like adaptive and anticipatory error correction. These behavioural oxytocin effects did not, however, cascade into treatment-related changes in self-reported closeness, connectedness, similarity, or liking (Aaron et al., 1992; Lumsden et al., 2014) or changes in social cohesion ratings of “souvenir” photographs taken of participant pairs after each session. Analysis of the full dataset (along with video-based proxemics measures of seating distance) is underway.

CONCLUSION & IMPLICATIONS: Our preliminary findings suggest that oxytocin may contribute variability to internal timekeeping in ways that facilitate synchronization when accuracy is low but may have reverse effects at higher baseline performance. This lends credence to an emerging socio-allostatic theory positing that oxytocin promotes behavioural flexibility and critical brain states modulating social behaviour in constantly changing environments (Hansen & Keller, in press).
 
REFERENCES: 
- Aron, A., Aron, E. N., & Smollan, D. (1992). Inclusion of other in the self scale and the structure of interpersonal closeness. Journal of Personality and Social Psychology, 63(4), 596. 
- Barr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: keep it maximal. Journal of Memory and Language, 68(3), 255-278. 
- Fitch, W. T. (2005). The evolution of music in comparative perspective. Annals of the New York Academy of Sciences, 1060, 29-49. 
- Freeman, W. J. (2000). A neurobiological role of music in social bonding. In: N. Wallin (ed.), The Origins of Music (pp. 411-24). Cambridge, MA: MIT Press. 
Freeman, W. (2000). A neurobiological role of music in social bonding. The origins of music.
- Gebauer, L., Witek, M. A. G., Hansen, N. C., Thomas, J., Konvalinka, I., & Vuust, P. (2016). Oxytocin improves synchronisation in leader-follower interaction. Scientific Reports, 6, 38416. 
- Grape, C., Sandgren, M., Hansson, L. O., Ericson, M., & Theorell, T. (2002). Does singing promote well-being?: An empirical study of professional and amateur singers during a singing lesson. Integrative Physiological & Behavioral Science, 38(1), 65-74. 
- Guastella, A. J., Hickie, I. B., McGuinness, M. M., Otis, M., Woods, E. A., Disinger, H. M., Chan, H.-K., & Banati, R. B. (2013). Recommendations for the standardisation of oxytocin nasal administration and guidelines for its reporting in human research. Psychoneuroendocrinology, 38(5), 612-625.
- Hansen, N.C., & Keller, P. (in press). Oxytocin as an allostatic agent in the social bonding effects of music. Behavioral and Brain Sciences.
- Harry, B., & Keller, P. E. (2019). Tutorial and simulations with ADAM: an adaptation and anticipation model of sensorimotor synchronization. Biological Cybernetics, 113(4), 397-421.
- Keeler, J. R., Roth, E. A., Neuser, B. L., Spitsbergen, J. M., Waters, D. J. M., & Vianney, J. M. (2015). The neurochemistry and social flow of singing: bonding and oxytocin. Frontiers in Human Neuroscience, 9, 518. doi:10.3389/fnhum.2015.00518
- Kendrick, K. M., Guastella, A. J., & Becker, B. (2017). Overview of human oxytocin research. In Behavioral Pharmacology of Neuropeptides: Oxytocin (pp. 321-348). Springer, Cham.
- Lumsden, J., Miles, L. K., & Macrae, C. N. (2014). Sync or sink? Interpersonal synchrony impacts self-esteem. Frontiers in Psychology, 5, 1064. 
- McNeill, W. H. (1997). Keeping Together in Time. Cambridge, MA: Harvard University Press.
- Mills, P. F., van der Steen, M. M., Schultz, B. G., & Keller, P. E. (2015). Individual differences in temporal anticipation and adaptation during sensorimotor synchronization. Timing & Time Perception, 3(1-2), 13-31. 
- Nilsson, U. (2009). Soothing music can increase oxytocin levels during bed rest after open‐heart surgery: a randomised control trial. Journal of Clinical Nursing, 18(15), 2153-2161.
- Savage, P. E., Loui, P., Tarr, B., Schachner, A., Glowacki, L., Mithen, S., & Fitch, W. T. (2020). Music as a coevolved system for social bonding. Behavioral and Brain Sciences, 1-36.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/131%20hansen.mp4?vrtx=view-as-webpage;;
;96;SMS (S1);Caroline Palmer;Thomas Wolf, Natalie Sebanz and Günther Knoblich;The role of period correction and continuous input from a co-performer in joint rushing;"joint action
joint music making
sensorimotor synchronization";"Recent studies provide experimental evidence for joint rushing - the phenomenon that participants in rhythmic group activities, such as performing ensemble music, unintentionally increase their tempo. Studying the mechanisms causing joint rushing will further our understanding not only of joint music-making, but also of interpersonal temporal coordination during joint actions in general. 

In three experiments, we investigated what leads to joint rushing and which factors affect the magnitude of the tempo change. We hypothesized that joint rushing arises from the combined workings of established sensorimotor synchronization mechanisms (e.g., phase and period correction), with a simple phase advance mechanism, that has been studied in mass-synchronizing insects. In particular we addressed the following questions: How does the phase advance mechanism interact with other correction mechanisms? Which performance parameters influence the magnitude of the tempo increase? Are trained musicians less prone to rush?

We invited musicians and non-musicians to participate in tapping experiments that were based on a variant of the synchronization-continuation task. Participants were asked to continue to produce a constant target tempo alone (solo) or together with a partner (joint). We experimentally manipulated auditory feedback and the initial level of synchronization between partners.

The results show that joint rushing induces a lasting period correction but stops when auditory feedback of the partner’s taps is removed. Furthermore, performance parameters such as tapping variability and interpersonal asynchrony influenced the extent of the tempo change. Musical training reduced the magnitude of joint rushing but did not eliminate it.

We conclude that joint rushing is driven by an automatic, low-level process, which depends on continuous input and is hard to counteract, even for musically trained participants. This is best explained by the assumption that joint rushing occurs because adjustments induced by a simple phase advance mechanism alter the period of internal timekeeping.

The results presented in this study imply that simple timing mechanism for rhythmic behavior that are already present in insects persist in human performance.  The interaction of this mechanism with mechanisms affecting the internal timekeeper causes joint rushing. Due to the highly automatic nature of phase advancement conventional musical training might not be sufficient to eliminate joint rushing during ensemble performances.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/96%20wolf.mp4?vrtx=view-as-webpage;;
;59;SMS (S1);Caroline Palmer;Ole Adrian Heggli, Ivana Konvalinka, Morten L. Kringelbach and Peter Vuust;Self-other integration in rhythmic synchronization;"Interpersonal synchronization
Rhythm
Self-other integration";"Abstract
Interpersonal synchronization may emerge spontaneously as when an audience’s applause transitions to a steady beat, or it can be a necessity of a task as in the case of performing music together. The latter have been extensively studied in joint finger tapping studies [1]. A key finding from this field is that synchronization is multifaceted, with synchronization behaviour relying on different strategies (or dynamics), such as leading-following, mutual adaptation, or leading-leading [2-4]. In previous work we have shown that a Kuramoto-based coupled oscillator model is able to reproduce these strategies in a bottom-up fashion [5]. However, there are still multiple open questions regarding the mechanisms underlying the emergence of these strategies and their behaviour over time. Here we propose a model of self-other integration in rhythmic synchronization which conceptualizes dyadic synchronization as a continuous process of integrating and segregating self- and other-related signals. Our model creates a metastable system with two particular attractor states: One of self-other integration, and another of self-other segregation. Together, these two states explain the three known synchronization strategies, and our model makes testable predictions about the dynamics of interpersonal synchronization both in behaviour and for the brain. In addition, we show how this model can be implemented as a top-down process controlling the generative coupled oscillator model to create an improved representation of rhythmic synchronization.


References
1.	Repp B.H., Su Y.-H. 2013 Sensorimotor synchronization: a review of recent research (2006–2012). Psychonomic bulletin & review 20(3), 403-452.
2.	Konvalinka I., Vuust P., Roepstorff A., Frith C.D. 2010 Follow you, follow me: continuous mutual prediction and adaptation in joint tapping. The Quarterly journal of experimental psychology 63(11), 2220-2230.
3.	Heggli O.A., Konvalinka I., Kringelbach M.L., Vuust P. 2019 Musical interaction is influenced by underlying predictive models and musical expertise. Scientific Reports 9(1), 11048.
4.	Gebauer L., Witek M., Hansen N., Thomas J., Konvalinka I., Vuust P. 2016 Oxytocin improves synchronisation in leader-follower interaction. Scientific reports 6, 38416.
5.	Heggli O.A., Cabral J., Konvalinka I., Vuust P., Kringelbach M.L. 2019 A Kuramoto model of self-other integration across interpersonal synchronization strategies. PLoS computational biology 15(10). (doi:https://doi.org/10.1371/journal.pcbi.1007422).";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/59%20heggli.mp4?vrtx=view-as-webpage;;
;6;SMS (S1);Caroline Palmer;Michael Schutz and Fiona Manning;Moving to hear: Exploring the complex relationship between training, movement, and rhythm perception;"Sensorimotor interactions
Multisensory integration
Rhythm perception
Meter perception
Expertise
Musical training
Movement effectors";"Movement can affect our perception of both rhythm (Brett & Grahn, 2007) and meter (Phillips-Silver & Trainor, 2005). However the perceptual implications of different types of movement and how they can improve our rhythm perception remain open questions.  Which types of body movements are most effective at improving timing perception?  Does the specific movement-focus of different instruments (i.e. finger movements for a pianists’ right hand vs. broad arm movements of violinist’s right hand) shape sensorimotor interactions?   This talk will describe an ongoing series of experiments raising intriguing questions about how training and expertise affect both the quality and consequences of musical movements.  

Our core paradigm involves stimuli consisting of several repetitions of a woodblock pattern followed by a silent “timekeeping” period, and finally a probe tone on-time for 50% of trials.  Participants indicate whether the probe tone is correct under in two conditions: (1) listening while tapping (movement) or (2) listening while stationary (no-movement).  Participants receive equal numbers of movement and no-movement blocks (Manning & Schutz, 2013).  This design is well suited for examining movement’s effect on time, as well as the lesser-studied role of movement type (finger tapping, stick tapping), and effector-specific training (finger tapping for pianists, stick tapping for percussionists).  We tracked both movement timing as well as its effect on rhythm perception amongst numerous groups (i.e. those with no musical training, highly trained pianists, highly trained percussionist) and with different types of movement (traditional finger tapping, tapping on a piano keyboard, tapping with a drumstick on a drumpad).  

Our results (some of which are published) together illustrate that (1) movement can not only change but objectively improve rhythm perception, (2) this effect of movement is stronger in those with extensive musical training, however (3) surprisingly, effects of musical training vanish in the no-movement condition, and (4) stick tapping lead to the best performance—even for those heavily trained on finger tapping (i.e. pianists).  We will discuss the implications of these findings, as well as applications in future research on sensorimotor integration.

This issue is of great value both to scientists exploring the mechanisms of sensorimotor integration, as well as musician educations who frequently disagree on the best practices for teaching about music and movement—for example, whether foot tapping while playing is something to be embraced or avoided.  Consequently this work speaks to a broad audience, making RPPW the perfect forum for interdisciplinary discussion of these results.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/6%20schutz-moving-to-hear.mp4?vrtx=view-as-webpage;;
;105;Speech (SP);Molly Henry;Yi Wei and Edward Large;Entrainment stability predicts melodic intonation therapy performance and reading fluency;"entrainment
synchronization
dynamical system
reading
melodic intonation therapy";Melodic intonation therapy (MIT) has a long history of application for patients with non-fluent aphasia. The fundamental technique involves synchronization of movements to syllables while speaking/singing. Research has also shown impairment of the ability to entrain movements to an auditory rhythm in clinical populations with language related deficits, such as aphasia and dyslexia. In this study, we explored the relationship between rhythmic entrainment, MIT synchronization, reading fluency, and reading comprehension in healthy English- and Mandarin-speaking adults. First, we examined entrainment stability by asking subjects to coordinate taps with an auditory metronome in which unpredictable perturbations were introduced to disrupt entrainment. Next, we assessed MIT synchronization by asking subjects to coordinate taps with the syllables they produced while reading sentences as naturally as possible. Finally, we measured reading fluency and reading comprehension for native English and native Mandarin speakers.  Entrainment stability correlated strongly with MIT synchronization and with language fluency, and both findings generalized across English and Mandarin speakers. Implications for the development of interventions based on rhythmic entrainment are discussed.;;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/105%20wei.mp4?vrtx=view-as-webpage;;
;87;Speech (SP);Molly Henry;Sophie Scott and Kyle Jasmin;The neural basis of rhythm and sensorimotor co-ordination;"functional magnetic resonance imaging
joint speech
neurobiology of auditory processing";The neural processing of sound in primates has shown a role for distinct functional and anatomical streams of processing, associated with different kinds of perceptual engagement. In this talk I will explore the implications of these streams of processing for rhythmic processing in humans, and the ways that this links into motor systems. Specifically, I will contrast the roles of rostral and caudal streams of auditory processing and argue that the evidence for the finer temporal resolution of the caudal stream, and its links to sensorimotor processing, may mean that it has an important role in rhythmic processing and the control of rhythmic actions. I will illustrate this with fMRI data from joint speech tasks.;;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/87-scott.mp4?vrtx=view-as-webpage;;
;78;Speech (SP);Molly Henry;Courtney Hilton and Micah Goldwater;A metrical Stroop effect?: Misaligning meter with syntax disrupts sentence comprehension and sensorimotor synchronization;"Speech
Meter
Syntax
Sensorimotor synchronization
Neural entrainment
Metrical alignment
Cognitive control";"Analogous to music, the rhythm of speech tends to be hierarchically organized into meter in the minds of listeners. This is thought to confer affordances for perception, memory, and motor coordination. Speech meter also aligns with phrasal structure in systematic ways. In three experiments, we show how this alignment affects the robustness of syntactic comprehension. We manipulated meter-syntax alignment while complex sentences with relative clause structures were either read as text (experiment 1, n = 40) or listened to as speech (experiments 2, n = 40, and 3, n = 30). All three experiments measured comprehension. Experiment 2 also had participants finger-tap in time with the metrically accented syllables. Experiment 3 recorded EEG.

Across the three experiments, the commonly attested meter-syntax alignment, roughly corresponding with the so-called ‘nuclear stress rule’, resulted in the most robust comprehension. More mistakes and greater response times ensued from other alignments. 

Experiment 2 also showed an effect on sensorimotor synchronization. That is, when the task required participants to tap in time with typically unstressed syllables, their tapping was more variable. We interpret this as a kind of ‘metrical Stroop effect’. That is: a conflict between bottom-up reflexes expecting a metrical accent in one location and top-down cognitive control imposing it on another. 

Finally, experiment 3, looked at the role of neural oscillations. Contradicting some previous predictions, delta-oscillations were shown to reliably track the perceived metrical structure across the different meter-syntax alignments, rather than tracking the syntactic structure. 

We discuss the implications of these results with respect to theories of sentence processing and suggest an important cognitive role for metrical rhythm. We also briefly speculate about how this may also apply to music perception and production.";;https://www.uio.no/ritmo/english/news-and-events/events/conferences/2021/RPPW/videos/talks/78%20hilton.flv?vrtx=view-as-webpage;;
